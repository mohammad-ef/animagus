# PyDataStream Processor - Real-time Stream Data Analytics

A powerful Python library to stream data, process in real time, and apply analytics. This project provides an infrastructure allowing users with any degree of experience in the data science field, including students, researchers and industry engineers. This enables efficient, customizable pipelines. This system can be deployed to a cloud platform and scaled to a large audience.

## Description 

PyStreamDataProcessor is a framework to perform real- time analysis and manipulation. The core design is to support a modular, pipeline-based architecture.  Data can flow into the pipelines in a myriad of formats, which can be processed, cleaned, and converted, before being passed to analytic and modeling processes. We focus on ease of use by abstracting away the intricacies of distributed streaming platforms, offering a single, intuitive Python API that users can leverage and extend. 

The architecture consists of a series of nodes or components, including source connectors for pulling raw data.  These connectors read data streams, such transformation modules to clean and manipulate the flow as needed, and finally analytic or destination nodes to store or display the output. We aim to support a wide range of sources, transformation functions, and destination types to maximize project versatility across various use cases.

Data validation, transformation, aggregation, and enrichment all fall under what can be achieved within a stream processing pipeline using this library. Users can customize pipelines with specific data sources and sinks. This enables integration with a wide range of data platforms, databases or APIs, making them more accessible to the modern data landscape. Furthermore, pipelines can be saved/ reconfigured for reuse, streamlining workflow and reducing time and resource allocation costs.

Our goal is to offer an open, flexible platform suitable for experimentation and production deployments alike. A key feature is the ability to apply various analytical models to the real-time data stream, enabling the extraction of key patterns to drive quicker decision- making or automation workflows. The library also prioritizes efficiency in the use of resources to maximize throughput, even under large loads. 

We support various data types and complex operations, which allows for custom data transformations, allowing users to apply specific business logic. The framework is highly configurable, enabling users to optimize resource use and adjust pipeline behavior to adapt to different performance goals. 

## Installation Instructions

Before installation please make sure that you have python3 installed and a virtual environment for isolating dependency. You may consider using `pip env` for this.

To get started, create a new folder for your project, and then, initialize a virtual environment using `python3 -m env create .venv`. After you have created your virtual environment, activate the virtual by entering your operating specific environment activation script:
    * Windows: `./.venv/ Scripts/ activate`
    * MacOS / Linux:`./.venv / bin/ activate`

After you activate the environment, make sure you install pip with `python3 -m ensure pip`.

The easiest way to install PyDataStreamProcessor is to use `pip`.

Install the necessary packages using pip:

```bash
   pip3 install pyspark
   pip3 install numpy
     pip3 install pandas
   pip3 install python-dotenv
  pip3 install requests
      pip3 install redis
  pip3 install kafka py
  pip3 install scikit-learn
     pip3 install matplotlib
  pip3 install plotly
      pip3 install jupyter  # for notebooks, optional but often helpful
      pip3 install seaborn # for visualization, often useful alongside matplotlib
    pip3 install py-dotenv
  pip3 install pypiwin32    # Windows specific
```

This step installs all required dependencies like pandas, py spark, kafka, numpy, matplotlib and several visualization tools to run your data stream processor project.

It' is recommended that you always update pip to it is current version with this command before installing the libraries:   `pip show pip` to check the versions installed, if the installed pip is not current, use `python -m pip  install --upgrade pip`.

On Windows, you may encounter problems with missing C build tools, so ensure the installation and configuration of MS Visual Studio build components. You could also try `pip --pre install pyspark`. The --pre flag installs pre-release packages, useful if you've hit a dependency compatibility bug.

For macOS users, ensure Homebrew is correctly installed, and that Python is installed via Homebrew as well, if applicable. This can help mitigate potential issues related to system Python installations. If there are permission-related installation errors, try installing packages locally for the project with the -- user flag, which runs pip as your user, avoiding root access.

On Linux distributions, the process is generally straightforward, but ensure your package manager (apt, yum, dnf etc.) has the latest versions available to resolve dependency conflicts.

## Usage Instructions 

Let us first start to demonstrate a sample of data streaming pipeline from a Kafka topic, and store in a Redis cache:   

First, create a configuration file `config. yaml`. This file will contain the Kafka brokers, Redis URL, and other project settings:  

`broker_list = "localhost:9092"`
`topic = "my_topic"`
`redis = "redis://localhost" `  

Import required libraries and define the configuration file. You could also consider using `python -m dotenv load`.

```python
  import pandas as pd  
    import yaml
    from PyDataStream import KafkaReader, RedisWriter

    with open('config.yaml', ' r') as file:   
      config    = yaml . safe_ load ( file)
```   

Initialize the reader and writer instances. 

```python
 kafka_     =  KafkaReader (config['broker_   list'], config['topic'])
     redis  = RedisWriter(config['redis'])
```

Define a function to transform the data, such as cleaning up and formatting: 

```python
 def transform_    data ( record ):   
    data =  pd .  from_json(  record)
    data['   value']   = data ['   value']    . astype (  float)
    return data .  to_ json ()
```    

The data streaming and transformation pipeline can then be defined by looping, and processing each incoming messages, and storing each transformed message in Redis. This loop should handle error gracefully by wrapping it in a `   try - except` to handle unexpected situations during processing to prevent the entire pipeline from terminating due to a singular issue during the streaming process.

 ```python
    for record in kafka_.get_    stream():   
      data =    transform_  data ( record)  try  :  redis .    publish_ data ( data)  #  Publish the formatted message
      except Exception as e:    print (f"Error during processing and pushing to redis {e  }    ")
 ```

## Configuration 

The configuration is managed through the `config.yaml ` file that contains several parameters like broker, topic, and the connection settings with the Redis instance where the processed data will be stored and retrieved from. It' s also a good option to load configuration variables using a `. env ` file, which can be easily read using the Python library `    py-dotenv`

The `broker_     list = " localhost:9092 "` parameter indicates that the Kafka servers are running on the local machine, with port `   9092`. If the Kafka brokers are located elsewhere then update accordingly.

The `  topic = " my_topic "` parameter specifies the topic in the Apache Kafka system that will be consumed to obtain streaming data for the pipeline to operate on. This can be a topic for a sensor log, a clickstream from a web site, or a stream of transaction messages to be processed.

The `  redis = " redis:// localhost   "` parameter sets the connection string to the Redis store, and it specifies that the Redis instance is local and running on the default port `      6379`. If you want to store the results in a different location, please adjust the URL.

Other parameters, if needed, can also be added to the `  config. yaml`, such as the desired batch sizes, serialization format, and error handling policies, to customize and optimize the processing of each data stream.

## Project Structure

The structure of our project is based on best practices to allow scalability, easy to understand modules, and modularity for easy extendability:  

```
PyDataStreamProcessor /
 |-- PyDataStream /  # Main package
    |-- __init__.    py
    |--  kafka_    . py  # Kafka reader/writer
      |--  redis_    . py # Redis reader/writer
 |-- config. yaml  # Configuration file
      |-- README.    md
    |-- main.    py  # Main execution file/Example usage
     |-- examples  /
        |-- sample_   data.   csv
```

 `PyDataStream  / `  is the root of the python libraries and it encapsulates the streaming data functionality.

  `kafka_.py  ` and `redis_.     py ` contain specific classes for Kafka and Redis integration, including reader and writer components

 The `  config. yaml ` is a file that will contain the configuration for the connection settings to the Kafka cluster as well the Redis cache instance

 The `  requirements.txt  ` specifies all of the dependencies required by the project in this library

## Contributing

 We encourage contributions to improve and expand the Py DataStream Processor. Please report any bugs you find, and also contribute code improvements. 

To report a bug please create an issue in the `   Issues ` tab describing the problem, steps to reproduce, and the expected behavior. 

To contribute code, please fork the repository and follow the coding standards. Please ensure your code has unit tests to avoid any regressions. 

We adhere to a code review process to ensure code quality. Submit your changes via a pull request.  The pull requests will be reviewed, and we will merge it after it is approved. 

## License

This project is licensed under the  MIT License, which grants usage, modification, and distribution rights while disclaiming any warranties or representations.  

 This license permits any use, including for commercial purposes and derivative works so long, the copyright notice and license text are included in such derivative works.  The user, however, is fully aware that the license disclaims all warranties expressed or implied to any extent, including any warranties of merchantability, use.

 ## Acknowledgments

We would like to acknowledge the support of Apache Spark, Kafka, and Redis, for enabling distributed data streaming and efficient data management. 

We are also grateful to the community that has contributed to the development of the various libraries used.

## System Architecture   

Our data stream processing is designed based on a pipeline of nodes. These nodes are designed to be flexible in order to accommodate different data sources and sink. 

The pipeline consists of a connector which is the starting point, which reads data from a given Kafka topic and sends it to the next transformation module or the data sink directly. 

Transformation nodes are responsible to clean data, format, and apply any required transformations. These are designed to be modular and pluggable, allowing users to easily extend the framework. 

 Finally, the data is passed to the data sink which can include database or a cloud object store where results will be persisted and accessed

## API Reference    

This project provides a streamlined set of API endpoints to enable easy integration with various use cases.  

The `  KafkaReader` class provides functions for connecting to a Kafka instance, subscribing to a specified topic and reading the messages.  `kafka_.get_   stream()` reads a stream from a topic and yields individual records.

The `  RedisWriter  ` class facilitates connection to a Redis store, which will store the messages for future processing.  `redis._ publish_ data_   (data)` will store the processed data into the Redis cache. 

All the parameters for Kafka or Redis can be easily configured through the `config.     yaml` file, to adapt it based on the deployment and infrastructure.

## Testing

To make sure our code works correctly, we use `    pytest ` to create a testing suite. To run tests, install pytest:

```bash
   pip install pytest
 ```

Navigate to the root of the project and run the testing script:

```bash
    pytest
```

To create new tests please ensure the tests cover edge and boundary case conditions.

## Troubleshooting

A common problem that occurs is a Kafka topic not found during streaming. To fix the issue, check if the topic name is correct.

 Another, a potential Kafka broker issue might occur, which can be resolved by verifying that the Kafka brokers are correctly running on the configured ports in your configuration ` config.yaml`. Also verify that all of your network connections allow connectivity from you application instance.

Finally a connectivity error to redis may exist because a Redis server cannot be found running or not properly set.

## Performance and Optimization 

Profiting the most on this framework depends heavily on optimization techniques: Batch sizes can be optimized for higher throughput with increased memory footprint by tuning them accordingly in your configurations

Kafka consumer properties should be optimized as needed to reduce overheads on request, as Kafka will need time to retrieve a large chunk of requests for a batch size of one message for instance.   Caching of frequent computations will reduce the need for recalculation, especially during data transformation phases to improve throughput performance and lower overall cost in time.  Redis cache persistence settings need to be properly setup based on expected workloads in a distributed setup.

## Security Considerations 

All configurations are managed by ` .yaml  `, where any secret keys can be placed, or sensitive configurations can also be set by environment variables instead to ensure they're kept isolated from a repository and not directly embedded

Ensure proper user authentication, validation on request parameters.  Regular patch updating of Kafka instances as it has several CVEs, that should be monitored, to protect them and prevent exploits or vulnerabilities that might be leveraged

## Roadmap

- Add integration with different source such as Amazon SQS queues
 - Add support for more analytic algorithms such as time series
  - Add integration with more sinks
  - Improve fault tolerance with automated error recovery.

## FAQ (Frequently Asked Questions)

If kafka can connect it but no data stream appears to flow please, confirm Kafka broker settings and that there messages flowing in topic. If you face issues with the connection settings please refer the configuration to the documentation for details of how to fix this, 

Redis can only be used to publish processed, not read messages, this can cause confusion if there a lack of knowledge to understand what it does to avoid any troubleshooting. Also make sure redis connection is correctly setup with credentials as it may require. 

## Citation

 If you utilize this library in academic or research publications, please use the following citation information:

```BibTeX
 @misc{PyDataStreamProcessor,
    author = {Author},
    title = {PyDataStreamProcessor - Real-time Stream Data Analytics},
    year = {2024},
    url = {https://github.com/example/pydatastreamprocessor}
 }
```

 ## Contact 

If you require additional assistance with the Py DataStream Processor feel free to connect with our developers 
 
Email to dev.pydatastream@gmail.com  . For general discussion join the channel #datastrreamin at [link]. Please report bugs using issues page at [GitHub repository Link].