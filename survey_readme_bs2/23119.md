# DataInsight Analyzer - Predictive Analytics Engine

## Description

Developed by the Stellar Analytics Group for businesses aiming to improve decision processes utilizing AI/ ML techniques to uncover trends from existing datasets. It offers a streamlined solution for analyzing, preprocessing datasets.  DataInsight Analyzer provides powerful capabilities including exploratory data analyses, anomaly detection, predictive modeling, and comprehensive reporting with customizable dashboard visualization.

The core architecture relies on an extensible plugin-driven approach, easily integrating new data sources, algorithms or analytical features. This modular design allows for rapid adaptation to evolving analytic requirements.  The underlying framework incorporates best- practices in terms of data security and efficient data transfer across the system. Data is transferred via encryption protocols using secure APIs.

The tool primarily uses the Python framework with popular libraries for data manipulation (`pandas`), visualization libraries (`matplotlib/ plotly)` and predictive models using tools from  `sklearn`. It has been optimized for both CPU, utilizing efficient vector calculations with optimized routines to enhance performance, making use of libraries with C based extensions for faster performance. GPU optimization is planned for the v.2 of the application.

We have developed a highly scalable architecture using containers, supporting the creation for multiple instances. Each container handles multiple requests using an async approach. This approach allows the application to scale easily and maintain its reliability. The containers communicate over a secure messaging bus.

The application can be integrated with various data warehousing platforms using the provided API' and connectors for data ingestion. The tool is designed as a flexible solution, adaptable to organizations of any size. It can be deployed both cloud environments (AWS or Azure) or a local on -premises setup depending on your specific requirements for governance and cost optimization goals.

## 1. Installation Instructions

The following details how to install our powerful DataInsight Analyzer. Before proceeding please confirm all dependencies have access and the correct environment set up is available. It should also be verified there sufficient memory on the system for large datasets.

To initiate, ensure your machine has Python 3.8 or higher is installed. Python's package installer ` pip` will be essential. The easiest way to get these components are installing through Anaconda from the official site. Verify the version with using command `python - V` within the terminal, and it should show your Python 3 version.

Next is downloading the code base and extracting it. Download the latest version through the releases page, and then extracting the archive `tar.gz` using standard tools on your Linux environment, like the `tar` command. The same approach would work for MacOS using standard utilities for unzipping archives. On Windows use 7-Zip or a standard zip extraction tool. 

Move the directory to its designated location where you want to store all of your source code files in. It is common that source code repositories get stored within `/opt` folder on Linux machines for ease management. After that, ensure the current user has sufficient permissions to read and write in the directory that you' ve created.

To establish the Python environment, enter the directory, then run the following: `pip install -r requirements.txt ` This will resolve and download all project dependencies. Note this may take sometime, depending on your internet connections, and available packages. Ensure the command runs successfully, and all dependencies are resolved successfully before the installation continues. It may be beneficial to update pip before hand. To accomplish this use: `pip install --upgrade pip`.

For Windows users with a Python environment setup using venv, you must activate it using `.\venv\Scripts\activate `. If using an Anaconda distribution, ensure you have activated the target environment. You might also consider creating a new virtual environment for better isolation from any other existing packages.

For Linux or MacOS environments, after navigating to the project directory, you can initialize a new venv and activate it with commands like : `python3 -m venv .venv` and source `. venv/bin/activate`. This approach helps in keeping all packages and versions separated. If you are encountering issues installing certain packages, try installing `wheel` as a global dependency.

Now the project has its requirements setup, configure the environment variables.  ` DATAANALYZER_HOME` must be set to the path of your installation root and `DB_CONNECTION ` must have connection details. This can be defined via shell commands ` export DATAANALYZER_HOME = /your/ installation path`, `export  DB_CONNECTIONS = "user:password :hostname:port:database "`.

Once this has been done, verify by checking the installation via command ` DataAnalyzer -version`. Verify the output shows the version of the Data Analyzer. This should ensure a successful installation.  If issues still arise, check that all packages are in the python path.  

## 2 Installation - Platform Specific Notes

On Windows, it' is recommended to use ` pip install --target=<target_directory> <packages>`. Replace `< packages>`  with the packages listed in your `requirements.txt  `. After successful install, add the `< target directory >` to `PYTHON_ PATH`, which allows you to use modules directly.

MacOS may need XCode tools to handle dependencies and compilation. You can install them using HomeBrew by executing : brew install  xcode-command-line-tools. This step is important, especially during dependency resolution with libraries having C extensions. Ensure you' ve updated all packages before the installation finishes.

On Linux you should also install any needed dependencies for the specific libraries. The documentation for each library will detail which are needed for proper functioning, this may include build- essential, libxml2-dev, and others. It is best practice that your OS is always up to date.

## 3. Usage Instructions

Once installed, start the DataInsight Analyzer service using the following command: `Data Analyzer start `. This starts a server on the localhost with the default port. Check the server with command `netstat - an |grep 5000`(assuming the server is set up on 5000). You should see a connection.

The core functionality can be leveraged via the REST API endpoints which is available at `http://localhost : 5000`. The API is fully documented in a Swagger UI which can be accessed at ` /docs` after the server is up & running, where you can test the API calls. You can use any REST client for making calls to the end points.

To upload a CSV dataset, make a POST request to ` /upload`, with a JSON object that includes the data file, the desired delimiters. Ensure file size limits are observed to ensure optimal performance. The server should return a success message, along with the ID.

For basic data analysis such as calculating statistics and correlations, use the ` /analyze `endpoint.  The parameters for analysis should be specified in the JSON payload. The API will return statistics on the specified column of the dataframe.

For advanced predictive modelling, the `/predict` endpoint should be used.  This will accept an ID of a pre-uploaded data set, the features, target variables, algorithm and hyperparamters. The endpoint will return the predicted results. Please ensure the model training data has been uploaded and properly formatted. It may take a minute or so for a prediction to complete depending on the complexity.

To create visualizations for the uploaded datasets use endpoint   `/visualize`. The request should define the chart type and the relevant data columns. The API will return a URL to the generated image of the visualization. It supports a diverse range of chart types to accommodate all your visual data exploration needs. This endpoint is a very effective way to get a feel for datasets quickly.

## Configuration

The core configurations of DataInsight Analyzer are controlled via environment variables and a global configuration file `configs/ analyzer _config.yaml`. This enables flexibility and ease of deployment in various environments. The `configs/ ` directory should be accessible to the running application.

The `analyzer_config.yaml` file defines global settings such as database connection details, default analysis parameters, and logging configuration settings. It contains sections for `database`,  `logging`, and `model`, defining the database connection string and model default hyper parameters.

`DATABASE_URL` environment variable dictates the location of the database. For local environments, a PostgreSQL database connection string should be supplied. For instance: `postgresql://user :password@ host:port/database`. For cloud environments the database URL may be different depending on the managed services.

`DATA ANALYSISER_HOME` points to the main application directory.  This variable is often used for locating resources and configuration files. It can be overridden by the system if required. For example, `/opt/data -analyzer`.

Logging level can be configured via the `LOG_LEVEL `environment variable. Available logging levels includes `DEBUG`, `INFO`, `WARNING`, and `ERROR `. The level can be set to `DEBUG` for detailed debugging during setup, which provides detailed log messages.

You can override the default parameters in the ` analyzer_config.yaml` file by setting environment variables with the same name but prefixed with ` ANAL YZER_.`, such as ` ANAL YZER_ALGORITHM = "RandomForest" `. It provides a simple and effective way to customize the application' behavior.

The maximum file size for uploads is configured by environment variable `MAX FILE_SIZE`. If this is not set, then defaults to 100MB. The server may throw an exception if the file size exceeds the maximum. This should be set depending on your data size.  

## Project Structure

The project's codebase is structured as follows:

*   `src/`: Contains application source code, including data analysis functions, model training scripts, and API endpoints. This is the central location for all custom Python code. It is well documented.
 *   `src/api`: Handles REST API endpoints with request handling and route mapping.
 *   `src/data`: Includes modules related to data loading, pre-processing, and transformation.
 *    `src/models`: Implements predictive models with model creation, and validation routines to enhance accuracy.

*   `configs/`: Stores configurations and parameter settings. Includes `analyzer_ _config.yaml`.
    * `configs/ analyzer_config.yaml`: Global configuration settings.

*   `tests/`: Contains automated test cases, including unit and integration tests to verify the code.
  *   `tests/test_data`: Tests for data processing modules.
  *   `tests/test_models`: Tests for model training and prediction methods.
  *   `test/test_api`: End- to-end testing and API verification.

*   `models/`: Stores serialized model artifacts. The models are saved to this directory once trained.

*   `data/`: Sample data for testing and demonstration.

    *   `data/ sample_data.csv`
        : Example CSV dataset.

 *   `README.md`:  Project documentation.
 *     `requirements.txt`:  Python dependency list.


 ## 7. Contributing

We welcome all contributions to DataInsight Analyzer. The following guidelines outline how to contribute effectively.

To report issues, open a new issue on the GitHub repository providing as much context as possible, including relevant error messages and code snippets. Detailed information helps us to reproduce and fix problems quickly. Clear issue descriptions are essential.  

If you have a fix or enhancement, create a new pull request against the main development branch. Pull request descriptions should be well structured detailing the nature of the change.

All code contributions are expected to meet our coding standards, which includes following PEP 8 for Python code, using meaningful variable names and comprehensive docstrings.

Before submitting pull requests, perform thorough tests to confirm that your changes don' t introduce regressions. Unit tests should be developed and added to verify the functionality.

We use Git for version control. Branching from the main branch and submitting pull requests is a common development workflow.  Ensure your code passes all existing tests before submitting.

## 8. License

DataInsight Analyzer is released under the **MIT License**.

This license permits the use, modification, and distribution of the software under certain conditions including, but not limited to, the preservation of copyright notices and the inclusion of the license within distributed copies of the software. It provides a permissive free software licensing approach. Please refer to the full license file for details.

## 9. Acknowledgments

 We thank the open-source community for the invaluable libraries and tools that form the basis of DataInsight Analyzer, particularly:

*   **pandas:** For efficient data manipulation and analysis. The extensive functionalities are invaluable for many operations.
*   **scikit-learn:**  Provides machine learning algorithms for predictive modeling and classification. It's the backbone for the ML capabilities in the project
*   **matplotlib/plotly:** For data visualization. It helps us provide insights into trends quickly and effectively.
*   **Flask:** For constructing REST API and building a web service. It enables seamless data ingestion.
*   **Python Community:** To developers for contributing and helping shape and build a strong foundation.



## 10. System Architecture

DataInsight Analyzer has distributed architecture using the flask based RESTful APIs.  It leverages a layered architecture where modules communicate via well-defined contracts, improving separation of concerns.  A modular system enables scalability on a cloud deployment.

Data is loaded from external source and then transformed using pre processing pipeline implemented with data processing libraries, namely Pandas. These transformed are used to construct training data, with a configurable model using scikit learn library to make accurate predicitions on incoming requests.

Data flow involves uploading dataset files and invoking APIs calls. Once the upload finishes and a successful response occurs it stores a link. Data requests can then invoke prediction API requests which then calls the trained models for a quick and effective output

API calls utilize an asynchronuous implementation and load balancers across several regions for maximum redundancy to provide consistent uptime to all requests to ensure optimal response times even when experiencing higher traffic volume and to prevent overload issues and improve availability for a distributed application setup.

The data layer is isolated using a database to store uploaded files.  It enables persistent datasets and provides a robust and stable platform on any given environment.  A message broker allows decoupling modules for efficient task queueing, improving application performance, resilience.




## 11. API Reference

DataInsight Analyzer exposes REST API endpoints for all functionality, detailed and structured to make it very simple for any developers who want to utilize its core functionality

### /upload

**Method:** POST

**Description:** Uploads a data file (CSV format).

**Parameters:**

*   `file`: Data file in CSV format (required).
*   `delimiter` The delimiter that defines a separation point. Default `","`
**Returns:**

*   JSON: `{"message": "Upload successful", "file_id": "unique_file_id"}`
  `{"status": error, error}` in error.

### /analyze

**Method:** POST

**Description:** Perform Basic Statistical analysis

**Parameters:**

* `col_name`:  Data to be analyed in String.

**Returns:**

* `col`: name, count , Mean, Standard Devitation and correlation to another value



### /predict

**Method:** POST

**Description:** Performs Predictive modeling and provides predicted output.

**Parameters:**

* `file_id`: Unique Identification file that exists from file Upload

**Returns:**

* `Predictions Array ` Output array in order based on the file_ID passed to function call




## 12. Testing

Testing forms a vital aspect in the overall process for this software package and the following are key instructions in setting the environment and running various modular and integrated testing suites.

You will first be asked to make sure a python 3.9+ or newer python is configured on the target machines, the virtualenv needs to be properly configured for this to succeed

Then using an automated test execution runner using ` pytest`. `pip` must have `pytest` module installed prior execution

You must have to execute a set of automated commands by going to a dedicated module in your local terminal

The testing modules contain various testing cases, with reminders, automated test case execution and a summary to confirm a test is valid for execution




## 13. Troubleshooting

Common issue include dependency issues when building and setting environment configuration, and runtime errors when running a module

Dependency problems arise because you have to confirm versions are properly set or that libraries do exist on target systems. It might arise when building, it may not install certain required packages to support functionality, such issues may often get resolve after a pip reinstall to ensure all libraries can properly installed

For runtime Errors confirm the file permissions exist on files you attempt to write, confirm database configuration, check log errors for a more detailed error trace for better insight in what has went wrong. If there's no clear log information available then increase the log severity and re execute




## 14. Performance and Optimization

Several approaches for enhancing and optimzation have been employed. We utilize C libraries within Pandas and Scikit learn modules, as they allow vector operation

Furthermore a asynchronous task runner is deployed that improves responsiveness of APIs, allowing to execute long computations concurrently to minimize latency

Cache mechanism is in place, it is set with Redis. It improves response by quickly pulling previously generated results. Data sharding helps improve the scalability to allow large volumes.




## 15. Security Considerations

We employ encryption protocols for the secure transfers and utilize access and identity management. It prevents unauthorized users and systems to be accessed or utilized within systems that are in place to safeguard

Secure storage protocols for credential, secrets, configuration and access token and all sensitive files in a highly secure encrypted storage

Regular updates of libraries, and packages, along patching of all potential security vulnarilities is a priority to safeguard the application in an on-going capacity




## 16. Roadmap

Future features planned and prioritized

**V1.1:** Support integration into cloud environments ( AWS S3). This enables the ability to run data analysis directly in AWS. Also support additional algorithms

**V1.2**: Integration and enhancement into Spark data frameworks that would improve the ability of processing large scale distributed computation and datasets
*Implement GPU accelerated processing for increased computational speeds for models and algorithms

**V2.0**: Automated Machine-learning capabilities with an auto hyper-parameters optimizer




## 17. FAQ (Frequently Asked Questions)

*Why am I receiving errors when attempting to start?

Check all necessary dependencies in ` requirement . txt `.

What data type does this API accept ?

Accept CSV and delimited files




## 18. Citation

The use, modification, and reproduction can occur for the research, with an acknowledgment in all research and scholarly publication that utilize it in research and scientific activities, it must mention

 `@misc{ data_analyzer , author = {stellar } , title = { DataInsight analyzer }, year = {2024 }, url = { URL }, }`



## 19. Contact

Please reach out with your inquiries

Via the email address dataanalysistesting @email . com or through Github issue page and submit the concerns you might face and or the ideas you want us to implement, and suggestions in future versions of the software to further refine and develop a strong product.