# Stellar Insights Data Processor

**A platform for efficient ingestion, normalization, filtering, transformation & analysis of stellar observational data from heterogeneous telescope systems.** This repository encapsulates Stellar Insights Data Processor which is an open-source Python tool developed for astronomers and astrophysics researches for handling vast observational data. The tool provides functionalities ranging from data quality assurance up till performing data visualization for insights generation and exploration.  

The primary purpose is streamlining the workflow involved for handling astronomical dataset and accelerating scientific discovery from data-intensive observations. By offering pre-built data processing modules for data reduction, filtering, normalization, and feature extraction, it lowers barrier of data entry while simultaneously allowing for customization with Python scripts. It also integrates a flexible data storage mechanism to handle large volumes of data efficiently for faster analytics.

This project addresses a critical issue: the difficulty of integrating disparate telescope data into a unified and analysable framework. Observatories frequently employ different instruments and observing techniques generating various format data which are not directly comparable without complex processing pipelines. Our project provides standardized methods for transforming such heterogeneous information, allowing researches to gain meaningful knowledge by merging multiple information channels. Stellar Insights helps astronomers in generating scientific results quicker by minimizing laborious preprocessing effort, improving efficiency while enhancing collaboration amongst astronomers and research.

Stellar Insights employs a modular, object-oriented architecture to ensure extensibility and ease of customization.  Core data manipulation functions exist in `processing_modules` while storage management lies under `data_storage`. Data models and configurations exist inside `config` while the user interaction scripts are placed in `interface`. A well-structured and clean directory arrangement supports scalability with easy integration for newer observational technologies and analytical techniques in the future. It utilizes Pandas for tabular data handling, Numpy for computational performance, and Astropy for handling astronomical formats such as FITS and coordinates, allowing fast access.

Finally, the software's adaptability and community driven development approach will ensure long-term supportability while promoting broader accessibility and scientific discoveries within astronomy domain. The focus remains centered towards simplifying and improving workflows related to big data processing, allowing users more resources towards actual astronomical insights, instead of time invested in complex infrastructure management or tedious scripting procedures. We also prioritize robust and extensible documentation with detailed usage instructions, API descriptions to enable researchers of varied backgrounds and data skills in contributing and making optimal utilization from software capabilities.

## Installation Instructions

Before using Stellar Insights, make sure Python 3.8 or above is installed on your system. Python 3.9 or higher is strongly recommended to benefit from newer libraries & functionalities. It should run with minimal overhead depending on the size of datasets used in operations. A compatible C++ compiler is also required if some dependencies need compiling, like numpy itself on certain system distributions. This should already exist with many default Python installation methods but verify the setup if installation issues appear later on.

Download Stellar Insights from GitHub using Git. Alternatively, directly clone repository with a single line of instruction to start your data insights pipeline: `git clone https://github.com/Stellar-Insights/Data-Processor.git`. The downloaded source directory needs extracting before starting any related installations as per instructions, as the git repository structure contains all related scripts for execution in the next steps of configuration setup and usage guidelines provided ahead. 

Once cloned or downloaded the directory, change current working directory with this location using this simple `cd Data-Processor` in a bash-shell or terminal prompt on any modern operating system setup to access installation instructions in further processing to install Stellar Insights on any set platform. It requires standard python packaging setup which can handle the required dependencies to execute core features, with clear guidelines on platform specific configurations required as well, explained ahead to help setup.

To install the software use python `pip package management utility,` execute this: `pip install -e .` This is the recommended installation procedure as using ` -e` installs a development installation, where your modifications inside project's repository will instantly effect on installed modules and packages in system. The installation can sometimes get complicated, requiring some external library to install which depends heavily on platform setup details; in which instance, consult with error-messages or look to documentation in `INSTALL.md` in repo, which is generated along-side with source directory during clone/download process to provide clear guidance in case such scenario arise for platform configuration specifics or installation dependencies.

The `requirements.txt` file in the repository contains the required Python libraries, dependencies and tools that can be install automatically, simply through `pip` installation procedure to install packages mentioned to avoid compatibility or installation problems on a particular operating system, especially for dependencies with C extensions, and/or specific platform configurations and dependencies.  It will ensure correct dependencies and required libraries versions aligned, preventing conflicts, enabling seamless data flow, and maximizing system reliability with a stable operational environment to run the program smoothly across varied modern computer platforms with minimal hiccups, as tested in several test runs and platform configurations before release, to assure consistent results as possible in the data analysis pipeline process

Platform-specific installations often necessitate particular package variants to cater unique system attributes such as memory or architecture. For example, installing certain dependencies might mandate a compiler ready for compiling C extension, on MacOS or specific distributions for Windows to ensure the code compiles smoothly and executes seamlessly on targeted platforms, preventing unexpected behavior or failures at program execution during installation phase and ensuring smooth integration to avoid future headaches during operational runtime and execution. Check the specific library documentation, as it often includes details and platform dependent instructions, if problems are detected with installing basic requirements in ` requirements.txt`, as the documentation might suggest a better setup procedure specific to that library for better integration into current running operating systems and architectures to maximize overall efficiency and reliability of program operations as expected, while also preventing errors later-on

For macOS users, installing required packages sometimes may necessitate Homebrew as underlying system manager due to some platform compatibility constraints, so check your existing setup with brew installed, then try install dependencies through that if installation issues appear in regular method as otherwise you can execute regular python installations via using pip command as outlined, while ensuring your terminal session is properly setup, or running through the IDE or custom virtual envs that is preffered during coding to help with dependencies management as described previously. 

For Linux system administrators can rely standard system packaging tools to assist with resolving dependency and compatibility problems. For instance apt in Debian distributions will provide an efficient method, which is similar, and also can resolve dependency conflicts by automatically managing packages as necessary in resolving any conflicts arising when integrating external components. Windows users may consider using tools, as anaconda environment management tool can greatly reduce potential setup and management complexities by streamlining the dependencies resolution and isolating packages. These are valuable methods to manage project's environments and maintain stability and consistency when working on a wide variety of system and platform environments across varied operating conditions and configurations during data ingestion or processing.

The `venv -- without virtual environments are NOT encouraged.` to prevent global package installations from potentially impacting different development or system-level projects and to ensure isolated project dependency management, which minimizes version incompatibility concerns when deploying across diverse operating environment settings and system setup conditions to maintain project isolation for consistent execution and deployment as recommended for software development practices for long-term system reliability with minimized maintenance efforts during production operation stages and lifecycle as standard development workflow.



## Usage Instructions

After successful installation you are good to use it by invoking its entry-point, `sidelook`, using `python -m sidelook <configuration file name>`. The `sidelook` program takes your input config-file that dictates data locations and processing stages to define pipeline flow as a starting parameter for its functionality to start operating with a configuration. This is the basic command to execute and launch core data flow.  It reads configurations and then initiates processes accordingly in a defined fashion, ensuring all necessary resources for operation such as memory and computing availability before launching any core data flow pipelines

First make sure you are at correct working location with your `Data-Processor` installed properly, using a virtual python environment is strongly preferred for project-dependent configurations that isolates it to your project's directory so external packages are unavailable during execution unless it specified during the setup to maintain a controlled and stable operating environment with predictable dependencies as required, avoiding unexpected runtime or dependency conflicts with global package settings during operations across varying platforms to guarantee system performance and stability. It helps isolate environments to maintain a more predictable execution process during deployments to avoid external packages from impacting overall operation as standard software deployment practices.

You may navigate into example-config in `example-config` subdirectory, where they showcase common usecases. The file can serve a basis from the example configuration that contains the most secure and recommended usage, so feel free modify for customization and experiment. Basic configurations will be enough initially.

Example invocation `python -m sidelook config/data-analysis-pipeline.yaml` starts execution pipeline from provided configuration. `sidelook` script parses YAML-style configs file that outlines input datasets location(s), output paths for processing steps and transformation stages for processing pipeline.

The script provides detailed outputs in a `results` or specified `outputs-path` within your configs which is configurable, that outlines stages progress as operations are underway for pipeline operations that will indicate status with error if it arises at certain step to facilitate easier monitoring during operations or for automated logging, with comprehensive error-logs.

More complex processing steps involve executing several sub-steps such as data filtering using a defined set filters, transformation steps to change the datasets from raw-input form into a normalized standardized structure that will enable easy comparison of observations or analysis steps such and feature extraction, as the configurations can include these in various pipelines for custom analyses or data explorations based requirements that enable efficient execution to generate useful insight with customized pipelines, as needed to meet the project goals as a requirement

Beyond the configuration pipeline there is `sidelook-test.py` file that  contains a test pipeline, to check core processing steps, with example usage instructions on executing a test flow with example configuration that demonstrates pipeline execution which can act like quick sanity test to verify all necessary packages work properly to ensure smooth running operations.

## Configuration

Stellar Insights uses YAML-formatted configuration files to define processing pipelines. Each config file typically consists of several main elements to ensure pipeline operation runs with minimal setup required from end user

Firstly you need specify an "input_path" which contains paths for data files which are going to get read as raw observations from different observatories. The path must be accessible by system to allow data flow for proper functioning and operation as the program runs on specified operating platforms for efficient execution during data pipeline operations for scientific purposes. Input can either a single file path for simplicity to enable easy setup or directories containing all observational files to be incorporated within a larger-dataset analysis, as required, depending dataset volume requirements and project configurations needs during data flow.

`transformations` parameter are key component, specifying transformation stages applied after initial loading, with a chainable order which only can happen sequentially in this pipeline, allowing to implement various processing methods, as needed with different stages and functions to meet project specific scientific research objectives and requirements in various observational data. Each stage contains the module to utilize with function to implement processing using pre-built functionalities for normalization steps or feature generation or filtering as well with a list for specified functions as well and parameter settings.  Parameters passed to function will enable customization to adjust parameters of operations during transformations.

Furthermore "output_path" indicates directory where final analyzed processed datasets should save to and it will automatically generated, and it also allows specification output name and format which are critical aspects to ensure datasets compatibility during later operations or data analyses and also to ensure that output datasets are correctly accessible in the intended locations in system during data operations or for later retrieval or integration.

"parameters" key specifies parameters applicable for transformations steps where values can override values configured within function definitions in code modules, and this offers additional levels flexibility for customization that allows researchers fine tune operations during data pipeline stages for tailored data explorations and specific scientific research needs in customized data pipelines during scientific investigations for improved results in operations, ensuring data pipeline runs optimally and provides useful insights

For instance: to filter data using custom parameters: "transformations: filtering.custom_filter_by_snr", parameters can adjust `snr_threshold`. This configuration provides high degree flexibility in defining pipelines that caters diverse requirements in data operations as standard data exploration pipeline setup and management, enabling efficient execution of tailored analysis and processing as standard development workflows in various platforms as specified during configurations setup in a modular way

For initial setups you should use example-configs to get a clear grasp how Stellar Insights is being used to generate and operate pipelines. They offer a good template basis with various parameters settings in the pipelines configurations that provide an efficient and effective framework, enabling streamlined scientific analysis and data discovery with customizable settings and options to meet varied project research demands for tailored scientific insights generation during operational processes in overall workflows as part of development cycle

## Project Structure

The structure is carefully designed with modules, configs & examples. It facilitates modular extension for ease-of-usage & development with clearly labeled functions and directories to allow for easy debugging

*   `src/` Contains the core python scripts for reading data formats such as .fits as astronomical observation standards. Data cleaning is conducted in `clean.py` and data analysis is handled in `analytics.py` while processing functions lie inside a module named processing\_modules. This allows the main functionality is easily accessible for integration within pipelines, and it helps separate different aspects.
*   `tests/` Unit and integration tests for various processing modules are located to provide confidence of program reliability to enable consistent operations in diverse environments and operating setups and systems
*   `configs/` This stores YAML files that are needed as configs for data ingestion, cleaning processing pipelines that provide customization for users for easy configuration during usage, while it also serves templates that provide quick setups with example configurations, for various pipelines, enabling rapid pipeline deployment with customizable parameter sets
*   `data/` Sample files or data files required during development and debugging stages, to verify proper pipeline operations or functions and testing friendly environment supports, while enabling smooth operations across multiple configurations in a development-testing environment as well and for mitigating setup issues for easier debugging
*   `interface` User-interface scripts are kept within, which provides an intuitive command line usage, for executing processing steps, and pipeline operations from a console for streamlined control.  Entrypoints for program execution such functions and commands that users directly interact within the terminal to initiate data-pipeline execution.
*   `scripts/` This houses the primary executable scripts, for pipeline initiation such that `sidelook`, the main execution command is placed.

## Contributing

We appreciate collaboration from other researchers to make data pipelines easier. Please submit issues detailing any observed error conditions while working on code to allow for faster resolution of identified concerns for overall improvements for software and pipeline reliability as needed with a quick fix

Before opening PR please create branches. Follow Python coding guidelines in coding and PEP8, which ensures a unified development style for easy review of PR for faster integrations as needed with automated lint checks. Add clear and descriptive messages beforehand as they serve an indicator during merging operations for documentation, as the messages will reflect what changes the changes were introduced, to help mitigate integration-caused-risks and errors for future development and deployment stages in an agile and scalable approach for streamlined integration process and improved development practices and overall team productivity

Create comprehensive unit tests. Ensure that new functionality, and changes to core logic has accompanying test coverage and test scripts. Test code should have sufficient coverage to verify the code functionality to enable robust integration to mitigate regression bugs in integration cycles during deployment operations in standardized manner as a development standard best practice and also to ensure long-term operational consistency in various systems, as a part of standard operational processes, ensuring consistent testable workflows as a core requirement during code integration for quality and performance and stability and robustness in software development.

Testing should cover the most common operations while edge-case scenarios are also included to guarantee code quality.

## License

Stellar Insights is licensed under the **MIT License**.
---
This license permits freely usage of our data-process software for non-commercial and commerical purpose while maintaining a copy with license text as part distribution of this open source program for attribution, to allow proper attribution of software as well during distribution stages

By making the software code publicly and available, Stellar Insights grants the flexibility of adapting or changing to explore the potential scientific advancements as required. However users must ensure that original authors are given due acknowledgment during redistribution for recognition of the open science principles to encourage collaboration within astronomy community, fostering open knowledge, promoting innovation and ensuring transparency of data usage within astronomy community with minimal restriction, while still providing attribution and appropriate usage terms.

