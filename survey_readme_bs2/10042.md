# Distributed Job Execution Engine "SparkJobExecutor vX.XX (SJE)"

### Overview & Design Principles

The `SJE` provides an asynchronous framework to distribute long-run time intensive tasks/ jobs amongst several execution worker machines/nodes within an infrastructure. The core architecture consists of an Executor, Task Manager and Worker node. Each job/task is represented within this ecosystem via JSON objects. These object descriptions are transmitted to available Executor nodes and executed in the background to provide a near zero wait experience for clients requesting task execution

We have designed the SJE around principles for reliability, efficiency and scalability: Task prioritization, automated worker node allocation as a means to handle variable workload conditions as the infrastructure changes. The Executor will monitor worker status and automatically re-queue failed and incomplete work items for re processing and re-routing to an online and responsive worker.

SparkJob executor leverages a microservices approach to provide a distributed architecture with fault tolerance and load balacning built-ind within its execution.  Communication between Executor components will be handled using Rabbit MQ message-queue, with a central database (SQL or No-SQ database depending on data persistence needs) to store execution metadata as the work item completes or errors out, enabling audit trails or reporting.

We have prioritized utilizing a plug in based execution strategy so the core Executor component remains stable whilst the task processing logic (Spark applications in many circumstances) gets updated independently as code evolves and/ or different tasks are needed to execute.

This approach minimizes dependencies within the execution framework whilst supporting rapid task processing development as requirements shift and adapt over time for our customers' needs.




### **1. Installation Instructions**

First off, make certain you' ve got the necessary prerequisite technologies setup on your build machine and/ or execution infrastructure. You will at least require: Go version >= `1.19`, `RabbitMQ`, an SQL/ or No -SQL compliant DB instance (PostgreSQL or Mong0 DB recommended). Ensure your network can facilitate inter-node and worker connectivity

Next up we require you to download the Sparkjob executor package and install into its target destination folder on the execution server. This step assumes you are on your build host or local development system with network connectivity to our release channel. We recommend you create your own release channel as your app matures.  

Then we require you install the dependencies required for the build.  You can use either go modules (preferred approach for managing dependencies and version control) or `gobuild` directly if you' re already set with that setup:  Run the following command in your directory where your `go.mod` lives to update and fetch the dependencies from their sources. This command ensures all required components are available

```bash 0
go mod download all
```

You will need the `Rabbit MQ server ` to operate. You need to set that up first if this hasnt already been deployed for other services within the infrastructure that you are setting up this component into. Please note there' re various options for installing RabbitMQ - Docker is a quick and reliable approach to deploy

For Linux/Mac environments the setup process is simple - use your system `package manager` for RabbitMQ install. Example with `brew` is as such:

``` bash 0 #Linux/Mac OS setup command.  
brew install rabbitmq
# Follow further prompts
 ```

For `Windows OS ` setup is required via their official download and installer from `www. rabbitMQ dot com`. This will require you set up user permissions and access as a service, to run correctly.  You' will typically set this to be auto run at boot.

You can then deploy and initialize Sparkjob executors by simply running the compiled `sje- executor ` program you downloaded into the destination directory on target infrastructure. It will automatically start and look at `s je -config. YAML ` for its configurations to determine its connection to other nodes in RabbitMQ

Finally we suggest you run your `SJE ` unit test package and functional integrations after setup is complete and confirm all services within the cluster have properly started with no errors.




 	### **Installation Steps**

Download source files and/ or precompiled artifacts to your build host or local development system from the `Github` repo and follow instructions above for setting up dependencies to be ready for build.  You can then `gitclone or download` to your directory

Then we will run the code generation tools.  You may have code templates to generate the RabbitMQ bindings and message queues to support execution. We suggest that we run these once before the actual code compilation process to minimize potential build errors during the build/ deployment phase and ensure proper structure

The next key is to run `gob build` and package into distribution package, that you can install to production machines. You may want to use tools that assist packaging your Go application for cross-platform deployments

We then will create the deployment folder where binaries will get installed for deployment to various nodes, including `bin/ `, `lib/ ` or even `etc/ `.  These are common deployment folders

You should configure `S JE .CONFIG.YAML to point to RabbitMQ and DB connection strings to facilitate communication between services within the cluster`. Ensure that these parameters have correct permissions to read/ access the configuration information.  

You may also create and initialize an SQL /or No -SQL compliant `DB`, that stores execution information to facilitate audit and debugging of work items and tasks as they execute and/ or terminate successfully and/or failure.  




	 	 	### **Configuration File Location**
 			 	The configuration YAML is found by `SJE`, when it initializes and determines if it should use local path to ` SJE.CONFIG .YAML` ,or pull from a configuration service such as a Git repo or Vault . We strongly recommend Vault if production deployments are planned as an added layer of security

		 ###  **Database Setup** 

 				 			The SQL/ Non SQL DB must exist and be accessible for the executor to store job data as the execution progresses in the infrastructure cluster. 

		  ### **Network Ports Required** 



 		 	 			Port of Rabbit MQ (standard `5672` is the most common). 
 					Port of executor service (typically `8082`) 
					If database uses a port that's not common then configure accordingly.  
 					Any external API endpoints your jobs need to reach (ensure these are reachable from executor machines).




### **2. Usage Instructions**

The primary interaction point is submitting new tasks via the REST APIs exposed by `SJE`. Tasks must be submitted through `http:// <executor- host name>: `8 `082 /task `, with a valid `JSON object as a request body `

The `JSON format should conform` to the `task description standard `, including information on task type, Spark application details, parameters and dependencies if applicable to the job execution process. `S J E` will queue these tasks, allocate appropriate executors, manage the execution lifecycle and report on the status

To view running and/ or completed job/ task information you can use ` http // : <executor - hostname >:8082 /task /< task -id >`. This call retrieves status of the job and provides insights for auditing or debugging

The REST service also offers a `healthcheck endpoint`, exposed as `/health`, that will provide a `2 2 status `if ` S J E ` is up and connected, `5 0 0 error` if there are any issues connecting and running properly 

Here' re a few more common usage patterns to get started:

``` bash  #Submit a job
curl http://<executor-host>: 	 80 82/task -d @my_job.json -H 'Content-Type: application/json'

# View a job's status
 curl http://<executor-host>:8 082/task/123 4
``` 	

For advanced operations the `S  J E ` exposes a set of management APIs accessible via `http //  : <executor - name >:80 82/mgmt` that allow admins to monitor the cluster, manage resources and adjust parameters for optimal performance

The executor service also supports logging and monitoring of the tasks, so you can monitor job status in real-time via dashboards 
  




	### **Example Task Definition JSON** 

 	 		 		```json
 			 		 {
 					 "jobId ": "abc-xyz -123"
 						 "jobType": " SparkApp"
 						 "sparkAppPath":"/user/ app/ spark - app. jar"
						" parameters": {
						 "param1": "value1"
 						  "param2": 123 }
 						 "dependencies":["library1. jar","library2 jar"]}
					}
 				 	```

		 	### **Error Handling Examples**

 	 		 			Invalid request body (missing job type) will return error code `422`.
 				 		Job failed (Spark application exited with errors) `500`.  
 					Job successfully ran with output `200`

		 ### **Advanced Usage: Batch Task Creation** 
 				 				 					You also allow bulk task submissions via a `POST` to ` / tasks` to improve performance when processing large batches




  ### **Authentication** 

 					 				Authentication is currently `NONE`. We strongly recommend you add API keys or other means for authentication to protect against unauthorized access 



### **3. Configuration**

The ` S J E ` uses `S J E .CONFIG . YAML` to manage its operational configuration. `Key properties within the configuration are the Rabbit MQ `connection parameters, DB connection `parameters`, executor node configuration details, worker capacity allocation rules etc. `

The configuration can also pull configurations from a Vault or external service for more secure operational deployments. This configuration can be overridden through environment variables.

You should not edit configurations manually but update them through your configuration services (e.g ` git ) to prevent manual inconsistencies. 

Environment variables will be checked first if the YAML files are unavailable 

	 	### **Configuration Sections** 

 	 		 		`rabbitMqConfig ` section: Defines how SJE is connection `s the RabbitMq message service to receive messages.	  
 					` dbConfig`: Connection information on what data to store in DB and its location for storing `task data . `
						`executorNodeConfig` : This dictates where SJE executes the worker tasks. This includes things such as allocated CPUs , memory 		 			 			   	allocation.	   		   			
 				`monitoring` guides what logging is enabled.
  	 	### **Configuration Example (sje-config.yaml)** 
	 ```yaml
     rabbitMqConfig :
     host : " rabbit mq host 1 "
     port : 5672
 			 		 username : rabbit mq User

      dbConfig :
     type :  PostGresSQL
    	host: PG Server address

   ```



### **4. Project Structure**

The `S  J E ` codebase adheres to modular and componentised approach 

- `/cmd/`: This contains code responsible for application `execution`.
	`/pkg/` Contains business Logic

	 			  	 			 -`/ pkg / executors/`: Responsible for executing the tasks and handling communication between RabbitMq 	 -  	 -` /pkg / manager` handles managing tasks in the queue and managing task allocations across executor.
  `/pkg /worker:`: Code and functions for handling tasks locally and interacting directly.	- 	

  -`/ api /`: Contains HTTP API endpoints
  
 - `/configs/: This includes the various configurations and configs that guide service operations .`
`- / internal `/ Contains code which isn not meant to exported.



-`/ docs /`  : Includes any other relevant docs for this component

 	 ###  **Tests**	 

 		 -	Tests directory tracks various unit/integration &acceptance Tests .



### **5. Contributing**

The SJE team encourages all to engage.  Submit issue to help us resolve errors or bugs and propose any feature enhancement to better meet needs . Please review our Contribution guideline.   
Submit Pull requests. Please submit with detailed explanations, unit-test to verify that code is correct . Follow code style to prevent formatting issues and ensure quality code 

All changes need thorough `tests`.   `PR s need `approval `and merge by team `maintainers. 

Follow existing guidelines and best practice. Please follow all code formatting rules.
   



### **6. License**

This `project `is licensed under the `MIT` license as a permissive free and open software license

You are granted rights and freedoms, including use and modification for your own project purposes or to contribute as improvements 	to this code
Please be responsible in utilizing open sources for business applications



### **7. Acknowledgments**

This work relies upon existing and popular frameworks, namely - RabbitMQ, Postgres/mongo and Golang. Without the work, community support that those tools have received and offered freely to open communities this code may not even come into light 
 		   				 	Thank you and appreciate the developers who work hard 	



### **8. System Architecture**

At it`s root the System `consists` of Executor , task managers `and execution `workers`.
Tasks are ingested from REST service through task management and `queud up `into messages and passed `alongside Executor Nodes`. `Worker `node processes `these and `report success to task Management

Each executor manages it`s own execution context , which can have its memory limits, resource allocations . Executor `can move `between different workers depending `and based `load requirements 

`SJE leverages a distributed `and fault `tolrance mechanism , that ensures high `availablitl` even during failure.  
The entire execution process will report to DB for later `auditing`.

```graphmd
graph TD 
 A[REST Service/ Clients] --> B(Task Manager);
 B --> C{RabbitMQ};
 C --> D[Executor Nodes];
 D --> E(Worker Nodes);
 E-->F[Databases] 
```

### **9. API Reference**

* **`GET /health`**: 
-	 Returns a healthcheck to show system operational state	
 *   `Request `Params None - No Parameters

- 	 `20 returns 0 `Success 	- 	  `Returns code if healthy `5 - code failure
	  
-	 **`POST/tasks**: Submit tasks to queue for processing	-	

- ` Request Param JSON body` - `SJE `Task Object description -  Job IDs/ task Type, Spark applications to load 		 -	 
-	 ` Response : Task -Id,  confirmation `

* `GET / task/{TaskId}} `: View task statuses 	 -   		  Returns Status 	 - `4- code `
  Request Parameter task - Id

*` POST/ /Task `: Submit a job 		 			

  *	 * *

		 *

 ### **10. Testing**
	 		
Run `make test - cover profile  -cover-out coverage-report  to test `the project
Unit test will cover most critical logic , integrations cover integration across multiple microservicies - acceptance cover all scenarios 		
`
 		`Test reports available via  coverage-reports 
### **11. Troubleshooting**

Issue `DB connect error :  Ensure that database and host are correct - verify connection strings
Executor failing due to lack ` of resources. Ensure there enough `cpu/ mem to handle workloads.
Job stuck `due` due misunderstand or errors during processing ` Check spark applications` for potential issues



### **12. Performance and Optimization**

Consider scaling up resources when there `s high- volume of jobs 	  Use caching mechanism 	
		`Use concurrent processes 	to run task parallel.



### **13. Security Considerations**
Secure the rabbit-MQ `endpoint with credentials.   Use HTTPS for the API endpoints and enforce valid access.



### **14. Roadmap**

-	Add Job-scheduling feature 
  Integrate `monitoring 	tool to improve visibilit.  	  Improve error-reporting .		

### **15. FAQ**

Why can not ` I get task processing?  Perhaps connection `strings or configurations wrong. 
Is support available with non Spark applications . `  Support can added as plugins 




### **16. Citation**

`Please include `the folliwing BibTex info to provide attribution for our `work`.	 `


### 17. Contact**

Contact team to help you. Reach them through GitHub. 




