# Distributed Job Execution Engine "SparkJobExecutor vX.XX (SJE)"

### Overview & Design Principles

The `SJE` provides an asynchronous framework to distribute long-run time intensive tasks/ jobs amongst several execution worker machines/nodes within an infrastructure. The core architecture consists of an Executor, Task Manager and Worker node. Each job/task is represented within this ecosystem via JSON data structures, allowing easy portability amongst worker node instances and enabling easy scaling and management of job resources within an overall infrastructure deployment

The engine uses zero configuration deployments, leveraging the native infrastructure and its existing security constructs. Each Executor and Task Manger can run anywhere with appropriate network access to the cluster's worker pool nodes, minimizing operational burdens.  

SJE is designed primarily towards data processing tasks and is highly modular and flexible with a plug in interface which provides easy extension for specific task-dependent operations (e.g., logging, persistence, monitoring).  

Furthermore it supports various task dependency configurations, ensuring that tasks run sequentially or parallel, allowing developers the control required to orchestrate complex data workflows, ensuring reliable and scalable processing pipelines, reducing bottlenecks, minimizing costs by distributing workloads.

We leverage a publish-subscription architecture for communication among components allowing event- driven decoupling, simplifying system management and scalability. This enables easy extension, addition to new tasks, scaling, and integration of additional systems and features as our infrastructure evolves.
 
Spark Job Executor allows the decoupling of computationally expensive, long-running jobs from primary request processing threads, which ensures that main web/api servers do *not stall.*

 

 
 
 -------------------------------------------------------  
 **Project Title:** SparkJobExecutor vX.XX( SJE) - A Distributed Job Execution System  
 
 ### **1. Description**
 
 The SparkJob Executor vX.XX (hereinafter "SJE") facilitates asynchronous, distributed execution of time-consuming processing jobs/tasks, leveraging a scalable worker network and JSON data structure, designed for data processing tasks where long runtimes might affect core operational servers. 		 		 		

 SJE's modularity is a key advantage; plug-ins allow for task dependent custom behaviors like data logging and monitoring with a plug in based architecture to ensure flexibility. The plug in API ensures that custom behaviors like logging, metrics gathering, task retry/ failure, can readily be applied, making deployment adaptable.  					

 It supports complex workflow orchestrations utilizing dependency resolution for sequential asynchronouse parallel operations. This is accomplished with a publish/subscribe system allowing for loosely coupled task execution, enhancing resilience to component failures.

 Zero config deployments minimize management overhead, leveraging native network/ security protocols, reducing the time to onboard new tasks to an already established worker pool and simplifying the operational burden of a distributed execution engine deployment and management. 

  This architecture minimizes overhead to web servers as long processing activities are delegated to an independent worker pool which enables them to focus primary tasks, reducing the latency of main operational services to clients. This enables high availability and performance under load with a distributed architecture designed for scalability and fault tolerance.  				 				

 -------------------------------------------------------
  **Project Title:** Installation and Configuration Guide

 ###  **Instructions:**

 This section provides installation steps and instructions to configure SJE within an infrastructure. Before initiating installation, review dependencies section below, to assure required environment components exist.

 Firstly create your virtual environments to isolate the project. We recommend `Virtual Env` tool, `conda create -name sparkjob -y python=3.9` This is a common approach but `poetry` or another dependency managmenet tool is acceptable. 

 Activate environment `conda activate sparkjob`

 Install core requirements using package management tool `Pip`, or other preferred approach such `Conda Install`

 The next critical component of your SJE is your message bus/ queue. The project requires access a reliable publish/ subscribe system. This example, Rabbit MQ has shown great performance, though other implementations are acceptable as an alternate solution (Redis Pub/sub, ZeroMQ ).

 Download RabbitMQ and install on an available instance `apt update && apt install rabbitmq-server`. Configure RabbitMQ with the appropriate networking access rules allowing worker nodes access.

 Ensure proper network configuration is in effect, permitting Executor instances to send, receive, and queue messages with the broker service. 			 

 Verify required system libraries are available on each host, including Python interpreter, `json` libraries, `logging `and `requests` modules. The specific dependencies can easily be installed using `PIP requirements.txt install`.	

 Lastly configure environment variable pointing the location of `config. yaml`, the core of configuration settings that drive the behavior of components. The location will typically depend on environment deployment.	  
 
 Configure your RabbitMQ service. Ensure the necessary queues exist, and appropriate exchange bindings for communication. `declare queue` commands are often required within the queue management UI/CLI. 	  			

 Configure user access control and network access restrictions for all nodes. Secure the Rabbit MQ instances and other components to prevent malicious access attempts and safeguard data within the infrastructure.	

  After installation, verify connectivity and communication. Ensure the components interact appropriately and that task execution flows as intended via the message broker instance within your environment and deployment setup.

 Test your environment configuration, to confirm all dependencies exist, the correct versions of the libraries and dependencies installed within the environment and Rabbit MQ instances are operational with connectivity enabled, before deploying tasks into your operational pipeline environment.

 The final step to setup is defining the required task configuration within `config.yaml`, including Rabbit MQ credentials and other task specific configurations.

 -------------------------------------------------------
 		 
 ### Usage Instructions

 	 To run a task within Spark Job Executor first, configure and deploy your Executor instance within your environment and deployment setup.	 

 Once Executor service deployed to a running host within the environment.  You can now invoke task via an POST request with `JSON body.`
 To invoke your task, create JSON formatted structure representing your job and send POST request with that structure:  `curl --data '{"task_id":"task_1", "function": "calculate_data", "params":{"input_data": [...], ...}}' http://<executor-host>: 	 8080/submit 		`.

 Monitor task progress through Executor logs and message queues, observing task states and completion events for a better workflow overview.

 For advanced configurations and dependencies define the `task_dependencies` within your `configuration file`, ensuring that tasks run sequentially as a chain of asynchronous operations, allowing workflow management. 		 				

 The engine supports event- driven processing, with events triggered on task success and failure enabling real- time notifications of task state updates, allowing integration with monitoring services.	  
 
 Error handling is a key element of task processing. When exceptions are caught, they will trigger error notifications, alerting the operators to issues requiring resolution within the operational environment, enabling efficient problem resolution 				

 The engine offers REST API, including `/status <TASK_ID` endpoint for monitoring, `/cancel 	 <TASK_ID>` and other management endpoints, which are useful during debugging and troubleshooting. 						

 Task dependencies can also be handled within code, enabling the task to perform additional operations based off of previous executions.

 -------------------------------------------------------
 

 ### Configuration

 SJE is configured primarily via `config.yaml` file, containing task settings that drive its behavior within the overall operational environment. 

 `RabbitMQ` settings include hostname, port, username, and password to establish connectivity, authentication credentials, to ensure communication with broker instance 	

 `Executor` configuration parameters like `worker_nodes` list and `max_concurrency`, define which nodes can execute jobs in a worker node list, and number of workers running concurrently on the host.	

 Task- Specific parameters, including `retry policy`, timeout, `logging levels` can be customized based upon workload characteristics and requirements of operational infrastructure.	  			

 `Plugins configurations`, allow enabling or disabling features, customizing behavior of modules and plugins, to enhance functionality as required and tailored within the infrastructure.	 		

 Security configurations include `authentication methods`, `authorization scopes`, and `SSL settings`, which are important to secure the execution engine with the proper controls.		 

 Environment variables can override YAML configuration settings, allowing externalized configuration for deployment in dynamic, infrastructure environments.	 		

 Logging configuration, define `logging levels`, `output destinations` and formatting to enhance debugging and monitoring efforts. 						 

 The `task_dependency configuration` enables chaining of tasks for workflows. Define a sequence of tasks and order, allowing complex data flow pipelines. 

 `Monitoring integrations `, include metrics collection configurations and alert rules to facilitate proactive issue detection, alerting operators, enabling efficient problem resolution.	

 Finally, review your overall configuration file thoroughly and perform testing to identify unexpected interactions between different components within operational environment.		  
 
 To adjust logging levels modify the `log_level configuration` and apply to relevant logging components in the configuration file.	

 Ensure the RabbitMQ connection details are correct and that user/ password are valid for authentication.	

 Verify `worker_ node list` accurately reflects all available execution nodes.  Incorrect configurations will cause execution failures.				

 ------------------------------------------------ -------
 ### Project Structure

 The SJE project follows a modular structure to ensure flexibility and extensibility. This section outlines core elements of project architecture.

 The `src 			 /` directory holds all the project source code including `Executor`, `Task Manager`, and `Worker Node implementations` as core execution components.

 The `plugins `/ directory contain all plugin source code, enabling custom behaviors and extensibility. 		

 `tests/ ` contain unit, integration and regression test to verify functionality as well provide quality gate assurance to project maintain contributor efforts	

 The `config 			 /` directory contain configuration management settings that dictate engine operation within different configurations and operating parameters		 				 				

 The `docker-compose.yml` directory contains all Docker definitions necessary to launch local and/ test operational deployments for quick setup and experimentation		 
 The  `deploy/` directory stores the configuration files for deployments in different infrastructure and cloud setups, to manage the environment for production operations		

  The main file entrypoint is `/Executor /main. py` which initializes the executor components including queue connectivity configuration as part of initial configuration		

   A core element, the file 			 `/Task Manager/manager. py` coordinates the overall processing of jobs.		 
 Finally	 ,	the worker component, implemented within `/Worker / worker.py`, manages execution environment as an operational node, running all task execution.
 --------------------------------------------------------
 ### Contributing
 	 Contributors interested can participate and make suggestions through a standard issue report, describing the bug and/ expected results within appropriate channels 					 				

 When opening an issue or Pull Request ensure the issue description accurately reflect and provides reproduction steps and detailed logs and error trace back to allow efficient analysis			 
 Please read existing code conventions to understand code styles. Submit pull requests containing tests covering all changes for review before acceptance in production environments				

 Before submission ensure that tests cover newly modified functions, to guarantee changes and updates are validated for system operation within the execution infrastructure		 					 		

 The project embraces code coverage testing. Tests must pass the minimum acceptable test thresholds and meet high-quality guidelines within operational standards			 		 		

 To improve the documentation please suggest edits to README documents with suggestions for clarifications or improved content for ease understanding		

 All changes are subject to reviews by the SJE core teams/maintainers as per established guidelines. 					

 The maintainer teams encourage open and productive collaborations, to enable contributions of new features, code optimizations as an ongoing improvement efforts					

 For major contributions consider contacting a contributor team or opening issues prior.

----------------------------------------------------------

### License
SJE is licensed under the Apache 2.0 License. It means users have permissions for using SJE freely within projects or for personal usage as a software library or distribution tool.
It allows users modification within this scope.  
Restrictions: SJE must always display proper acknowledgement, with the Apache 2.0 copyright license and attribution as stated by the project team to comply to terms in full

 ----------------------------------------------------------

### Acknowledgments
Special recognition for contributions of RabbitMQ development group that enable efficient asynchronous operations in the engine deployment 	

  Thanks go to the Python language core team who provides the underlying platform enabling efficient data management for complex workflows within execution engines		
The community provides open sourced logging, networking modules which provide a foundation that enables a modular engine architecture within infrastructure 		

   This system leverages the extensive community work and resources from data storage management tools for operational monitoring capabilities		 
Appreciated the contribution provided and open sourced frameworks within system architecture for efficient resource orchestration as an underlying foundation for task scheduling					

 Thanks is also owed to all community members of the Apache Software Foundation whose work is leveraged within our license and open standards compliance efforts. 					
------------------------------------------------------------

### System Architecture
SJE employs a distributed system architecture comprised of several distinct components that coordinate tasks in distributed environments

The key elements in the core of SJE are an `Executor component, task manager and individual Worker Nodes ` that execute task assignments, ensuring reliable processing across nodes, with efficient workload division
The core communication layer utilizes RabbitMQ to implement Publish Subscribe architecture that decouples task submissions and task assignment. The architecture enables loose connections between various services	 					

Task submission is facilitated using JSON-formatted message published to queue that defines all details related the specific assignment including the function execution to implement

Workers nodes continuously subscribe for tasks to queue. Task is pulled and execution begins, which includes monitoring status and providing metrics and logs as a task execution is processed
Plugins play important function by allowing custom operations such monitoring or retry behaviors as extensions. Allows task behaviors tailored within infrastructure environments
Dataflow proceeds via messages published via queues allowing for flexible routes of data transformation or workflow execution through task execution

Dependencies managed in the YAML configurations to establish the task workflow.	

The architecture promotes flexibility with independent task assignment enabling dynamic workload adjustments within operational infrastructures 		

Monitoring is facilitated utilizing logging framework allowing for comprehensive visibility across components of distributed system, aiding operators.
  	
----------------------------------------------------------

### API Reference
 SJE utilizes an intuitive RESTful API exposing essential management operations as key interface for operational activities

 The API entry point to `Submit Job ` is defined through HTTP POST with path `/<executor_host>:/tasksubmit `, accepting `JSON formatted structure containing the task`

 Task management includes endpoints, `/Task /state/<Task_ID>, /Task / cancel` 	 , providing insights of the task state status of a particular job 			 				

 `Plugin` integration can use cloud specific configurations.

 Error codes include: -`1 -Invalid request format -1- task is unavailable-1 Task not assigned.` - and `Error in worker execution`.	 		

 The Executor API offers documentation as part its service, allowing easy integration into operational tooling as required and providing operational insight		

	  REST interface enables seamless interactions and allows external applications integration to trigger job submission for streamlined management	
  			  
-----------------------------------------------------------

### Testing
	To begin test SJE run test scripts within test folders using Python test management library.	 
To begin, setup a test execution instance using virtual env with requirements set	

  Run testing scripts with appropriate flags using Python library for executing `unittest framework`, and `test coverage reports`, to measure quality of the execution code.
To simulate RabbitMQ instances and external systems we create Mock environments, isolating the code and reducing test dependency complexity for faster operation	

 Test scripts should adhere the existing testing convention. To improve coverage and robustness we recommend implementing more integration or unit to test complex components of architecture			 		

  We recommend running automated unit/ Integration tests using automated deployment system as part Continuous Deployment Pipeline for automated verification	
The SJE test scripts can use test configurations that provide a framework within an environment and can easily be adjusted as part test iterations

 We suggest that new contributors provide new testing coverage as part their submission.			 		

 The goal to maintain robust and quality system, by automating and proactively running all test as an ongoing quality management initiative within development teams	

  To enable rapid test executions create a local development instance and configure it appropriately within a Docker setup as recommended by deployment teams

 -----------------------------------------------------------

### Troubleshooting
	 When an SJE task fails, review error log to determine reason, particularly those found by worker nodes	

  A `network communication error occurs `, if executor instances do not reach brokers or the worker network, and this may be because firewalls prevent access, confirm appropriate access control configurations
When tasks time out verify the Rabbit MQ queue is correctly populated. Check task configuration parameters such as execution limit	 					 
 If the execution worker crashes examine execution environment configurations.

 When an unexpected behavior occurs verify RabbitMQ queue and message format is correct as a key source in the overall task flow
 To solve worker issues try increasing memory allocation as it often leads failures in execution and/ processing operations within environments		 		 	

 When an application does fail verify configurations. The configurations must have access, correct privileges within an operational network and environment	

 For configuration conflicts use configuration overrides as environment parameters. 					

 Check version dependency for Python interpreter to ensure they match project dependencies	
	 To solve logging failures review logging settings to see what actions need adjustment and what is needed in a configuration update to solve the issues		 				
If task is handed back improperly check queue bindings and RabbitMQ permissions and network settings, and ensure that the proper credentials/ connections	

   For more troubleshooting steps refer detailed document within `Documentation folder.`		

----------------------------------------------------------

### Performance and Optimization
Optimize task by ensuring minimal resource requirements by utilizing a efficient data transforms within task operations
Reduce communication cost via batch data aggregation before publishing message on a broker to optimize the messaging operations in task processing pipeline
Employ efficient data structures within Python scripts for fast memory allocations for iterative data transforms
Optimize database query with proper indexing within a task processing operations.
Implement task dependencies carefully and optimize concurrency, ensuring no bottlenecks will impact operational efficiency in data workflows.

Leverage multi-processing to increase processing speeds, but limit concurrency count and prevent overload.
Monitor metrics such as execution time of the tasks, resource utilization of task worker machines and network I/O operations, using monitoring systems.		 					 
	  Optimize by tuning system memory configurations for optimal resource efficiency
Consider asynchronous data transfers to prevent delays during task operations in an optimized execution framework		 	

 Cache common results within internal data storage, improving processing time for repetitive executions
Use appropriate algorithm for tasks that optimize speeds with a focus and consideration of operational efficiency in processing data
  

--------------------------------------------------------

### Security Considerations
Ensure that all connections within SJE are encrypted by using transport-level protection and TLS to maintain integrity for networked transmissions 	

 Implement strict role based user access restrictions for access controls and authorization restrictions within system permissions and data storage access		

 Secure external API and external API by using proper authentication procedures. Ensure API endpoints use strong validation, filtering, to defend potential injections.
Implement strong passwords as authentication mechanism.

 Protect against potential malicious data by using proper code and sanitize inputs as required
	 Secure environment credentials and store keys and API access safely using environment configurations, to limit sensitive key management within the project

  Implement intrusion systems and monitoring logs, regularly scanning system vulnerabilities and promptly patching any of detected system failures	

 Provide training to the staff as they work, allow operators to handle incidents and vulnerabilities as part the training programs.	

 Ensure secure code practices. Follow Python best coding conventions and code reviews for code quality assurance and operational standards
Regular audits will provide a comprehensive system overview of operational and network controls within system infrastructure 	 

 Implement automated scanning tools and automated patch procedures and update security configurations as new threats occur to maintain an optimal environment		

 ----------------------------------------------------------
 ### Roadmap
The project will focus to support a distributed cache mechanism and to enhance performance with task caching in subsequent versions of SJE
The project roadmap plans support dynamic resource assignment for scaling up execution environment, based real-time demands for workload requirements
Future release is scheduled for task dependency visualization to help operational manage and debug the dependencies in task processing flow
Support additional orchestration framework is in the plans and scheduled with modular architecture extensions, to improve overall SJE capabilities and scalability 	
	
The roadmap is committed for improved API with new functionality to manage workers nodes.
SJE also planned with a web dashboard for centralized monitoring task, providing better overview of overall performance.		

  Planned feature will allow dynamic plugin configuration to reduce manual setup steps, enhancing SJE usability 	
SJE future releases is expected for increased integration support across other services and third party platforms with modular and flexible extensions				

 SJE also intends support additional broker types such Redis and ZMQ to broaden deployment environments with a variety configurations		

-----------------------------------------------------------

### FAQ
Why the task not executing and received an uncatchable execution state error, check your worker access configuration for proper credentials and connectivity within infrastructure deployment. 					 				
What can you do on failure for a failed Task if you can't determine the exact reason?, Configure retries with a delay between execution retries. 

Why the queue appears as dead letters when submitting Task requests? Verify connectivity, connectivity rules, authentication, authorization configurations with message Broker service 		 		

How to optimize execution and performance with SJE?, Ensure minimal task resources, optimize concurrency limits to improve data transfer and processing efficiency within operational infrastructure		
 			 			 	

----------------------------------------------------------

### Citation
SparkJobExecutor can be properly attributed within any academic/research setting using a common publication method such: ABC project - SJE - VX.XX version available through the following software repositories [repository address] [date access of source ]

  The citation helps promote the software within scientific publications with attribution requirements of code usage 
To reference code for operations research consider referencing as [Author name/s ] [Project Title]. The source code repositories for SJE will allow researchers to access source as it enables the ability review of source for verification 					
		

-----------------------------------------------------------

### Contact
	 For issue submissions open issues in public SJE project within Git source hosting.  
If you require additional support please join project's public forum and community discussion channel for real-time support.  					
To submit requests/ suggestions send to developer contact email or submit issue through external reporting tool	
If security vulnerabilities detected notify the core SJE maintenance/support channel to allow for remediation.		 		 		
 				 				