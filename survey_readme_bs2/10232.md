# Project Phoenix - An Event-Driven Data Pipeline Framework

## 1. Introduction and Overview of Project Phoenix (Description Paragraphs - Goal of Framework & Function of the event loop architecture and its use of Python and Kubernetes to provide modular event handling with automated orchestration capabilities )
**Project "Phoenix"* is an orchestration framework, facilitating event- driven processing, primarily intended around the automation of the creation, deployment of pipelines and the efficient processing of event data.** The goal here isn&#x20;

;s the simplification of complex workflows that need automated handling in a highly modular, and flexible fashion. Phoenix aims specifically in creating modular data handling workflows. It leverages a core Kubernetes based system of containers that can run independently or in a chain of events triggered from external sources like Kafka, SQS queues or other event sources. It's based on python and leverages it to orchestrate pipelines of containers and their events with minimal setup or overhead

At its core, it features a distributed event loop architecture which monitors configured event sources for new events to process, triggering the necessary processing components and routing data to appropriate locations based on rules. The Python code handles this loop and defines the pipeline components and their connections.  The framework is highly adaptable allowing custom modules and event handling routines for specific needs, enabling easy expansion and specialization.

The design prioritizes containerization, leveraging Docker containers, orchestrated by Kubernetes, for event handlers to promote isolation, consistency, and ease of deployment. These Kubernetes containers are created from prebuilt templates or user created ones, allowing rapid pipeline construction, testing, validation and deployment. The system supports automatic scaling and self-healing capabilities through Kubernetes, providing high availability and fault tolerance.

Project Phoenix is relevant for organizations needing to manage and react to continuous event streams.  Typical use cases include real-time data transformations, IoT device data ingestion and processing, log analytics, or automating business process orchestration based on external events. Its modularity promotes iterative development, allowing new event handling capabilities added incrementally.

Finally, project Phoenix supports an easy integration for third-party data storage or analysis components such as Apache Spark, Elasticsearch or any service capable of integrating to Kubernetes pods for ease of modular event routing based upon defined event attributes.

## 2. Installation Instructions

Before deploying project Phoenix ensure prerequisites like `kubectl`, and a `kubernetes cluster`.  A suitable `kubectl` is also critical so verify your setup is configured correctly, using a configuration like minikube is a quick option, while using any other k8s service such and cloud deployment such as GCP's kubernetes or AKS or EKS on AWS can provide a more complex production like deployment. This detailed documentation describes a deployment for development on `minikube` with Python v3.8+, Kubernetes 1.23+.

Once the prerequisites are satisfied you must ensure Python 3.8 or greater is available as all the code will need the dependencies available through PIP which can also use a Python container, such as one provided within your k8s setup and that Python has network and k8s connectivity in order to execute against k8s pods and resources

The Phoenix installation begins by downloading the code repository, which may be achieved using GIT. You must clone this entire repository. You then move into the repository's base folder using the CLI with `cd`

After the initial git clone and repository navigation the dependencies will need to be pulled into your system to support this framework which are specified using pip, this is typically performed as: `pip install -r requirements.txt` in the project&#x20;

;s base directory, which is the `requirements.txt`. You need python and `pip` for the framework. These will pull necessary dependencies such as k8s python SDK to interact and manage deployments to your Kubernetes deployment

The Kubernetes Manifests directory within Phoenix is crucial and is used in deployments for creating necessary pods for Phoenix to manage itself which includes a central controller pod, associated configuration pods. It also uses service deployments in K8s which will handle the communication of external clients with phoenix components which are deployed using YAML definitions, these can then be loaded with commands: `kubectl apply -f manifests/deployments` for the basic Phoenix core deployment to a kubernetes cluster

You can deploy it by simply importing those yaml and deploying them to your cluster which are contained within `manifests/kubernetes`. Ensure the kubernetes config `~/.kube/config` contains proper authentication credentials. The framework is also built for cloud deployments so a configuration such as GCP service accounts may work with some minor updates to your YAML

Once applied to a cluster it will take 3 to 5 minutes before phoenix is available which includes a central orchestrator that fetches external events for your data sources or services for routing data to appropriate containers and components. It is recommended that after successful deployement to confirm this by using k8s tools to ensure components running in your pods as defined

To install additional tools needed by Phoenix you can refer to the external dependency documentation. The main dependencies include: deployment frameworks for Python. Additionally you must maintain and regularly update these dependencies using `pip freeze` commands

The framework is configured for a development kubernetes, you might encounter some permission related issues if trying to access the k8s api, so ensure the K8s service configuration and deployment account settings  have necessary authorization access, for deployment of the controller service account needs proper RBAC access roles assigned. This also extends when running with cloud deployments.

You should now ensure all necessary services are running properly and the initial controller has launched. It can be observed with K8s using `kubectl get pods`

The deployment and configurations detailed here can all be updated for a production k8s cluster using some YAML and K8s configurations

## 3. Usage Instructions

Phoenix's event pipeline creation uses Python scripting with a defined `event.py` configuration which must follow strict conventions which allows easy event definition creation, this defines what kind of external triggers to observe and where they originate and what the ruleset should be applied in processing events which also handles authentication for the source events

First the user configures an Event configuration. An example `event.py` configuration might include:  event type "user.signedup" sourced from "kafka://<hostname>:<port>", and with associated authentication using API keys

You create event handler modules that are Python based with functions decorated by an appropriate `@pipeline_route` that can handle an inbound events by parsing relevant event parameters.  For the "user.signedup" events, it should then be mapped into another function or pipeline. It then transforms or validates incoming data, sending the transformed result into another event pipeline.

To test a simple setup you could trigger external services, by manually creating new entries to the kafka source specified by events, and then confirm they route properly in phoenix, you should use a local K8s setup like Minikube to confirm basic connectivity between your pipeline

After initial test the framework supports advanced pipelines where an inbound source will kickstart many outbound routes which is managed by an external controller, for instance the controller could observe an email event for a newly added contact which then triggers two separate container actions which is validating data then writing data

You can connect various services to Phoenix. This requires the service API keys, configuration settings for each source to define a new `route`, this can happen on a per service, which is crucial

Advanced pipeline routing rules allow filtering by specific event types with associated rules which may filter out some inbound requests and routes for processing, and may allow for dynamic routes based on events of varying nature to route based on a specific attribute value, this enables dynamic processing based on data context which may enable a new set of container actions that use different components, it allows routing for a more specific processing requirement and allows dynamic rule routing which requires external validation rules

Once events are created, and pipeline definitions created, these can then be managed through `Kubernetes` deployments for easier updates or versioning, and can enable dynamic rule sets. Phoenix includes tooling in order to allow for the validation against existing events

To monitor your pipelines the K8s monitoring tool provides visibility in order to view container states which also details error events

## 4. Configuration

The project utilizes YAML configuration files alongside environmental variables and code.  `pipeline.yaml` specifies pipeline routes while `events.yaml` specifies the incoming external event triggers to the system for pipeline processing

First we configure your event definitions through an YAML definition, which specifies the events and associated trigger variables to evaluate which externally originates data from. Events include name of incoming triggers to the pipelines to be routed to different tasks, which defines what events the pipeline monitors as input, and associated configuration parameters for authentication and external routing to trigger outbound events, the configuration includes API credentials for the sources.  `event types` should align between pipeline configurations

The pipeline route configurations can use a set of environmental variables for the k8s cluster for setting authentication and deployment locations that should match to deployment locations for easier management in K8s

Without external validation you cannot run this project. The YAML configuration files should always adhere to schema definition to provide an automated way for configuration.  For validation of your YAML files against predefined, it supports external tooling which enables automated validations, for schema enforcement and configuration error checking, which also prevents unexpected pipeline deployments with improper configuration and helps identify common configuration-based problems and potential routing and processing faults

Environment variables, for authentication keys, are essential. This strategy ensures separation between sensitive and configuration code. The most relevant environmental keys include external api source, external data storage keys as these will enable external connections

The framework uses an abstract factory design that can dynamically route payloads to various pipelines inside Kubernetes which can also handle different payload and event types which also provides routing logic, enabling efficient processing with minimal code.  You also configure preferred pipelines based on payload size

You can override pipeline configuration values from external environments, and use configuration inheritance. It also includes an API for configuration that provides dynamic code updates that allows runtime re-evaluation

Phoenix supports a centralized configuration that uses external tools like Kubernetes Secrets in a repository or service configuration. Configuration should follow an agreed format with schema and automated tests to enforce consistency which enables configurations across pipelines in various locations and deployments

All components and containers have configurations to specify resource allocation such as memory requirements. You need these parameters defined within YAML, as the kubernetes deployment manager is the only entity allowed

## 5. Project Structure

The `Project Phoenix` code repository is organised in directories which allows a streamlined approach to managing pipelines within the k8s orchestration. A clear and defined file-system approach will enhance collaboration

At the base is the directory structure: `controller`, contains core event controller, event routing management, k8s interaction. It includes `api` for internal API calls for k8s deployments and the central loop

The main folder containing pipeline configuration is called: `pipeline`. These files contain configuration for various processing pipelines including `pipeline.yaml`, defining events routed by pipeline rules, it covers inputs, output routes which also defines the routing rules to external components which includes the authentication requirements and configurations, these also contain routing configuration details which includes API Keys. This folder can support various yaml and code definitions which include authentication keys or service configuration

Within directory `/kubernetes` you find kubernetes manifest for creating required resources in cluster which enables easier and rapid deployements with a declarative format for defining K8s resource, for example the core deployments and controller. Also the main deployments and controllers and other monitoring dashboards which enable monitoring pipelines through the kubernetes UI

There directory called `docs` provides a collection of various information related documents for the pipeline. This can be helpful with debugging performance. You will have various YAML definitions. The `/event_definitions` includes various YAML for describing different source of probes or pipelines for event ingestion.  Also `/modules` includes modules, such as python libraries

`configs` provides default configuration for the framework, it provides configurations to external systems which provides configurations of how it connects or handles certain services such as Kafka sources to be able to trigger the controller, it provides various configuration settings, and authentication details for different components that will enable external connections to source components, which include keys to enable access, or authentication keys and mappings for the pipeline components which allows it connect and access source external components such as databases to access event source. This can allow easy integration to different components to handle incoming requests with a streamlined and simple approach

A directory dedicated for the integration test which can test end ove flow. `testing/integrations`. This can allow automated and integration based validations with different source pipelines. The directory is intended for unit integration, it allows easy validation. There directory contains testing modules

With clear organization of folders with a modularity of files and code will streamline deployments

## 6. Contributing

To start contributing ensure you can reproduce the development setup with Minikube with the documentation provided in earlier sections, as any code updates and tests needs this. After reproducing the initial set-up it's best to first clone from `dev` or create an equivalent of one, as you can avoid any breaking of main production deployments. You should create separate branch, so there is clear tracking with the feature you enable to improve it further, with separate testing and code check

When submitting PR ensure there testing can cover a good number of edge-cases as that helps avoid potential future regression or unexpected behaviours when a deployment is performed, ensure all code changes pass unit-tests as those are important

Adhering to Python 3 standards.  It's expected to write Python3 standard. Ensure there isn\'t backwards or any deprecation warnings, any warnings must have an updated version, or fixed before submission of pull request, and must follow code conventions. This enables consistency among contributions to maintainability with future code enhancements. This should follow PEP8 and use consistent and identifying naming for each module which makes code easier

Ensure all the PR is properly formatted using Markdown documentation standards as outlined, as well the tests. All commits have to have descriptive and detailed descriptions because future team will review code to maintainability with ease to enhance and understand purpose with a short explanation and summary which helps understand code and reasoning to ensure it integrates well. You have documentation as that will help to maintain and expand

The community welcomes issues or contributions on any part to expand or address any concerns in code to ensure it aligns with overall architecture which also helps in maintaining stability. Please follow guidelines for issue reports which will assist with faster turnaround.

## 7. License

Project Phoenix is licensed under the **Apache 2.0 License**. It allows users the freedom to use, modify and distribute the software as they wish while requiring appropriate attributions of authorship when redistributing the package and sourcecode in their deployment, with clear and defined requirements in the license terms with appropriate acknowledgments and credits, to protect copyright. Any code that is derived from original project needs clear and visible licensing to protect against unauthorized distribution which will maintain code quality as a standard, which helps avoid copyright issues or potential legal concerns for distribution in any fashion or manner for any external deployment, with clear terms that will protect intellectual code property from any misuse for any potential purpose in the code distribution which includes licensing restrictions or potential commercial use cases.
 

## 8. Acknowledgments

Project Phoenix owes a debt of gratitude to various components for building it out to its current design, including but not limited to Kubernet and Apache Kafka, that enable the overall throughput capabilities of our pipeline and allows the rapid processing. It also utilizes several external tools which enhance its performance for validation, monitoring for ease in the debugging process. It uses a lot of libraries which enhance functionality of our system for rapid pipeline build, for external event source integration such as Amazon&#x20;

;s SQS. We acknowledge various individuals from various online resources. It leverages and benefits significantly from Kubernetes' extensive capabilities to enable modular design, containerisation for the framework's ability to be effective and highly efficient with the deployment pipeline, for easy auto deployment capabilities in k8s which is crucial to the scaling. Also we recognize various developers for their community efforts on Github in providing tools and documentation for various external integration and frameworks that have been incorporated which enables easier deployment with rapid integration capabilities which helps reduce code.

It acknowledges and leverages the power and benefits and open sources that contribute significantly in this framework

## 9. System Architecture

The Phoenix Architecture operates around two primary core modules namely: *Pipeline* which consists in event handlers for various event routing which also provides event transformation. And an orchestration core acting to handle the pipeline management, the external routing to external systems, which handles pipeline configuration for event source integration as well external API calls to handle events

Kubernetes forms the bedrock for all deployments in order to support scalable event orchestration that is handled dynamically for all containers within Phoenix that are responsible in routing to events, and also the management, this is critical because Phoenix utilizes containers. Kubernetes orchestrator enables scaling the containers and enables self heal to ensure reliability which handles events, the central orchestration module, it manages events

A core module for routing pipelines for incoming external event requests which is critical and also manages pipelines which includes authentication and authorization and external connections with an integrated API calls. It leverages k8s deployment, service for the management. Also external API call which allows easy access from other modules. There will have API calls, authentication to connect external source for events such as external data sources that will require API authorization keys for those external connections that require validation. This is important. It uses an external source that needs access validation from authentication.  There a core API for external source to enable external integrations with various services with the system in mind for a highly flexible integration with minimal dependencies which is essential and scalable architecture to enhance and maintain the stability, which ensures all components are integrated seamlessly which provides rapid scaling with the ecosystem. 
There an internal service between K8s, so that allows the orchestration

## 10. API Reference

Phoenix has multiple API components: Event Router, and the Event Management Service.

**Event Router:** `/api/v1/route`. Accepts an `event` payload which includes an ID for external events with the routing configuration which handles which pipelines to process for routing purposes. This has an optional query `?type` with specific types

`GET /api/v1/routes?type=UserCreated`. This is how a developer requests routes by the types that the framework has defined with its own configurations. The response has all routes, with an ID, configuration YAML details with source events which also defines event processing parameters for event processing for different components and services and external validation rules
**Event Management Service:** Provides API methods in a k8s controller to allow event definitions which can enable and enable external source configurations, with a post and get API

```json
  POST /events - Creates event definition - { eventName: UserSignup} -
    response - { event ID} - returns an API endpoint to validate the event ID