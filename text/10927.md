# Automated Log Processor (ALPEX - ALP v2)

This repository holds the code for the Automated Log Processor, version 2 (ALPEX), a powerful tool for analyzing, filtering, and aggregating server logs. ALPEX helps system administrators and DevOps engineers to identify issues faster, monitor resource use, proactively detect patterns, and optimize system stability across various log data sets. It is highly configurable with support to many log file formats like JSON logs, CommonLog files and custom text delimited log formats.

ALPEX addresses the challenges of analyzing massive volumes of server log data. Traditional approaches of reviewing logs using tools like `grep` are manual, slow and inaccurate; especially for detecting subtle or infrequent patterns. This project utilizes efficient indexing, advanced filtering algorithms to identify errors, performance issues in a streamlined automated pipeline, and delivers a dashboard of aggregated metrics, and automated alerts, improving overall operational efficiency.

Our architecture is event- driven with a modular design. Input log events are parsed, processed by a series of customizable pipelines, and stored or visualized in a structured format. The core component is a log parser, which extracts key data from each line or structured JSON data and creates key values that form a standardized representation which simplifies later stages.

 ALPEX supports many server technologies and application frameworks, including Nginx, Apache, Kubernetes clusters, and microservice deployments. By providing insights into the behavior of the entire IT infrastructure it facilitates rapid root cause determination. Furthermore, the customizable alerting and notifications system ensures prompt attention to important incidents.  

The primary goals of ALPEX are to reduce troubleshooting time, improve system uptime by allowing proactive identification of issues through pattern matching and threshold violations, and provide a clear, concise overview of the system health in a dashboard, all within an automated framework. ALPEX aims to replace cumbersome manual tasks with a highly efficient automated system to allow for greater visibility into the system health.

## Installation Instructions

Before beginning installation, ensure that Python 3.8 or higher is correctly installed, alongside `pip` the dependency management tool. Verify your Python version using `python3 --version`. A virtual environment is strongly recommended to prevent dependency conflicts, and can be created using `python3 -m venv .venv`. Activate the environment on Linux/ macOS by using `source .venv/bin/activate`, or on Windows using  `.venv\Scripts activate`.

Clone the ALPEX git repo locally by executing `git clone github.com <repo_name>`. Navigate into the new repository by using `cd <repo_name>`. This will create a new directory containing the ALPEX codebase, configurations, and supporting files.  

Install the Python dependencies using `pip install - r requirements.txt`. This file lists all external libraries used by ALP to ensure a consistent, functioning deployment, and avoids version conflicts by specifying minimum library versions. If a dependency installation error occurs due to conflicts with other libraries in the environment, consider upgrading `pip` to the latest available version and attempting the installation steps again, or recreate the virtual environment.

If you intend to use the built-in visualization component, which uses a simple web interface, ensure a web server such as `nginx` or `Apache` is properly installed and configured on the target system. Install a graphical tool like `matplotlib ` and its related dependencies.  

On Windows, consider utilizing a virtual environment management tool, such as `conda`, to streamline dependency resolution and avoid issues with path variables. The system path needs to be configured to ensure Python and pip binaries can be accessed correctly.  

To enable logging, configure the `logging configuration file (config/alp_logging.ini )`, which dictates the format and level of the log entries, and ensure appropriate write access to the log directory (default is `logs/ `) to enable efficient tracking during the execution of the system, especially during debugging or troubleshooting.

To set up a scheduled task (e.g., using systemd on Linux) to run ALPEX periodically, define a systemd service file that specifies the execution command (e.g., `python3 alp.py --config config/alp configuration.yaml `) and the user under which it runs. Enable and start the service by commands `systemctl enable <service_name>` and `systemctl start <service_name>`, enabling continuous operation. For MacOS, you can use `launchd ` to schedule the task.  

On MacOS, it is highly recommened to ensure that your `PATH` variable is set correctly, and that permissions issues do not hinder execution.  

Verify the installation by running a basic example, described in the "Usage Instructions" section. Inspect the `logs/ ` directory to ensure log entries are being generated correctly, which confirms the installation was completed and is working as intended.

## Usage Instructions

To execute ALPEX, use the command: `python3 alp.py -- config.yaml`. This will trigger the main ALPEX script which then proceeds to read log entries from a directory specified in `config.yaml`. By default the input will point to a local data set within `/samples`. The data within these logs may require adjustments, to properly test ALP, you should configure an appropriate directory.

The `config.yaml ` file allows you to set all relevant options to filter log messages and perform aggregation operations, or run an API based service to ingest external sources like CloudWatch Logs and Elasticsearch clusters. You may use this file as needed, but for testing you only need to run with `-- config.yaml ` flag for now. We support several output options which you configure through configuration as well including, file based storage with timestamps, cloud data lakes as an external service.

Run an analysis by passing command `python3 alp.py --log_input_path /your/data/logfiles.txt  -- log_format custom`. Replace the placeholder with an appropriate source of your own test log entries. The flag, `-f ` allows you to configure custom delimiters or structured json based data formats within each individual line of a dataset for custom log processing. This flag allows you to customize each line based data parsing process within the system for greater flexibility with unstructured datasets.

For an advanced demonstration using an API endpoint you need to configure it as described within configuration sections (described within). Use tools such as Postman or curl. to interact with exposed RESTful endpoint by using commands, and send the appropriate requests with a JSON-structured format which can include custom filters as needed to query data and generate reports or dashboards with real-time analysis and visualization, for enhanced operational control, especially across complex infrastructure systems. The base API path should look similar to, `localhost:5000/query/events` and you can query based on various filters specified as key value pair data entries such as event severity (INFO, DEBUG).

Check for output generated in either console and `logs/`. Examine these to make sure all validations pass. For debugging, set config values like `-d ` (for verbose), which may expose more debugging and logging details that help you resolve unexpected behaviors or edge cases encountered within data ingestion and log filtering and processing steps during operational execution.
Run an automated validation using command  `python3 tests/unit_tests.py`, for automated code tests, this validates code against various edge case scenarios, confirming code integrity. Run integration testing using command `python3 tests/integration_tests.py`.
## Configuration

The main configuration file is `config/alp_configuration.yaml`. This file uses YAML format.  Comments in YAML can only exist after valid parameters with `#`

Modify the  `log_input_path ` variable to the full or relative path where your logs are saved on your filesystem. If you want to test a specific source of logging, ensure there are no directory/access errors or exceptions during initialization. You will want to ensure that it does exist and has correct access permissions during startup and processing cycles, typically as root to enable system wide scanning, if applicable to a particular server deployment.

The `log_format ` configuration property lets you specify format for processing log records which may contain custom patterns, JSON data types including `csv`, custom `txt`. Set delimiters for text log file data entries as well within a `log_format. delimiter_chars` key to parse custom text based datasets for processing in pipelines with a specified `custom format `

Environment variables also control the application's configuration; they are accessed in ALP via using environment configuration file specified by  `Environment. config_path.yaml `  You may need to restart for environmental configuration changes. The application will also check to use the environment configurations. If environment config files are available, this takes precidence on settings.   You might need restart service, after any modification on configurations or settings on device or system

To manage alerting mechanisms and output destinations (such as cloud platforms, file destinations) edit corresponding `out put` configuration entries for setting destination addresses to store output in desired destinations as per need or configuration requirements, including database connections with authentication details. These entries are also configurable, enabling the system adapt dynamically as requirements or environments vary, facilitating streamlined operation with different data and output destinations.

Consider utilizing a central `secret_store`, for handling keys/credentials that will be needed for connecting with data services to enhance system safety, and reduce potential vulnerability by centralizing and safeguarding keys/credential. It should be secured from direct external or system accesses, which will reduce vulnerabilities in system deployment environments, by isolating critical components in an environment-secure configuration management process.
You must define a list in configuration that outlines specific patterns to monitor using the  `filter. custom_regex_filters` variable. Each item must represent Regex and is evaluated for incoming logs as they traverse into pipeline and may filter specific event log patterns to reduce unnesscary events. Ensure you provide correct expressions, which could lead unexpected filtering behavior, impacting monitoring results as it affects log data visibility across pipelines and analysis tools

Use  `global_thresholds ` variable, within configurations to define alert threshold values to identify anomalies in operational data for system health checks and performance. This is used by the anomaly monitoring pipelines to flag and identify critical issues and ensure that something out-of-norm gets alerted. Threshold values are crucial when configuring alerting rules.  You could also set these as an env var instead
## Project Struc