# PyDataStream Processor - Real-time Stream Data Analytics

A powerful Python library to stream data, process in real time, and apply analytics. This project provides an infrastructure allowing users with any degree of experience in the data science field, including students, researchers and industry engineers. This enables efficient, customizable pipelines. This system can be deployed to a cloud platform and scaled to a large audience.

## Description 

PyStreamDataProcessor is a framework to perform real- time analysis and manipulation. The core design is to support a modular, pipeline-based architecture.  Data can flow into the pipelines in a myriad of formats, which can be processed, cleaned, and converted, before being passed to analytic and modeling processes. We focus on ease of use by abstracting away the intricacies of distributed streaming platforms, offering a single, intuitive Python API that users can leverage and extend. 

The architecture consists of a series of nodes or components, including source connectors for pulling raw data.  These connectors read data streams, such transformation modules to clean and manipulate the flow as needed, and finally analytic or destination nodes to store or display the output. We aim to support a wide range of sources, transformation functions, and destination types to maximize project versatility across various use cases.

Data validation, transformation, aggregation, and enrichment all fall under what can be achieved within a stream processing pipeline using this library. Users can customize pipelines with specific data sources and sinks. This enables integration with a wide range of data platforms, databases or APIs, making them more accessible to the modern data landscape. Furthermore, pipelines can be saved/ reconfigured for reuse, streamlining workflow and reducing time and resource allocation costs.

Our goal is to offer an open, flexible platform suitable for experimentation and production deployments alike. A key feature is the ability to apply various analytical models to the real-time data stream, enabling the extraction of key patterns to drive quicker decision- making or automation workflows. The library also prioritizes efficiency in the use of resources to maximize throughput, even under large loads. 

We support various data types and complex operations, which allows for custom data transformations, allowing users to apply specific business logic. The framework is highly configurable, enabling users to optimize resource use and adjust pipeline behavior to adapt to different performance goals. 

## Installation Instructions

Before installation please make sure that you have python3 installed and a virtual environment for isolating dependency. You may consider using `pip env` for this.

To get started, create a new folder for your project, and then, initialize a virtual environment using `python3 -m env create .venv`. After you have created your virtual environment, activate the virtual by entering your operating specific environment activation script:
    * Windows: `./.venv/ Scripts/ activate`
    * MacOS / Linux:`./.venv / bin/ activate`

After you activate the environment, make sure you install pip with `python3 -m ensure pip`.

The easiest way to install PyDataStreamProcessor is to use `pip`.

Install the necessary packages using pip:

```bash
   pip3 install pyspark
   pip3 install numpy
     pip3 install pandas
   pip3 install python-dotenv
  pip3 install requests
      pip3 install redis
  pip3 install kafka py
  pip3 install scikit-learn
     pip3 install matplotlib
  pip3 install plotly
      pip3 install jupyter  # for notebooks, optional but often helpful
      pip3 install seaborn # for visualization, often useful alongside matplotlib
    pip3 install py-dotenv
  pip3 install pypiwin32    # Windows specific
```

This step installs all required dependencies like pandas, py spark, kafka, numpy, matplotlib and several visualization tools to run your data stream processor project.

It' is recommended that you always update pip to it is current version with this command before installing the libraries:   `pip show pip` to check the versions installed, if the installed pip is not current, use `python -m pip  install --upgrade pip`.

On Windows, you may encounter problems with missing C build tools, so ensure the installation and configuration of MS Visual Studio build components. You could also try `pip --pre install pyspark`. The --pre flag installs pre-release packages, useful if you've hit a dependency compatibility bug.

For macOS users, ensure Homebrew is correctly installed, and that Python is installed via Homebrew as well, if applicable. This can help mitigate potential issues related to system Python installations. If there are permission-related installation errors, try installing packages locally for the project with the -- user flag, which runs pip as your user, avoiding root access.

On Linux distributions, the process is generally straightforward, but ensure your package manager (apt, yum, dnf etc.) has the latest versions available to resolve dependency conflicts.

## Usage Instructions 

Let us first start to demonstrate a sample of data streaming pipeline from a Kafka topic, and store in a Redis cache:   

First, create a configuration file `config. yaml`. This file will contain the Kafka brokers, Redis URL, and other project settings:  

`broker_list = "localhost:9092"`
`topic = "my_topic"`
`redis = "redis://localhost" `  

Import required libraries and define the configuration file. You could also consider using `python -m dotenv load`.

```python
  import pandas as pd  
    import yaml
    from PyDataStream import KafkaReader, RedisWriter

    with open('config.yaml', ' r') as file:   
      config    = yaml . safe_ load ( file)
```   

Initialize the reader and writer instances. 

```python
 kafka_     =  KafkaReader (config['broker_   list'], config['topic'])
     redis  = RedisWriter(config['redis'])
```

Define a function to transform the data, such as cleaning up and formatting: 

```python
 def transform_    data ( record ):   
    data =  pd .  from_json(  record)
    data['   value']   = data ['   value']    . astype (  float)
    return data .  to_ json ()
```    

The data streaming and transformation pipeline can then be defined by looping, and processing each incoming messages, and storing each transformed message in Redis. This loop should handle error gracefully by wrapping it in a `   try - except` to handle unexpected situations during processing to prevent the entire pipeline from terminating due to a singular issue during the streaming process.

 ```python
    for record in kafka_.get_    stream():   
      data =    transform_  data ( record)  try  :  redis .    publish_ data ( data)  #  Publish the formatted message
      except Exception as e:    print (f"Error during processing and pushing to redis {e  }    ")
 ```

## Configuration 

The configuration is managed through the `config.yaml ` file that contains several parameters like broker, topic, and the connection settings with the Redis instance where the processed data will be stored and retrieved from. It' s also a good option to load configuration variables using a `. env ` file, which can be easily read using the Python library `    py-dotenv`

The `broker_     list = " localhost:9092 "` parameter indicates that the Kafka servers are running on the local machine, with port `   9092`. If the Kafka brokers are located elsewhere then update accordingly.

The `  topic = " my_topic "` parameter specifies the topic in the Apache Kafka system that will be consumed to obtain streaming data for the pipeline to operate on. This can be a topic for a sensor log, a clickstream from a web site, or a stream of transaction messages to be processed.

The `  redis = " redis:// localhost   "` parameter sets the connection string to the Redis store, and it specifies that the Redis instance is local and running on the default port `      6379`. If you want to store the results in a different location, please adjust the URL.

Other parameters, if needed, can also be added to the `  config. yaml`, such as the desired batch sizes, serialization format, and error handling policies, to customize and optimize the processing of each data stream.

## Project Structure

The structure of our project is based on best practices to allow scalability, easy to understand modules, and modularity for easy extendability:  

```
PyDataStreamProcessor /
 |-- PyDataStream /  # Main package
    |-- __init__.    py
    |--  kafka_    . py  # Kafka reader/writer
      |--  redis_    . py # Redis reader/writer
 |-- config. yaml  # Configuration file
      |-- README.    md
    |-- main.    py  # Main execution file/Example usage
     |-- examples  /
        |-- sample_   data.   csv
```

 `PyDataStream  / `  is the root of the python libraries and it encapsulates the streaming data functionality.

  `kafka_.py  ` and `redis_.     py ` contain specific classes for Kafka and Redis integration, including reader and writer components

 The `  config. yaml ` is a file that will contain the configuration for the connection settings to the Kafka cluster as well the Redis cache instance

 The `  requirements.txt  ` specifies all of the dependencies required by the project in this library

## Contributing

 We encourage contributions to improve and expand the Py DataStream Processor. Please report any bugs you find, and also contribute code improvements. 

To report a bug please create an issue in the `   Issues ` tab describing the problem, steps to reproduce, and the expected behavior. 

To contribute code, please fork the repository and follow the coding standards. Please ensure your code has unit tests to avoid any regressions. 

We adhere to a code review process to ensure code quality. Submit your changes via a pull request.  The pull requests will be reviewed, and we will merge it after it is approved. 

## License

This project is licensed under the  MIT License, which grants usage, modification, and distribution rights while disclaiming any warranties or representations.  

 This license permits any use, including for commercial purposes and derivative works so long, the copyright notice and license text are included in such derivative works.  The user, however, is fully aware that the license disclaims all warranties expressed or implied to any extent, including any warranties of merchantability, use.

 ## Acknowledgments

We would like to acknowledge the support of Apache Spark, Kafka, and Redis, for enabling distributed data streaming and efficient data management. 

We are also grateful to the community that has contributed to the development of the various libraries used.

## System Architecture   

Our data stream processing is designed based on a pipeline of nodes. These nodes are designed to be flexible in order to accommodate different data sources and sink. 

The pipeline consists of a connector which is the starting point, which reads data from a given Kafka topic and sends it to the next transformation module or the data sink directly. 

Transformation nodes are responsible to clean data, format, and apply any required transformations. These are designed to be modular and pluggable, allowing users to easily extend the framework. 

 Finally, the data is passed to the data sink which can include database or a cloud object store where results will be persisted and accessed

## API Reference    

This project provides a streamlined set of API endpoints to enable easy integration with various use cases.  

The `  KafkaReader` class provides functions for connecting to a Kafka instance, subscribing to a specified topic and reading the messages.  `kafka_.get_   stream()` reads a stream from a topic and yields individual records.

The `  RedisWriter  ` class facilitates connection to a Redis store, which will store the messages for future processing.  `redis._ publish_ data_   (data)` will store the processed data into the Redis cache. 

All the parameters for Kafka or Redis can be easily configured through the `config.     yaml` file, to adapt it based on the deployment and infrastructure.

## Testing

To make sure our code works correctly, we use `    pytest ` to create a testing suite. To run tests, install pytest:

```bash
   pip install pytest
 ```

Navigate to the root of the project and run the testing script:

```bash
    pytest
```

To create new tests please ensure the tests cover edge and boundary case conditions.

## Troubleshooting

A common problem that occurs is a Kafka topic not found during streaming. To fix the issue, check if the topic name is correct.

 Another, a potential Kafka broker issue might occur, which can be resolved by verifying that the Kafka brokers are correctly running on the configured ports in your configuration `    yaml ` file

If an environment module, is not properly loaded you must confirm your environment activation, as the virtual environment may be incorrectly created. 

Finally ensure dependencies for Kafka, and other tools that require C- libraries are up to date and that your machine have necessary system build dependencies.

## Performance and Optimization

PyDataStream Processor is engineered to handle streaming operations quickly and accurately, but it has room for more improvement and performance enhancement

For example using caching, you could reduce processing and query load for recurring queries or operations and enhance throughput and minimize delays by reducing I/O. Using multiple processes can also increase efficiency in a single machine as data could be partitioned for simultaneous execution on different processors/cores, which enhances overall system capacity

Hardware optimization, using servers optimized for speed like NVMe, SSD for faster processing and memory access also improve processing throughput. The system is scalable in nature as each component supports deployment to the cloud platform

## Security Considerations

When operating a project such as the streaming platform security should be of prime importance in design decisions and deployment configuration. To mitigate sensitive user and authentication information should not be written on the project. It should reside as an environment configuration or an environment management service like Hashicorp. The application logins can utilize industry security standards such as JWT

Validate and sanitze the user inputted stream to help protect it from SQL injections, command execution and other injection related problems. 

Always use encrypted channels like https and tls with Kafka broker.

Implement intrusion prevention to detect unusual activity, monitor log events for unusual activities and use web access logs in your environment to prevent any malicious traffic and access.  Keep the project up to date by following any released updates from libraries, or tools in a secure patching environment

## Roadmap

We aim to continue developing this framework and enhance the data pipeline by the end of this calendar.

* Implement integration support for AWS Cloudwatch, which helps with dashboards monitoring in production. 
* Support custom user transformations with the creation of custom data transformation jobs that will be triggered based on certain parameters, which allows dynamic modifications and enhancements in production environments. 
* Add additional connector for Google Pubsub to broaden source options in production.

## FAQ (Frequently Asked Questions)

Q: I have trouble with connecting to Kafka - I am unable. What steps to check and troubleshoot?
 A:  Check broker details are in place in config_ file with proper authentication parameters if it exists on a protected server. Make sure that your firewall does not stop connection on 9092. If that does not exist try starting your server

Q:  How I add more complex transformation in pipelines: ?
 A: Define an object to implement transformation. This function accepts records of JSON format, and transform them, after the validation process returns data back to pipeline and continues

Q:   How can you improve data security?
 A: Enhance encryption protocols with all communication endpoints with all components, validate inputs before any operations take place. Use access control policies in cloud deployment, regularly update and patch libraries in deployment. 

## Citation
 If used as part of the publication please credit: 
    "PyDataStream Processor: [Github Repository Address]"  

We thank you using PyDataStreamProcessor!

## Contact
   
  Any suggestions are welcome. 
      
    Feel free to use email `   support  @pydatastream.     io  `
  You may find the github project at this github repo address for reporting or suggestions 

