# DataInsight Analyzer - Predictive Analytics Engine

## Description

Developed by the Stellar Analytics Group for businesses aiming to improve decision processes utilizing AI/ ML techniques to uncover trends from existing datasets. It offers a streamlined solution for analyzing, preprocessing datasets.  DataInsight Analyzer provides powerful capabilities including exploratory data analyses, anomaly detection, predictive modeling, and comprehensive reporting with customizable dashboard visualization.

The core architecture relies on an extensible plugin-driven approach, easily integrating new data sources, algorithms or analytical features. This modular design allows for rapid adaptation to evolving analytic requirements.  The underlying framework incorporates best- practices in terms of data security and efficient data transfer across the system. Data is transferred via encryption protocols using secure APIs.

The tool primarily uses the Python framework with popular libraries for data manipulation (`pandas`), visualization libraries (`matplotlib/ plotly)` and predictive models using tools from  `sklearn`. It has been optimized for both CPU, utilizing efficient vector calculations with optimized routines to enhance performance, making use of libraries with C based extensions for faster performance. GPU optimization is planned for the v.2 of the application.

We have developed a highly scalable architecture using containers, supporting the creation for multiple instances. Each container handles multiple requests using an async approach. This approach allows the application to scale easily and maintain its reliability. The containers communicate over a secure messaging bus.

The application can be integrated with various data warehousing platforms using the provided API' and connectors for data ingestion. The tool is designed as a flexible solution, adaptable to organizations of any size. It can be deployed both cloud environments (AWS or Azure) or a local on -premises setup depending on your specific requirements for governance and cost optimization goals.

## 1. Installation Instructions

The following details how to install our powerful DataInsight Analyzer. Before proceeding please confirm all dependencies have access and the correct environment set up is available. It should also be verified there sufficient memory on the system for large datasets.

To initiate, ensure your machine has Python 3.8 or higher is installed. Python's package installer ` pip` will be essential. The easiest way to get these components are installing through Anaconda from the official site. Verify the version with using command `python - V` within the terminal, and it should show your Python 3 version.

Next is downloading the code base and extracting it. Download the latest version through the releases page, and then extracting the archive `tar.gz` using standard tools on your Linux environment, like the `tar` command. The same approach would work for MacOS using standard utilities for unzipping archives. On Windows use 7-Zip or a standard zip extraction tool. 

Move the directory to its designated location where you want to store all of your source code files in. It is common that source code repositories get stored within `/opt` folder on Linux machines for ease management. After that, ensure the current user has sufficient permissions to read and write in the directory that you' ve created.

To establish the Python environment, enter the directory, then run the following: `pip install -r requirements.txt ` This will resolve and download all project dependencies. Note this may take sometime, depending on your internet connections, and available packages. Ensure the command runs successfully, and all dependencies are resolved successfully before the installation continues. It may be beneficial to update pip before hand. To accomplish this use: `pip install --upgrade pip`.

For Windows users with a Python environment setup using venv, you must activate it using `.\venv\Scripts\activate `. If using an Anaconda distribution, ensure you have activated the target environment. You might also consider creating a new virtual environment for better isolation from any other existing packages.

For Linux or MacOS environments, after navigating to the project directory, you can initialize a new venv and activate it with commands like : `python3 -m venv .venv` and source `. venv/bin/activate`. This approach helps in keeping all packages and versions separated. If you are encountering issues installing certain packages, try installing `wheel` as a global dependency.

Now the project has its requirements setup, configure the environment variables.  ` DATAANALYZER_HOME` must be set to the path of your installation root and `DB_CONNECTION ` must have connection details. This can be defined via shell commands ` export DATAANALYZER_HOME = /your/ installation path`, `export  DB_CONNECTIONS = "user:password :hostname:port:database "`.

Once this has been done, verify by checking the installation via command ` DataAnalyzer -version`. Verify the output shows the version of the Data Analyzer. This should ensure a successful installation.  If issues still arise, check that all packages are in the python path.  

## 2 Installation - Platform Specific Notes

On Windows, it' is recommended to use ` pip install --target=<target_directory> <packages>`. Replace `< packages>`  with the packages listed in your `requirements.txt  `. After successful install, add the `< target directory >` to `PYTHON_ PATH`, which allows you to use modules directly.

MacOS may need XCode tools to handle dependencies and compilation. You can install them using HomeBrew by executing : brew install  xcode-command-line-tools. This step is important, especially during dependency resolution with libraries having C extensions. Ensure you' ve updated all packages before the installation finishes.

On Linux you should also install any needed dependencies for the specific libraries. The documentation for each library will detail which are needed for proper functioning, this may include build- essential, libxml2-dev, and others. It is best practice that your OS is always up to date.

## 3. Usage Instructions

Once installed, start the DataInsight Analyzer service using the following command: `Data Analyzer start `. This starts a server on the localhost with the default port. Check the server with command `netstat - an |grep 5000`(assuming the server is set up on 5000). You should see a connection.

The core functionality can be leveraged via the REST API endpoints which is available at `http://localhost : 5000`. The API is fully documented in a Swagger UI which can be accessed at ` /docs` after the server is up & running, where you can test the API calls. You can use any REST client for making calls to the end points.

To upload a CSV dataset, make a POST request to ` /upload`, with a JSON object that includes the data file, the desired delimiters. Ensure file size limits are observed to ensure optimal performance. The server should return a success message, along with the ID.

For basic data analysis such as calculating statistics and correlations, use the ` /analyze `endpoint.  The parameters for analysis should be specified in the JSON payload. The API will return statistics on the specified column of the dataframe.

For advanced predictive modelling, the `/predict` endpoint should be used.  This will accept an ID of a pre-uploaded data set, the features, target variables, algorithm and hyperparamters. The endpoint will return the predicted results. Please ensure the model training data has been uploaded and properly formatted. It may take a minute or so for a prediction to complete depending on the complexity.

To create visualizations for the uploaded datasets use endpoint   `/visualize`. The request should define the chart type and the relevant data columns. The API will return a URL to the generated image of the visualization. It supports a diverse range of chart types to accommodate all your visual data exploration needs. This endpoint is a very effective way to get a feel for datasets quickly.

## Configuration

The core configurations of DataInsight Analyzer are controlled via environment variables and a global configuration file `configs/ analyzer _config.yaml`. This enables flexibility and ease of deployment in various environments. The `configs/ ` directory should be accessible to the running application.

The `analyzer_config.yaml` file defines global settings such as database connection details, default analysis parameters, and logging configuration settings. It contains sections for `database`,  `logging`, and `model`, defining the database connection string and model default hyper parameters.

`DATABASE_URL` environment variable dictates the location of the database. For local environments, a PostgreSQL database connection string should be supplied. For instance: `postgresql://user :password@ host:port/database`. For cloud environments the database URL may be different depending on the managed services.

`DATA ANALYSISER_HOME` points to the main application directory.  This variable is often used for locating resources and configuration files. It can be overridden by the system if required. For example, `/opt/data -analyzer`.

Logging level can be configured via the `LOG_LEVEL `environment variable. Available logging levels includes `DEBUG`, `INFO`, `WARNING`, and `ERROR `. The level can be set to `DEBUG` for detailed debugging during setup, which provides detailed log messages.

You can override the default parameters in the ` analyzer_config.yaml` file by setting environment variables with the same name but prefixed with ` ANAL YZER_.`, such as ` ANAL YZER_ALGORITHM = "RandomForest" `. It provides a simple and effective way to customize the application' behavior.

The maximum file size for uploads is configured by environment variable `MAX FILE_SIZE`. If this is not set, then defaults to 100MB. The server may throw an exception if the file size exceeds the maximum. This should be set depending on your data size.  

## Project Structure

The project's codebase is structured as follows:

*   `src/`: Contains application source code, including data analysis functions, model training scripts, and API endpoints. This is the central location for all custom Python code. It is well documented.
 *   `src/api`: Handles REST API endpoints with request handling and route mapping.
 *   `src/data`: Includes modules related to data loading, pre-processing, and transformation.
 *    `src/models`: Implements predictive models with model creation, and validation routines to enhance accuracy.

*   `configs/`: Stores configurations and parameter settings. Includes `analyzer_ _config.yaml`.
    * `configs/ analyzer_config.yaml`: Global configuration settings.

*   `tests/`: Contains automated test cases, including unit and integration tests to verify the code.
  *   `tests/test_data`: Tests for data processing modules.
  *   `tests/test_models`: Tests for model training and prediction methods.
  *   `test/test_api`: End- to-end testing and API verification.

*   `models/`: Stores serialized model artifacts. The models are saved to this directory once trained.

*   `data/`: Sample data for testing and demonstration.

    *   `data/ sample_data.csv`
        : Example CSV dataset.

 *   `README.md`:  Project documentation.
 *     `requirements.txt`:  Python dependency list.


 ## 7. Contributing

We welcome all contributions to DataInsight Analyzer. The following guidelines outline how to contribute effectively.

To report issues, open a new issue on the GitHub repository providing as much context as possible, including relevant error messages and code snippets. Detailed information helps us to reproduce and fix problems quickly. Clear issue descriptions are essential.  

If you have a fix or enhancement, create a new pull request against the main development branch. Pull request descriptions should be well structured detailing the nature of the change.

All code contributions are expected to meet our coding standards, which includes following PEP 8 for Python code, using meaningful variable names and comprehensive docstrings.

Before submitting pull requests, perform thorough tests to confirm that your changes don' t introduce regressions. Unit tests should be developed and added to verify the functionality.

We use Git for version control. Branching from the main branch and submitting pull requests is a common development workflow.  Ensure your code passes all existing tests before submitting.

## 8. License

DataInsight Analyzer is released under the **MIT License**.

This license permits the use, modification, and distribution of the software under certain conditions including, but not limited to, the preservation of copyright notices and the inclusion of the license within distributed copies of the software. It provides a permissive free software licensing approach. Please refer to the full license file for details.

## 9. Acknowledgments

 We thank the open-source community for the invaluable libraries and tools that form the basis of DataInsight Analyzer, particularly:

*   **Pandas**: Provides flexible data structures and powerful data manipulation tools. It greatly enhances performance for dataset handling.
*   **Scikit-learn**: For a comprehensive range of machine learning algorithms, simplifying model training and evaluation. Its efficiency allows faster model performance and accuracy
*   **Matplotlib & Plotly**:  Provides robust visualization tools that facilitate understanding datasets through insightful plots. They extend to various chart-type needs and support custom visual designs.
*   **Flask**: A Python web framework enabling fast API endpoint building which supports the API development needs for deployment across cloud environments.



## 10. System Architecture

DataInsight Analyzer utilizes a microservice-like architecture composed of the following modules that work closely. A user sends requests that pass into an REST interface layer using ` Flask `.

Data preprocessing module: Takes raw datasets that get loaded, cleaned and validated with various statistical and machine learning checks using pandas data structure capabilities and other related python utilities for transformation.  

Predictive modelling component: Trains different types of model algorithms using ` Scikit Learn`, which are chosen during configuration via REST endpoint requests, using the preprocessed dataset dataframes as an input to produce trained predictions and insights that provide predictions based on uploaded historical training sets.  

Reporting Engine - Visualizations, and statistics, are displayed and generated by integrating data from various data components.

Database Component - The database component serves the storage requirements using database technologies ( PostgreSQL). Data from uploaded CSV or data source are loaded and retrieved. 

A central orchestration layer is built to control communication between services via an in-process messaging framework and API. Data can then move via secure HTTP endpoints across various cloud or cloud components for scalability purposes.




## 11. API Reference

Data Insight analyzer exposes multiple APIs to provide data access, and model management functionality:

* `/upload`

* **Method**: `POST`
* **Description**: Uploads CSV files.
* **Parameters**:

* `file`: `CSV file`, the dataset.

* `/analyze`

* **Method**: `POST`
* **Description**: Performs analytical processing, and computes statistical measures of the given dataframe column
* **Parameters**:

 *  `column_name`: name of the specified column that gets statistical measurements
 * `operation`: operation type. Available `stats`, `corr`



*   `/predict`

*   **Method**: `POST`
*   **Description**: Generates model prediction based on trained datasets and configurations, and specified parameters
*   **Parameters**:

* `id`: Identifier ID to the training dataset
* `target`: name of targeted variable to predict, or find predictions
*   `parameters`: hyper- parameter configurations for training, if provided
    ##  

## 12. Testing

Comprehensive tests for each module have been developed and should ensure quality, and reliability:

To initiate complete running unit and integration tests, the `pytest` framework gets deployed. Run this from command terminal, inside directory of source code, using command, and it would be automatically detect all the tests that were defined for each component `pytest -V`
Integration Tests will be used and run when API is tested, with the endpoint `/tests` which simulates different test case, by passing various input and data configurations to ensure it can process and provide correct result and data output for the data model and API.


## 13. Troubleshooting

Here are several frequently observed errors that have come to surface while running, troubleshooting steps for resolving those are listed below.
1) If installation failed, double-check the dependencies listed in your  `requirements.txt `.
    If it does fails during install then you would want to check that your system can build native Python packages by confirming the availability, installation of compiler packages and C standard libraries for native extension support to the installed libraries in system. 

2) Database error connection errors:
  * Make sure ` DB_ CONNECTION ` and ` DB_HOST` connection parameters set to particular values, which are correct database URL for your connection and server name/ hostname/ address and the respective port for communication purposes, and verify if that host server has access to your local server as the configuration has been setup and applied to ensure connection and functionality can get applied.

3)  Memory issues while analyzing or making prediction for very big data
 *   Consider increasing system ram, to avoid issues during the memory usage. Also check if your OS has a setting on managing page swapping for better resource control for performance improvements
4) API is returning error `500 `, double- check server running.
   It could have issues on code as you can look on terminal, check your logs and review your logs on debugging to identify root causes for any API request processing. 

## 14. Performance and Optimization

To ensure that the performance, consider caching strategies using a caching system to minimize data recomputations to reduce latency: Implement caching mechanism at data retrieval layer for frequent analysis needs: Redis and memcached systems for in-memory cache are great to improve API processing speeds for faster access
Implement parallel execution using asynchronous task processing to accelerate tasks like processing, predictions
Profiling using Python `profile ` tool. It can pinpoint areas of bottlenecks

## 15. Security Considerations

Protect data with proper input validations: Validate data input for all the requests and prevent potential code- injections or malicious inputs

Manage the credentials for sensitive information. Encrypt the sensitive secrets by leveraging encryption protocols for sensitive secrets like the database access information, or any credentials. Never hardcode secrets directly into the project, or commit into the public. 

Ensure all data communication is conducted over SSL with encryption.
Use firewalls.
Keep system, OS components and third party tools to latest releases for security patches

## 16. Roadmap

- Implement cloud deployment using tools and automation. 
- Support for other formats for ingestion, besides the standard csv.

## 17. FAQ (Frequently Asked Questions)

* How do I specify different delimiter formats on CSV file upload? Specify using `/upload`, with parameter called delimiter.  

* Does Data insight analyzers have support of multiple file type format for datasets: Yes currently only supported csv files, additional files are coming.



## 18. Citation

When using this in scientific contexts, the proper BibTeX reference for the data insights is the below
```BibTex
@software{DataInsightAnalyzer,
author = {Stellar Analytics Group},
title = {DataInsight Analyzer},
year = {2024},
url = {https://github.com/YourGitHubLink}
}
```

## 19. Contact
If having further difficulties reach Stellar Analyst Group:

Contact via email, and support can reach back out with resolution or troubleshooting support at analystgroup.info@companyname.com or through logging support tickets using portal link [Community Forum URL Link ]