# PyDataStream Processor & Analytics Library

## Description

This PyDataStreamProcessor is a library focused on efficient, scalable in- and offline stream processing and basic analytical tasks. It leverages a hybrid approach combining real time streaming via Zero MQ and traditional file system batch loading to create highly flexible pipeline options. The primary goal is to offer easy-to-integrate data manipulation and aggregation tools for rapid prototyping & operationalizing data workflows that ingest from varied source and deliver real time dashboards.

The core component of this system is the Data Processor, capable of consuming from various inputs and executing a series of configurable data manipulations. A robust configuration allows the Data Processor to dynamically adapt as the data source requirements vary. Data can be pushed directly into the processors and consumed as quickly as possible. Additionally batch data can be loaded, allowing historical analysis.

A key differentiator is its support for a wide range of message formats including JSON, CSV, plain text, & binary data. This enables seamless integration with diverse data streams. Additionally the modular architecture allows users extend processing functions with minimal overhead promoting flexibility & custom integrations. We have implemented some basic aggregation functions like windowed counts and rolling sums for demonstration, but the architecture can support more robust analytical tools.

PyDataStream is designed with a high emphasis on maintainability. Clean code standards, extensive test cases for core functionality, and comprehensive API documentation are provided. This makes it easy not only to use Py DataStream for your own data pipelines; additionally the system makes it straight forward to contribute custom processing steps and extend the library' s functionalities.

We' re constantly working on expanding the features and improving existing functionalities based on community feedback. Future enhancements will include better real-time dashboard integration, advanced pattern detection and anomaly identification, and more complex analytical functions.  Stay tuned! 

## Installation Instructions

Before installing PyDataStreamProcessor you must have Python >= 3.8 installed. Additionally ensure you have pip installed. The pip tool can usually be found in most Python installation distributions. Verify the version with a simple console check. The command `python3 -- version` will show you the Python installation. Verify the existence of Python and pip before beginning the install.

To begin, create a new virtualenv to encapsulate the project dependencies from global environment changes. This ensures isolation and helps managing your projects. This prevents accidental versioning conflicts with any other existing projects. Use the following command to create a Python virtual enviroment.

``` bash
  mkdir data_pipeline
     cd datapipeline
     python3 -m venv env
```

Once you create a new project enviroment. It is critical that you *activate* it. On Linux this can be done using:   `source env/bin/activate`. For MacOS:   `./env/bin/activate`.  For Microsoft Windows:  `.\env\Scripts\activate`. The environment will indicate that you are activated by prefixing the terminal prompt with ` (env) `

Now to begin, install the requirements file.  The requirements files contains the list all necessary packages. This helps to guarantee that all project's dependencies are consistent across machines. To install all packages in the file, simply run this following command

```bash  
pip install requirements.txt
```

Ensure that all modules have been installed correctly. To confirm run the following command:
```bash
pip list
```
This will print all packages installed within the environment. Check this list includes `ZeroMQ`. If you see `Requirement is satisfied` for all dependencies, the installation process has succeeded.

If you encounter errors relating to ZeroMQ during the install, try the following. On Linux distributions it may require installing the ZeroMQ shared library using the package manager. For Debian-based systems ( Ubuntu ):
``` bash
sudo apt- get install libzmq3-dev
For Fedora and similar systems:

  sudo dnf install zeromq-devel
For MacOs ensure you've installed dependencies like `pkg- config`.
```

If you're facing any installation issues please check our Github issue tracker to see if others are facing the same error and if it has been resolved. We also welcome your submission of bug reports.

After all packages are confirmed, you should test basic functionality. Create a test file to verify that the core functionality is available.

## Usage Instructions

After installing Py DataStreamProcessor, you can begin testing the pipeline functionality through a simple Python script or by running the built in demo. First you need a ZeroMQ publisher set up. This script will simulate a data stream for a testing purposes. Create the script below as `publisher_test.py`. Ensure all dependencies listed are already installed.

   ```bash
     from zmq import ZMQError, Context
     import zmq
     import time
    
   port = 5555 # define the publisher port
    
        # initialize connection
   context = zmq.Context()
   publisher = context.socket(zmq.PUB )
   address_publisher  = f"tcp://*:{port}"   # define the publisher port
    
     #Bind the port
     publisher.bind(address) # publisher will accept any IP
     print(f"Ready to publish on the following publisher connection: {address}")      # print the address
    

   # loop
   i = 0
   # while True:
   while i < 100:  	 
     msg = f"data-{i}" 
     message_to_send   = f"sensor:temperature, message:{msg}" # format message
     print(str( i ) + ":" + str( msg )) 
     message_to_send   = f"sensor:temperature; { message_to_send }"  	     # format the entire line for sending
     # publisher.send_string(message )
     publisher. send_string( f'{message }') #Send string
     # print ("Sent Publisher: " +  f' {msg }')
     publisher sendString(msg_to_send) 
     

     # sleep one second to avoid over-consuming bandwidth
     i += 1 # increment
     #time. sleep(1.0 )
   print("Completed")   
   context.term()  # close the connection  # print message and pause briefly
   ```

Then run the publisher:   ` python3 publisher_ test.py  `. In the same terminal create the script below as `consumer_ demo.py`. This file will show how the processor consumes. This file will show how the processor consumes. It connects to the publisher.

   ```bash
    from zmq import Z MQError, Context
    import zmq
    # define connection details
    publisher   = "tcp:// localhost: 5555"  # publisher address
    # create the context
    context    = zmq.context ()
    # create a socket 	
    socket     = context.socket(zmq. SUB )# subscriber socket

    # set subscriptions  
    socket   .setsock option (zmq.SUBSCRIBE,b"sensor: temperature")  # set subscription
    socket    . connects(f"{ publisher }")  # connect
    # receive and print messages
    poller   = zmq.poll(1) # create an empty list
    poller   . wait (socket)   # poll
    message    = socket.recv_ string ().decode () # retrieve messages. 
    # message = socket  . recV () #retrieve messages. 
    # message = message.decode () #retrieve the messages
    print(msg) #print

    #print(str( msg ))

    # print(f"Received: { msg })
    while True  : # keep running
    # print(message  )
    message    = socket.recv_string ().decode() # retrieve the string

    # print(str )
    print(message)  # Print message received. 
    print("Received" + message) # print
    # print()
   ```

To test basic processing functionality using commandline arguments use: ` python data_stream_processor.py --config config.yaml`. The configuration file defines the input source and processing steps.  Advanced processing can be done with API calls or by extending the existing processing functions to create custom pipeline steps tailored to your needs.

## Configuration

The PyDataStreamProcessor is configured via a YAML file specified using the  `--config` argument when running the main script. This provides a centralized and easy- to- manage way to define the pipeline settings, source information, and processing parameters.

The YAML file structure has three main parts: ` source`, `processor`, and `output`. The `source` block defines the data source type (ZeroMQ or CSV file ) and associated connection information like addresses and port numbers. The ` processor`  section lists the processing steps. It can include filtering functions, aggregation steps, and transformations. Each processing function needs configuration like input data fields or calculation formulas. The `output`  block configures the destination. 

For example, a `config.yaml` file might look like the following:
```yaml
 source:
   type: zeromq
    address: tcp://"localhost:5555" 
   topic:  "sensor:temperature "
 processor:  # processor
     - name :   " filter_low  " # processor function to use.
       function: filter_low_temperature # name of the process function
      args:   # function specific configurations
       threshold:   " 1 0 "  # threshold
output  :   # output configuration
    type:  "console" # output type console,  
```

Environment variables can also configure the processing steps if the configuration file is not available for a certain parameter or if environment-specific settings are used. The environment variables will override configurations set in the YAML file.  Environment variable names should match configuration keys. For instance, to override the temperature threshold, set the environment variable `TEMPERATURE_ THRESHOLD` to your desired value.

For example, if you wanted to modify the threshold in the filter_low function you should do:  `export TEMPERATURE_THRESH = 1 5`. This sets the `temperature_  threshold` variable to `1 5`. This will change the behavior from the configuration file.

## Project Structure 

The PyDataStreamProcessor project is organized into several key directories:

*   **`src/`**: Contains the core source code for the data processing library, including the DataProcessor class, processing functions, ZeroMQ communication logic, and the main script.
*   **`tests/`**: Holds unit tests and integration tests to ensure code correctness and functionality across various scenarios and edge cases.
*   **`configs/`**:  Contains example configuration files for different deployment scenarios. Includes sample ` config.yaml` file. Also stores default values.
  
    

*   **`docs/`**: Documentation source files for generating API documentation. 
   
*   **`data/`**:  Contains sample input data files used for testing and demonstration.
*   **`utils/ `**: Helper functions for utilities and reusable code snippets.  
*   **`pyproject.toml `**: Project metadata & configurations.
*   **` requirements.txt`**: List of all dependencies required. 
*   **README.md**: The README file, providing project documentation.

   
The overall architecture is modular. Processing functions are independent of the core DataProcessor class, enabling seamless integration of new functions.

## Contributing

We welcome contributions from the community! To help ensure a smooth and consistent development workflow, follow these guidelines:

First, fork the repository on GitHub. Then clone the forked repository locally.  Next, create a new branch for the feature or bug fix you are working on, using a descriptive name. For instance, "fix-zero mq-connection" or "implement-rolling- average". Ensure your code follows PEP 8 style guidelines using `pylint` or other code quality checks.

Before submitting your changes, create a pull request.  Include a detailed description outlining the changes you've made and the reasoning behind them. Add relevant tests to demonstrate your changes' behavior. Make sure all tests have passed successfully.

To maintain code quality, all contributions are subject to code review. Be responsive to feedback and iterate on the patch as necessary. 

## License

This project is licensed under the MIT License.  This license allows you to use, copy, modify, merge, publish, distribute and/or sell copy of the software. It provides a broad range of freedoms while also requiring you to include the original copyright notice and license terms in any copy or derivative works. This ensures that the open source nature of the library remains protected.  

## Acknowledgments

This project wouldn't be possible without the contributions of the amazing open- source community. We'd like to acknowledge the following:

*   The Zero MQ project for providing a high-performance messaging library.
*   The Python community for creating a versatile and powerful language. 
*   NumPy and Pandas for providing essential data manipulation libraries.
*   Pytest for providing a robust testing framework. 
*   The various contributors to the Py YAML library.

## System Architecture

The PyDataStreamProcessor system is structured around the Data Processor class, which serves as the central component for consuming, processing, and emitting data streams. The Data Processor is designed to be highly modular and configurable. It uses a plug and play approach.

The core architecture can be summarized as follows. The system has a source module that ingests raw data from different sources such as ZMQ topics or CSV files. The processing module then executes a series of processing steps as specified in a YAML configuration. Each step applies a transformation or filtering operation to the stream. Finally, the output module dispatches the processed data to a destination. It can use console logs or file writes to provide results.

## API Reference

The core `DataProcessor` class has two major methods ` start `and `  stop ` which initiates data streaming, as well as shuts it down gracefully when no longer needed. This is a key component that facilitates the main flow in PyDataStreamProcessor .  Additionally we include helper functions `read csv file, publish message on the ZeroMQ, process with the given configurations.`
The functions `process function, publish function.` These are called by other internal methods of data stream process class to iterate across steps defined. 

```python
from streamprocessor import DataProcessor

processor = DataProcessor (
      configuration   =  { ' config } )
processor .start (  )

...   

processor   .stop(  )

```
` DataProcessor`  class: `  Start(   configuration: dict ),   Stop,  ` Read Data- ` process Data,  Run tests ` . All parameters must follow a defined structure or an explicit error thrown to user
## Testing

Testing is an integral part of the development process to ensure functionality across the board and code quality is guaranteed. PyDataStreamProcessor utilizes Pytest. Unit tests can be performed for isolated processing functions to make it clear if each step operates accurately, regardless the context and environment .  Integration tests validate complete data pipelines end-to-end testing, to simulate the system running through an example workflow, ensuring all modules interact harmoniously and provide intended behavior.. Run test using :

```bash
python -m pytest -v tests/ 
```

The integration setup simulates real life usage patterns using a combination test datasets as a data streams for integration and system verification..
## Troubleshooting

A common issue is dependency resolution during install, often stemming from incompatibilities across different library versions or system libraries..  If ZeroMQ isn install or missing, double-check system libraries, ensure that the correct version are present and correctly installed, as specified earlier on..
If a ZeroMQ error appears when sending/receiving:  Check your port configurations, firewalls may also impede the message flow, disable temporarily.   Verify the connection strings, if they contain any typing or spelling mistakes that cause the error, correct these errors immediately to ensure the message transfer.. 

For unexpected processing result ensure data formats correspond correctly. If your CSV data uses inconsistent separators ensure configuration correctly defines delimiter in processing.
Finally, examine configuration parameters if the processing fails unexpectedly or generates error. Double- check if parameters match with your data schema..   Review error trace backs thoroughly to determine exact source issue . If issues arise contact community or contribute tests or issues .
## Performance and Optimization

Performance bottlenecks can typically be observed with the Data Processor in a production settings. ZeroMQ messaging library can offer substantial improvement over other message passing methods, and its speed makes data stream very fast in most use cases. 
For improved performance, caching data in local buffers or databases helps reducing repeated network or storage look ups, significantly improving response and reducing network I/O..
Additionally concurrency and/or asynchronism techniques may further optimize resource and task utilization;  for computationally demanding functions use multi- process, parallel threads to divide load efficiently; use asynchronous libraries for tasks, allowing for parallel execution without the burden to thread creation. Profiling and load Testing using benchmark suites is highly recommend as the system scales in performance

## Security Considerations

Protect your project through a variety of practices to avoid common attacks in real world production deployments, and maintain high standards security protocols, some practices you must be following are; avoid storing credentials like secrets in code, but use environmental configurations instead. Always implement strict and comprehensive validations on input from data- streams; this reduces potential code execution issues . Use encrypted transport protocols such ZMQ, when communicating between Data- Processors..   Ensure regular vulnerability and patch update to address new risks and mitigate attacks, stay vigilant about keeping software updated with all recent changes, including the core Data Processing and Zero MQ components.
   Report and handle vulnerability issues with the best practice.
## Roadmap

Here what's upcoming and on- the -roadmap : Enhanced Realtime Monitoring Integration to build intuitive dashboards to observe Data- streams. Pattern Detection integration takes a fundamental place as one major enhancement. It enables anomaly, detection, to identify unexpected patterns or behaviors. Advanced Analytical Features like window functions can expand data manipulation capabilities, providing greater processing. Version- 1 5: Improve ZeroMQ, add data transformations;  Release candidate for the Data Processing.   V 2-1; Data- Processing integration, enhanced monitoring integration to support advanced dashboard features . V3-2; Add anomaly Detection; and enhanced pattern- detection algorithms

## FAQ (Frequently Asked Questions)

Q: I can see a `zmq  bind ` errors what can cause it? A: This is commonly caused due the same ZMQ address port already occupied by existing service/instance; verify other instances and services are no listening to the ZMQ ports, change the configurations, and ensure port is free; Q:  How to configure a Plugable data-processing? The plugin function need follow certain standards for name registration .
A; All configured data processers must share function signature.  The signature will need parameters of `rawdata`, configuration file and `processing function `. 
   This helps keep all code in crucible.   Q: What is the most important factor? : It can change rapidly based on usage .  

## Citation

For citation to reference use:  [Name](https://GitHub .com ) , [year ] . ` DataStream Processor ` [software library] . [Repository url . ]. The reference can be used if Py Data- Stream library contributes toward a academic publications and or projects
## Contact

You can find additional support with following links : github repository for all open discussion if issues appear:
` github  url  / DataStream` .To provide more direct feedback or report issues you contact:   email support.data @email.  We' d love hearing more.
