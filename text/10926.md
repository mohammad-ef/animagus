# DataInsight Analyzer - Predictive Analytics Engine

## Description

Developed by the Stellar Analytics Group to provide a comprehensive, user- friendly, and highly scalable analytics service. This project leverages a distributed data processing framework to enable the efficient analysis of large data. Our goal is to democrat access to advanced predictive models by providing a simplified and customizable analytical environment. This includes data pre- processing, model training, testing, validation, and deployment all handled seamlessly. 

The architecture is designed around a plugin based modularity allowing users or other services to quickly add new analysis functions/ algorithms, or new data types. This ensures our tool remains adaptable. It can integrate with a broad range of data sources, supporting CSV, JSON, Parquet, and cloud database services like AWS S3 & Google Cloud Storage. The engine also provides visualization capabilities for exploring patterns, identifying outliers, & assessing the efficacy. 

We solve the challenge of extracting valuable insights from increasingly complex datasets, especially where time is critical. By providing a platform where data scientists * and citizen analysts -* can easily build, test, and deploy their model. The project supports both batch and (with a specialized configuration) streamed input. This flexibility caters for a diverse of business use scenarios, and provides immediate, actionable business intelligence.

This analytics engine is particularly relevant to industries that are highly data focused such as e- commerce, marketing, and finance, which can improve their forecasting abilities & operational efficacy. This engine is designed for cloud environments, leveraging autoscaling. This makes the solution both highly reliable and cost effective to operate, as you only use compute resources where you needed it.

Furthermore the tool provides a comprehensive audit and versioning trail on model training and deployment processes to ensure reproducibility, traceability to satisfy regulations, and to support model retraining when conditions change over time. It's designed to be robust & maintain simple to operate, even with minimal administrative involvement once the initial deployment is complete and configurations are in place to monitor the environment.



 ## 1. Installation Instructions 

This guide details installing DataInsight Analyzer. Before installing ensure you are comfortable with Python and basic system administration tasks. The installation process involves multiple steps from setting up your environment & downloading packages. 

We recommend creating and activating a Python venv:

```bash
  mkdir datainsight_venv # Create a new project directory
  cd datainsight_venv
 ```

Create a Python 3. 8 or newer environment: 

```shell
   python3 - m venv .venv # Create the environment directory named venv
   source .venv/bin/activate # Activates this venv
  ```

Clone the DataInsight Analyzer repository: You will need git:

    ```shell
    sudo apt update #For Linux - updates package list, not necessary on MacOS, may require admin privileges to update system packages.
    git clone <YOUR_REPOSITORY_ LINK> 	 
    ```

Install required packages via `pip`: 

```shell
pip install - r  requirements.txt # Install dependencies listed in the file
  ```

This step may vary on different platforms, ensure pip is using the venv and not an external one. For MacOS use `pip3` instead of ` pip`, as `pip` defaults may link to an outdated version.

For Windows users, ensure the Python interpreter within the virtual environment is associated with your `pip install` command. This can usually be handled by activating the venv.

The `requirements file` contains all the required python and system level tools. The system level tools required may have pre-requisite installs, ensure your base system is properly set for these dependencies, or run the system level installation guides prior to installing these requirements.

If you encounter issues while installing a package, check your pip and Python versions, and search for potential compatibility problems on the internet to ensure compatibility issues are identified, or you have the required libraries/ packages correctly set. 

If there are problems during the install, you will often see messages about package versions, ensure compatibility is considered, or consider using an older package versions in the requirements.txt if a package conflicts. 

Once you've installed the libraries, you can test the installation:
```shell
python - m  pytest # This will run the automated tests included in the codebase
  ```

This step will test the installed dependencies with a series of automated functions, this confirms a successful installation, and the code is functional.  If there are errors, please review the logs.

After successful testing, the analyzer can be configured as shown in the next chapter. This will ensure a successful and complete system install, with all tools in a working condition.   

  ## 2. Usage Instructions
   
To initiate analysis from the command line, run the `analyze` script within your DataInsight Analyzer directory, using the command ` python ./analyze.py - i <input data> - o <output file > - m <selected model> `- This command accepts several parameters to control the analysis process.

The basic usage is:
```bash   
  python ./analyze.py -i data.csv -o results.json -m linear_regression  
```
This performs a linear regression model against 'data.csv 'and saves the model in JSON format to 'results .json '.

To run a full automated test suite:
```bash   
   python -m pytest 
```
This command executes the test suite to validate the functionality of the installed packages. The test suite will run unit, functional, and integration tests across the codebase, to test various scenarios. This step is important, to ensure a successful, fully running system.

 For a more advanced configuration, such as using a streaming input, specify the `--stream `flag when running analysis:  ` python ./analyze.py -i stream.txt -o results_ stream.json -m random_forest - stream`. This tells the program to read the data one element at a time, allowing for a quicker analysis.  

If you have specific requirements, such as custom parameters for a given algorithm ( for example, setting regularization for ` linear_ regression`), these can be included in the input file and parsed accordingly, and passed into a custom training loop.  

If you have multiple models that need to be compared for a specific analysis task, run the `analyze. py ` script several times, one for each mode.

 To get more information about the arguments, or help with the analyzer, simply run the script with the command: 
```bash
 python ./analyze. py -- help
```
This will print a detailed message about the different parameters available to the analyzer.   

  ## 3. Configuration

To configure the project, edit the ' configuration. yaml ' file, that resides in the `configs ` directory. This is where you can specify default analysis parameters like dataset locations, file storage locations, and preferred algorithms.

The `configuration . yaml ` file is structured around a hierarchical configuration system:

    ```yaml
    #Default Configuration Parameters
    input_path : " data / raw / input.csv "    #Default file location. 
    output_path : " output / results . json " # Default file storage.
    selected_algorithm :  "  random_ forest "    # The algorithm to run.
    data_source : "CSV"
    data_schema : { # Define the schema if you are working with specific file formats. 
      "  feature1 " :"float",   # Defines the datatype. 
      "  feature2" :"Integer "      # Define datatypes here.  

    };  
```

You can override individual configuration parameters via command-line arguments passed during execution.  For example, to use an alternative output directory, run the ` analyzer ` script like this ` python ./analyzer .py -o alternate_output . json `- This overwrites the default specified in ` configuration . yaml `

The ` data - schema ` key lets you pre-define the datatype that you are expecting.  If this is left blank, a type inference system will determine these datatypes.  If there are problems, ensure the schema is correct.

  To modify the system's log level, you can also set it within this file as a variable `  log  = "DEBUG"` This will help debug problems with the file, as a more detailed log will be created and stored on disk, to aid in problem resolution.. 

The `configuration  . yaml  ` allows easy adjustments to the system without changing the source code. It provides a flexible, maintainable and robust system configuration to tailor this application for a diverse of needs.   

  ## 4. Project Structure

The DataInsight Analyzer repository is structured to enhance code organization and maintainability. Below is a breakdown of the main folders:

* **`src/`**: Contains the core source code, including algorithms modules, data loading & preprocessing components, and model training & evaluation logic and utility functions
*  **`tests/`**: Holds all the unit, integration, and system-level tests, to confirm the codebase meets the requirements. 
*  **`configs/`**: Stores configuration files such as  `confiquration . yaml ` that define default project settings.
* `docs/`: This directory contains all documents, including the `README`, as is. 

* **`logs/`**: Contains the file logs, and diagnostic messages from various functions, including the analyzer function and testing functions.
 * **`data/raw/`**: Contains example raw input datasets that may be useful for testing, or experimentation. 
*   **`output/`**: This is where the analyzer saves the resulting output.
* **`models/`**: Contains the generated models during the model training step, as well model checkpoints. This directory helps in the process of model versioning and rollback

This organization simplifies navigation and collaboration among developers, enabling clear separation of responsibilities and a scalable architecture. 

  ## 5. Contributing

Contributions to DataInsight Analyzer are welcome and appreciated.  To facilitate a smooth and efficient workflow, follow the steps below:

First, please report issues or enhancement requests through our GitHub issue tracker. Before starting a new feature or bug fix, open an issue. Describe the problem/feature in details, including the expected behavior, and include any relevant reproduction steps.

To contribute, create a fork and a new branch:

```bash   
    git clone < your repository - link >   # First Clone it. 
    git checkout -b feature / new -function    # Then create feature branches!  
```

Adhere to the project's coding standards which includes PEP8 for Python coding standards (consistent formatting, naming conventions, code style).

Submit a pull request, after your new branch and changes have been confirmed working with the unit & functional tests passing. Include a descriptive title and a detailed description of the modifications.

All pull requests will reviewed by the core maintainers.  Code quality, thorough testing, clear documentation, adherence to coding patterns, and impact on the overall project will be assessed. Please ensure all tests passes and there are no regressions.  



  ## 6. License

DataInsight Analyzer is licensed under the MIT License. 

You are free to use, copy, and distribute the software, subject to the following conditions:

*   The software must be accompanied with the complete, unmodified copyright notice of the License. 
*   The software is provided with no warranty - the user is responsible for their own actions. 



  ## 7. Acknowledgments

We are grateful to the open source community, without which, this project would not be possible. Specific acknowledgements include:  

The Pandas library, for data frame and data analysis capabilities. Scikit-learn, providing a wide range of machine learning models and tools for building, evaluating, and deploying analytical models. NumPy, for efficient array and matrix operations, essential for scientific computing and data analysis. Pytest, for a robust and flexible automated testing framework, which allows easy integration testing. 

We also wish to acknowledge our core research team at Stellar Analytics, who dedicated many hours in the research, development, & testing of this platform. 




  ## 8. System Architecture 

The DataInsight Analyzer follows a modular, layered architecture, designed for scalability, flexibility, and maintainability. It can be visualized as consisting of the following layers: 

1. **Input Layer**: responsible for handling diverse data source formats - including CSV, JSON, SQL Databases, & API endpoints -  It performs pre-processing like format parsing, type conversions and basic validations. 
2. **Preprocessing Layer**: The pre-processing step, which can clean up and modify your raw data for better insights from model. Includes handling missing values , outlier removal, data standardization and feature transformations 
3. **Model Layer**: It comprises several model types that include Linear Regression, Random Forest and neural networks. 
4.  **Evaluation Layer**:  It is responsible for assessing and refining machine learning outcomes with key indicators to gauge efficiency

Modules such as a model training engine that manages the lifecycle and deployment and management tasks of ML models in various platforms is a major architectural feature. The plugin base structure allows the extension with algorithms from other frameworks as well as additional file types to ingest and manage data as it evolves..



 ## 9. API Reference

DataInsight Analyzer is intended for CLI access. The primary entry point is the `analyze.py` script which allows execution via CLI command. Below lists a reference with common usage

   **--input `INPUT_FILE`**: (Required) Path to the input dataset.
      -  Supported Formats are: `CSV`,`JSON`,`Parquet`.

   **--output OUTPUT_FILE:** (Required) Output path & name of model. 
       - Supported format `JSON`, & `pickle `. 
       
    **-m --model MODEL_NAME:** (Required) Specify machine-learning `random-forest`,`lin_reg`.  The tool will select appropriate algorithms based on selection 

  **-- stream : ** Flag used for continuous streaming. Set input and configure streaming accordingly to ensure a stable and predictable model 

    Example command for model execution with CLI 
    `analyze.py- i  dataset .csv- m linear  ` - 



 ## 10. Testing

We rely heavily on automated testing to ensure code reliability and detect regressions quickly. Testing is handled with the popular python pytest framework

Run unit test
`python -m pytest src` : this run the core functionality of code tested

Functional tests : These are integrated within various modules of source and run with same pytest call
To create custom functional testing scripts add it under a ' tests `folder` directory 

Ensure tests coverage covers 80-90% code, as we expect to ensure high code maintainability. This includes all unit test for all code blocks as much as reasonably can



  ## 11. Troubleshooting

1.  **"ImportError: No module named ' pandas "'**: Indicates pandas isn t installed, re- run install from step above and ensure it gets loaded in virtual environments,
2. **FileNotFoundError: "[Errno 2] No such file or directory":** Check paths to your datasets.
3.  **RuntimeError: Failed to allocate memory":** Increase Python memory or use a lower memory data set, or upgrade your server
    .



 ## 12. Performance and Optimization

DataInsight Analyzer's scalability can be optimized in the configurations

* Implement batch mode, rather than processing elements at once in single thread 
* Optimize memory by preallocating arrays
*  Leverage GPU for training complex Neural models
* Use appropriate algorithms with optimized implementations



  ## 13. Security Considerations

Always secure the analyzer by following secure configurations
    - Avoid putting API keys and database secrets within config YAML. Use environment variable 
   - Ensure the user that runs has appropriate & secure privileges 
 - Input Data: Implement rigorous validation & filtering mechanisms before passing inputs into your analyzer to ensure safety of code & environment.



  ## 14. Roadmap

1. [ ] Feature Request to Support More Model Types, for more flexibility and adaptability to different data requirements. 
2. [ ] Implement automated ML features that will auto train a complete ML lifecycle from training to testing & deploy
3. [X] Integration With Popular Business BI and data analytics dashboard platforms



  ## 15. FAQ (Frequently Asked Questions)

  Q: I get an Error that says, Module `NotFound' , How Can Resolve this

  Check Python Virtual environments, if this is activated.  If it doesn not work install with command, or run again the setup.txt install.  Also Check pip versions to ensure this does work. 
   

## 16. Citation

We are open for citation requests as the Data Insight Engine becomes a useful research product for your projects, or other work in a development context. The following sample bibtext entry for the tool

 ```BibTeX
@misc{DataInsightAnalyzer,
    author = {Stellar Analytics Group},
    title = {DataInsight Analyzer - Predictive Analytics Engine},
    year = {2024},
    url = { < your -repository-url> }
}
 ```
We hope it will serve a wide use.   

 ## 17. Contact

If you find bugs or need support or just wish to contribute or give feedback, the contact info as following, for a well organized response 
 - **Email: DataInsightsupport@stellranalytics. com **
- GitHub issue tracking 

Feel free to create new features and send us PR requests.