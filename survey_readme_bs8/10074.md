# PyDataStream Processor & Analytics Library

## Description

This PyDataStreamProcessor is a library focused on efficient, scalable in- and offline stream processing and basic analytical tasks. It leverages a hybrid approach combining real time streaming via Zero MQ and traditional file system batch loading to create highly flexible pipeline options. The primary goal is to offer easy-to-integrate data manipulation and aggregation tools for rapid prototyping & operationalizing data workflows that ingest from varied source and deliver real time dashboards.

The core component of this system is the Data Processor, capable of consuming from various inputs and executing a series of configurable data manipulations. A robust configuration allows the Data Processor to dynamically adapt as the data source requirements vary. Data can be pushed directly into the processors and consumed as quickly as possible. Additionally batch data can be loaded, allowing historical analysis.

A key differentiator is its support for a wide range of message formats including JSON, CSV, plain text, & binary data. This enables seamless integration with diverse data streams. Additionally the modular architecture allows users extend processing functions with minimal overhead promoting flexibility & custom integrations. We have implemented some basic aggregation functions like windowed counts and rolling sums for demonstration, but the architecture can support more robust analytical tools.

PyDataStream is designed with a high emphasis on maintainability. Clean code standards, extensive test cases for core functionality, and comprehensive API documentation are provided. This makes it easy not only to use Py DataStream for your own data pipelines; additionally the system makes it straight forward to contribute custom processing steps and extend the library' s functionalities.

We' re constantly working on expanding the features and improving existing functionalities based on community feedback. Future enhancements will include better real-time dashboard integration, advanced pattern detection and anomaly identification, and more complex analytical functions.  Stay tuned! 

## Installation Instructions

Before installing PyDataStreamProcessor you must have Python >= 3.8 installed. Additionally ensure you have pip installed. The pip tool can usually be found in most Python installation distributions. Verify the version with a simple console check. The command `python3 -- version` will show you the Python installation. Verify the existence of Python and pip before beginning the install.

To begin, create a new virtualenv to encapsulate the project dependencies from global environment changes. This ensures isolation and helps managing your projects. This prevents accidental versioning conflicts with any other existing projects. Use the following command to create a Python virtual enviroment.

``` bash
  mkdir data_pipeline
     cd datapipeline
     python3 -m venv env
```

Once you create a new project enviroment. It is critical that you *activate* it. On Linux this can be done using:   `source env/bin/activate`. For MacOS:   `./env/bin/activate`.  For Microsoft Windows:  `.\env\Scripts\activate`. The environment will indicate that you are activated by prefixing the terminal prompt with ` (env) `

Now to begin, install the requirements file.  The requirements files contains the list all necessary packages. This helps to guarantee that all project's dependencies are consistent across machines. To install all packages in the file, simply run this following command

```bash  
pip install requirements.txt
```

Ensure that all modules have been installed correctly. To confirm run the following command:
```bash
pip list
```
This will print all packages installed within the environment. Check this list includes `ZeroMQ`. If you see `Requirement is satisfied` for all dependencies, the installation process has succeeded.

If you encounter errors relating to ZeroMQ during the install, try the following. On Linux distributions it may require installing the ZeroMQ shared library using the package manager. For Debian-based systems ( Ubuntu ):
``` bash
sudo apt- get install libzmq3-dev
For Fedora and similar systems:

  sudo dnf install zeromq-devel
For MacOs ensure you've installed dependencies like `pkg- config`.
```

If you're facing any installation issues please check our Github issue tracker to see if others are facing the same error and if it has been resolved. We also welcome your submission of bug reports.

After all packages are confirmed, you should test basic functionality. Create a test file to verify that the core functionality is available.

## Usage Instructions

After installing Py DataStreamProcessor, you can begin testing the pipeline functionality through a simple Python script or by running the built in demo. First you need a ZeroMQ publisher set up. This script will simulate a data stream for a testing purposes. Create the script below as `publisher_test.py`. Ensure all dependencies listed are already installed.

   ```bash
     from zmq import ZMQError, Context
     import zmq
     import time
    
   port = 5555 # define the publisher port
    
        # initialize connection
   context = zmq.Context()
   publisher = context.socket(zmq.PUB )
   address_publisher  = f"tcp://*:{port}"   # define the publisher port
    
     #Bind the port
     publisher.bind(address) # publisher will accept any IP
     print(f"Ready to publish on the following publisher connection: {address}")      # print the address
    

   # loop
   i = 0
   # while True:
   while i < 100:  	 
     msg = f"data-{i}" 
     message_to_send   = f"sensor:temperature, message:{msg}" # format message
     print(str( i ) + ":" + str( msg )) 
     message_to_send   = f"sensor:temperature; { message_to_send }"  	     # format the entire line for sending
     # publisher.send_string(message )
     publisher. send_string( f'{message }') #Send string
     # print ("Sent Publisher: " +  f' {msg }')
     publisher sendString(msg_to_send) 
     

     # sleep one second to avoid over-consuming bandwidth
     i += 1 # increment
     #time. sleep(1.0 )
   print("Completed")   
   context.term()  # close the connection  # print message and pause briefly
   ```

Then run the publisher:   ` python3 publisher_ test.py  `. In the same terminal create the script below as `consumer_ demo.py`. This file will show how the processor consumes. This file will show how the processor consumes. It connects to the publisher.

   ```bash
    from zmq import Z MQError, Context
    import zmq
    # define connection details
    publisher   = "tcp:// localhost: 5555"  # publisher address
    # create the context
    context    = zmq.context ()
    # create a socket 	
    socket     = context.socket(zmq. SUB )# subscriber socket

    # set subscriptions  
    socket   .setsock option (zmq.SUBSCRIBE,b"sensor: temperature")  # set subscription
    socket    . connects(f"{ publisher }")  # connect
    # receive and print messages
    poller   = zmq.poll(1) # create an empty list
    poller   . wait (socket)   # poll
    message    = socket.recv_ string ().decode () # retrieve messages. 
    # message = socket  . recV () #retrieve messages. 
    # message = message.decode () #retrieve the messages
    print(msg) #print

    #print(str( msg ))

    # print(f"Received: { msg })
    while True  : # keep running
    # print(message  )
    message    = socket.recv_string ().decode() # retrieve the string

    # print(str )
    print(message)  # Print message received. 
    print("Received" + message) # print
    # print()
   ```

To test basic processing functionality using commandline arguments use: ` python data_stream_processor.py --config config.yaml`. The configuration file defines the input source and processing steps.  Advanced processing can be done with API calls or by extending the existing processing functions to create custom pipeline steps tailored to your needs.

## Configuration

The PyDataStreamProcessor is configured via a YAML file specified using the  `--config` argument when running the main script. This provides a centralized and easy- to- manage way to define the pipeline settings, source information, and processing parameters.

The YAML file structure has three main parts: ` source`, `processor`, and `output`. The `source` block defines the data source type (ZeroMQ or CSV file ) and associated connection information like addresses and port numbers. The ` processor`  section lists the processing steps. It can include filtering functions, aggregation steps, and transformations. Each processing function needs configuration like input data fields or calculation formulas. The `output`  block configures the destination. 

For example, a `config.yaml` file might look like the following:
```yaml
 source:
   type: zeromq
    address: tcp://"localhost:5555" 
   topic:  "sensor:temperature "
 processor:  # processor
     - name :   " filter_low  " # processor function to use.
       function: filter_low_temperature # name of the process function
      args:   # function specific configurations
       threshold:   " 1 0 "  # threshold
output  :   # output configuration
    type:  "console" # output type console,  
```

Environment variables can also configure the processing steps if the configuration file is not available for a certain parameter or if environment-specific settings are used. The environment variables will override configurations set in the YAML file.  Environment variable names should match configuration keys. For instance, to override the temperature threshold, set the environment variable `TEMPERATURE_ THRESHOLD` to your desired value.

For example, if you wanted to modify the threshold in the filter_low function you should do:  `export TEMPERATURE_THRESH = 1 5`. This sets the `temperature_  threshold` variable to `1 5`. This will change the behavior from the configuration file.

## Project Structure 

The PyDataStreamProcessor project is organized into several key directories:

*   **`src/`**: Contains the core source code for the data processing library, including the DataProcessor class, processing functions, ZeroMQ communication logic, and the main script.
*   **`tests/`**: Holds unit tests and integration tests to ensure code correctness and functionality across various scenarios and edge cases.
*   **`configs/`**:  Contains example configuration files for different deployment scenarios. Includes sample ` config.yaml` file. Also stores default values.
  
    

*   **`docs/`**: Documentation source files for generating API documentation. 
   
*   **`data/`**:  Contains sample input data files used for testing and demonstration.
*   **`utils/ `**: Helper functions for utilities and reusable code snippets.  
*   **`pyproject.toml `**: Project metadata & configurations.
*   **` requirements.txt`**: List of all dependencies required. 
*   **README.md**: The README file, providing project documentation.

   
The overall architecture is modular. Processing functions are independent of the core DataProcessor class, enabling seamless integration of new functions.

## Contributing

We welcome contributions from the community! To help ensure a smooth and consistent development workflow, follow these guidelines:

First, fork the repository on GitHub. Then clone the forked repository locally.  Next, create a new branch for the feature or bug fix you are working on, using a descriptive name. For instance, "fix-zero mq-connection" or "implement-rolling- average". Ensure your code follows PEP 8 style guidelines using `pylint` or other code quality checks.

Before submitting your changes, create a pull request.  Include a detailed description outlining the changes you've made and the reasoning behind them. Add relevant tests to demonstrate your changes' behavior. Make sure all tests have passed successfully.

To maintain code quality, all contributions are subject to code review. Be responsive to feedback and iterate on the patch as necessary. 

## License

This project is licensed under the MIT License.  This license allows you to use, copy, modify, merge, publish, distribute and/or sell copy of the software. It provides a broad range of freedoms while also requiring you to include the original copyright notice and license terms in any copy or derivative works. This ensures that the open source nature of the library remains protected.  

## Acknowledgments

This project wouldn't be possible without the contributions of the amazing open- source community. We'd like to acknowledge the following:

*   The Zero MQ project for providing a high-performance messaging library.
*   The Python community for creating a versatile and powerful language. 
*   NumPy and Pandas for providing essential data manipulation libraries.
*   Pytest for providing a robust testing framework. 
*   The various contributors to the Py YAML library.

## System Architecture

The PyDataStreamProcessor system is structured around the Data Processor class, which serves as the central component for consuming, processing, and emitting data streams. The Data Processor is designed to be highly modular and configurable. It uses a plug and play approach.

The core architecture can be summarized as follows. The system has a source module that ingests raw data from different sources such as ZMQ topics or CSV files. The processing module then executes a series of processing steps as specified in a YAML configuration. Each step applies a transformation or filtering operation to the stream. Finally, the output module dispatches the processed data to a destination. This can be standard out or file destination. The system is implemented for concurrency to enhance its ability to manage real-time streaming.  The modular architecture is optimized to provide easy extensibility, making it easier to add and change pipeline functionality.

## API Reference

The `DataProcessor` class (in `src/data_stream_processor.py`) provides the main entry point to process data streams. 

*   **`__init__(config)`**: Initializes the DataProcessor with a configuration file (`config.yaml`).  `config` can be an existing dictionary, a path to file. 
*   **`start()`**:  Initiates the data processing pipeline by setting up data connectivity, parsing configurations, loading data, and starting stream processing threads. It returns None on success. Raises exceptions when setup is invalid or configuration is malformed.
*   **`stop()`**: Shuts down all threads used during runtime
*   **`process_stream()`**: The entrypoint that processes all stream configurations
*   **Processing Function**: Processing modules can use a number of functions.  Filter Low Temp function is provided and uses a parameter of " threshold: " for a specific numeric comparison and value filter .  This can accept string data as parameter values, or numbers to use directly. The aggregation is also done in-line within a process function and uses constants that must match configuration keys to perform the function as required.  

## Testing

We follow the principles to write data tests to validate each of our modules individually, ensuring the system behaves in the required way, and the dependency modules work correctly with their integration to one another.

Run unit tests for specific processing functions and Data Processor modules:
   ```bash
     pytest -v tests/unit_tests/test_data_processor.py  
   ```

The command will run all defined functions, reporting results as passed or failing in a summary. It will print all error messages, to allow you debug quickly and identify where failures occurred in your pipeline implementation. 
The command `pytest -v` run all test in project folders in the directory you run. 

To perform system test:
    ```bash
    pytest  -v test_data_streams_system.py   # system data flow validation  
    ```

To test edge and corner scenarios:   `  python -c ` pytest system.py`. 

## Troubleshooting

If the script cannot load configurations use: `check your ` yaml  formatting, indentation, key validity`  Ensure paths exist if loading from external locations.

ZeroMQ related error messages can result if a ZeroMQ broker or endpoint hasn't become initialized properly before execution and may indicate networking problems: verify `publisher  addresses` match `subscription   endpoints`.   If connection issues persist ensure that all firewall is properly setup to avoid port conflicts between the publishers and receivers

Incorrect data received during processing may mean the ` data filtering   steps aren  correct`, ensure the parameters passed in during  pipeline are configured as intended. Review configuration settings for proper mapping data types, and data source types in *data source configurations *.  Verify input parameters of data processors are correctly typed in order for processing to be performed correctly

Incorrect processing errors can often point at data inconsistencies during transformation steps. Validate all processing step parameters with the proper configuration keys, ensure proper data transformation steps are used when converting different file or string based input sources

## Performance and Optimization

Optimizing data streaming performance involves addressing several critical factors, and ensuring high availability and minimal latency. First use asynchronous operations in the source, data transformations and outputs whenever appropriate. The goal of async operation can allow to maximize resource throughput by allowing operations happen during blocking processes . This can prevent resource bottlenecks

Employ techniques such as efficient message serialization formats for ZeroMQ communications. Binary format reduces communication sizes which reduces transfer and parsing latencies and reduces processing times and increases system scalability

Caching data within transformations can improve response and decrease redundant data calculations for common queries that frequently repeat within pipeline operations. Consider applying data indexing in transformations when working on big data for enhanced query processing, improving performance on large streams with high read operations 

## Security Considerations

Ensure data security within streaming applications with best practice security considerations, including proper handling for credentials and secret information during deployments of the pipelines, such as using secure vaults

Proper validation should occur for input values during processing and filter any suspicious attacks or malicious input attempts by enforcing validation constraints in transformations and sanitizing external input to minimize the impact for possible injection vulnerabilities, and preventing the introduction of malicious elements

Secure all ZeroMQ ports with network security configurations to ensure access restriction to authorized sources. This includes firewall configurations

Implement data logging of processing pipelines, including sensitive events, for compliance, audits and anomaly detections for intrusion or potential compromises that might arise 
    
## Roadmap

The following features and enhancements are planned

* Implement advanced data filtering algorithms such as clustering to enhance stream filtering capabilities for complex pattern discovery
* Integration and development into dashboards to enhance data visualization capabilities with support to interactive components to support visualization
*  Develop real time anomaly identification with automated triggers. These actions include alert generation based on defined anomaly rules 
*  Expand data format processing through integration with JSONL to allow more data format integration for additional processing options

## FAQ (Frequently Asked Questions)

I'm having problems getting started.
First confirm the configuration YAML has properly configured and set values for input stream. Verify all required library packages are loaded and properly set during installation steps

Is zero mq mandatory to using Py DataStream ?
No, zero MQ usage not required but recommended and provides high speed, high reliability stream connections that are optimized when processing large amount of high data volume 

Why do errors appear regarding `ZeroMQ `
This usually results form an issue during installation of the ` ZeroMQ ` libraries and requires manual configuration, check documentation of ZeroMQ or contact your administrator
    
Where to get updates
Check github repo for most recent version, updates are released when issues are closed and improvements/changes suggested. Also stay updated via release note and announcements from maintainers  

## Citation

When citing this library in academic publications or research reports, please use the following BibTeX entry:

```bibtex
@misc{py-datasteam,
  author = {Author names and affiliation, If appropriate},
  title = {PyDataStreamProcessor: A Library for Scalable Data Streaming and Analytics},
  year = {2024},
  note = {GitHub repository: [https://github.com/your_repo_url](https://github.com/your_repo_url)},
}
```

Always include a proper explanation to highlight and acknowledge that our library helped to achieve the project goals.
   
## Contact

For bug reports, feature requests, and general inquiries, please open an issue on the GitHub repository [https://github.com/your_repo_url](https://github.com/your_repo_url).
Feel free to reach us on Discord, at https://discord.com/invite/xxxxxxxxxx.   For questions, contact data_devops_email@companydomain.net