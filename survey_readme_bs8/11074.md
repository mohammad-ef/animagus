# Task Management Engine (TME)

This repository contains code, instructions, and guidelines for developing and deploying an extensible task management framework called TME. Designed as a modular platform, users can customize workflow, integrations, reporting, and security based in their specific requirements and infrastructure. TME is geared toward both solo operators and larger development teams managing many processes or micro tasks. This allows for flexible configuration and adaptation to many unique operational contexts.

TME provides an abstraction of basic work processes that can be adapted for any use- case from data pipeline execution to managing development tasks to controlling physical machinery. At its core, an engine monitors task definitions, schedules their execution based on configurable rules, monitors execution, and reports on progress and errors. This is achieved by utilizing a robust queuing architecture which can be configured with multiple backend solutions (RabbitMQ, Redis, etc).  

The architecture is designed around pluggable worker components, enabling integration with a broad range of tools or external APIs, allowing seamless data exchange. Furthermore, a built-in dashboard enables real- time tracking of tasks' progress, facilitating efficient monitoring and prompt error correction within workflows.  

We use a REST API for interaction, supporting automated deployment via CI/CD pipelines for scalability. The engine supports multiple languages and can be used across many cloud infrastructures ( AWS, Azure, or GCP). We also include a rich set of CLI tools and client SDKs which are designed simplify task submission and monitoring for a broader audience. 

TME can be integrated into many environments. The modular architecture promotes rapid deployment and scaling, ensuring the ability to manage ever-increasing workload requirements while remaining secure and performant. It provides powerful reporting capabilities that provide a complete view across your tasks. The extensible and well documented API is ideal for creating automation tools for your workflow or for integration with existing tools that you have already adopted.


## Installation Instructions

First, please ensure you have the necessary dependencies installed on your system before proceeding. A minimum version of Python3.9 is needed, as the project utilizes features exclusive to it. Also, ensure pip is available and up-to- date; it is used for package management during the process.

We also need a task queue. While multiple backends exist and are configurable, RabbitMQ is the suggested solution. Download and install a RabbitMQ broker on your server or local machine. This will host messages as the engine processes tasks in parallel. Instructions vary for each platform. For a Linux environment this can be installed using apt with sudo apt install - y rabbitmq-server. You' will then need to allow access from your system by opening the relevant ports using ufw firewall rules.

Next, download source code. You can clone the repository using your git CLI: git clone https://github.com/organization/task-management . You may also wish to download source archive directly via clicking the button and extracting contents into a directory.

Once your code is retrieved, it is time to set up the Python environment. Create a virtual environment for isolation using python - m venv v env.  This keeps project dependencies separate from your system Python installation, improving consistency and avoiding conflicts.

Navigate to your extracted codebase directory via `cd task- management`. Now you can initialize the necessary environment variables using the following command: export TME ENVIRONMENT=local. The variable specifies how T ME behaves during deployment and should be set to production or other appropriate settings depending on deployment environment. 

Install requirements: Run `pip install -r requirements.txt `. This installs dependencies specified in the provided text file, including libraries such as Django, Cel ery, PyRabbit and more. This process might take some considerable time, as the dependencies can be complex and numerous to install.

After installation of libraries completes, configure your broker settings. Update task_management/ configs/ default. yaml, replacing the default connection string with your RabbitMQ brokerâ€™s connection details. This includes hostname, port and other relevant configurations depending on your Rabbit environment.

Finally, apply initial migrations with  python manage.py  migrate. This ensures database tables necessary and schema are properly initialized to support the core framework's functionality. This is essential before you deploy to production or start development work.  

The application will now be ready for usage. Please make sure you've configured and started RabbitMQ as the engine relies on it for task dispatching and queueing operations to perform efficiently.  

## Usage Instructions

To launch the application, navigate back to the `task management` project's top-level directories. From here, start the Django development server by entering python manage.py runserver. This launches a development server and provides you access to the TME API endpoint. Access your application at localhost:8000. This is for demonstration or small use cases and is not suitable for use in production.

Now submit test tasks using HTTP requests to the `/ tasks/ create`  endpoint using a REST API call. A POST request including a JSON body defining task properties like `name`, `description ` and other configurable fields is expected.

Check the tasks status using HTTP requests to API end point: `/ tasks/` . You should see your new tasks listed, reflecting their current statuses as the engine starts to queue and execute them, based on its internal scheduling rules.  

For more complex operations, consider using CLI tool that has been built for the management of tasks from your own terminal. Invoke the CLI tools by ` python manage.py task < command >`. Available tasks commands are list all tasks, create task, update status of tasks. 

You can also interact programmatically using the provided client SDK. Import the libraries in Python to interact with TME using the libraries and functions. Refer to the client SDK examples in the documentation to learn how to build applications with Python to interact with engine.

The built in dashboards will allow you to view and monitor tasks. The UI is accessible at `http://localhost:8000` and it will provide you an overview of all your configured jobs and associated statuses in one centralized view which simplifies troubleshooting and process monitoring across your tasks.

You can create complex task workflows by chaining tasks together using dependencies in the definition. This means one task will not start before a predecessor is completed or a predecessor fails, which can be configured in the task properties. This enables building complex automated chains which execute in response to an external event or other triggers within your system.

To delete any created task use ` python manage.py task delete < task_id >` from terminal. You will first need to get the ` < task_id >` from the tasks listing API to delete a specific task.

## Configuration

T M E's configuration is driven through a YAML format file called default. yaml, which is loaded automatically on application start. Located in directory `task_management/ configs/ ` by default. You can also override default settings using environment variables. This gives you an additional layer of flexibility.

The broker settings in your configuration file define TME's connection to the underlying tasks queue. Update the hostname, username, password, and port settings to accurately match details of your configured broker environment.

To adjust the engine's worker concurrency (parallelism) settings, find the ` WORKER_COUNT ` option in default. yaml and configure accordingly.  Increasing this value can accelerate performance, but should only be considered for high- throughput deployments, with adequate hardware.  

Adjust task timeouts to reflect typical task durations. Set these in configuration, ensuring that long- running tasks are not terminated prematurely while also preventing resources from being locked for unreasonably long times during unexpected situations. 

T M E supports various authentication and authorization mechanisms for API access and dashboard access. The configuration file specifies how user authentication works in T M E which you will need to adjust based on your needs and existing infrastructure.

Enable or deactivate features like logging, error tracking, and email notifications via the configuration file, customizing T M E based on operational monitoring and alerting needs. You can change the default level of log reporting, enabling or disabling verbose logs for debugging or auditing.

T M E's default YAML configuration can be easily overwritten. For this, create `override. yaml` in the same configs directory and add your settings there which override those in default. yaml.

## Project Structure

The project repository is logically structured for modularity and maintainability. The `task_management` directory houses core application logic. The `configs` subdirectory contains all configurations and settings.

The `src` directory contains the source code for the application, including API endpoints, task models, worker components and related code logic. You'll find your Django application within src/ task_managment.

The `templates` folder holds the HTML code for user-facing dashboards, API documentation, etc. T M E utilizes Django's rendering framework for creating the dashboard. This directory contains HTML templates.

The `tests` directory contains unit, integration, and other test suite for various components. Tests provide assurance for code quality and ensure new functionality behaves as expected.

The `scripts` directory contains auxiliary scripts, which might be used for deployments, data migrations, and other administrative tasks to manage application setup and operations effectively.

The `data `directory is used for storing static data used by the application, like seed configurations or pre- populated data for demonstration or tests. This directory will contain static data and seed configurations for initial deployment. This directory is often used for providing pre- populated data for demonstration or test purposes.

The `docs` directory is the place for all documentation relating to this project.  This can be API references, design documents, tutorials, and other information for developers and users. This directory can host comprehensive API references and guides to support development. The root directory contains the `README.md` file, and the `requirements .txt` which specifies project dependencies.

## Contributing

We welcome contributions. Please review the below before submitting pull requests.

First, review our coding style guide. Adherence to coding standards ensures a maintainable code base, facilitating collaborative development, as consistency improves code understandability. 

Submit bug reports and feature requests via the issue tracker. Clear and descriptive issue reporting helps in efficient resolution.

Fork the repository and create feature branches before submitting pull requests. Well- isolated branches simplify code reviews and prevent merge conflicts when integrating changes.

Before submitting a pull request, ensure your code has been fully tested. Thorough tests provide assurance for code quality and ensure new functionality behaves as expected. 

We adhere to a strict test driven development (TDD) model. Please follow this when developing new code for this project.

## License

T M E is licensed under the Apache License 2.0. This permissive license gives you freedom to use, modify, and redistribute the code, even for commercial purposes.

The license does impose a requirement to retain the copyright notices and license terms in any derivative works. This ensures that the project's provenance is properly attributed and users are aware of their rights and obligations.

## Acknowledgments

We acknowledge and appreciate the following projects and communities for their contributions to the development of T M E:

Django, the robust web framework providing a powerful foundation on which T M E is built and enables rapid web development.

Cel ery, a distributed task queue for scheduling and running long running processes in T M E enabling the execution of task workflows concurrently and improving overall throughput.

RabbitMQ is the message broker used to enable inter-process communications within tasks. T M E relies entirely on the robust Rabbit framework.

## System Architecture

TME has a microservice- like layered design for scalability. At the lowest layer is task queue ( typically a RabbitMQ cluster). Incoming tasks get queued in it and worker process retrieves it when the task can run.

At the next layer resides task workers responsible for actual tasks. These tasks run asynchronously based on configurable schedule, and the task worker communicates progress/error messages to the engine and data store (Django Models in our cases). 

Next up are REST APIs and dashboards that provides external user interaction point ( API access, task view/modification)

Database storage, managed via Django, serves for configuration settings persistence, task statuses storage and interim reporting and historical analytics. Data storage allows persistence in task state. The application architecture promotes horizontal scalability with the task worker process deployed across various servers, improving task processing capabilities as the volume goes up.  

Data exchange flows asynchronously. This is critical and provides loose- coupled interactions for scalability across distributed environments which improves system resilience during operational failures. Finally, configuration data gets consumed at initialization time which makes for flexible deployments without requiring re compilation, reducing maintenance and improving deployment speed.

## API Reference

The core interaction mechanism to TME are a set of RESTful APIs which expose functionalities to manage the system effectively and enable easy interactions via automation frameworks and client tools. All endpoints support JSON request and response bodies by default, ensuring data interchange across varied programming languages.

For retrieving available jobs: Use endpoint GET /tasks/ - to fetch a complete task status, including job ID, status details, timestamps of execution and assigned user if relevant to track overall process health across tasks in a single call. You should get list JSON object.

Create task requires POST requests sent to  /tasks/ create. JSON objects should describe the properties (name, schedule type, cmd). You receive response JSON that indicates job created and assigned JobId to be managed later. 

Update task states, you need an update call to the  `task/ <jobid>/. ` You will update statuses with the PUT or PATCH verbs with relevant job states (started, complete or fail, with any log outputs and details if any. 

Delete jobs, which you send the DELETE commands at task  `<jobid>` and get the HTTP status codes (success code is `204`). 

All responses should adhere with standard best practice, containing relevant status information as JSON for ease to automate integration and parsing with tools for various scripting frameworks for ease to consume information programmatically. Detailed specifications including parameter sets can be retrieved in ` docs` directories, with comprehensive explanation on request formats as JSON as example code snippets.

## Testing

The code repository uses a testing approach for all features that have an extensive suite that tests unit components as well individual API behavior. Tests use ` pytest` test runner to systematically execute various assertions against code logic. You must have installed ` pytest ` on the development server prior attempting any execution. 

You run all testing suites from project directory via a CLI execution, just execute `pytest`, or you run specific package tests, using a syntax similar to the command pytest ` / path/ tests.

The integration and system integration suite requires that all dependent components ( database connection , Rabbitmq and any API) run correctly. Before attempting a full- suite, ensure you can run all dependent resources correctly to get reliable testing environment results and prevent test suite failures from dependencies failing. The testing framework includes mocks of API integrations so enable to isolate unit tests, and provide a set standard responses when mocking out integration. Mock objects provide controlled responses when running a set tests to avoid integration with actual external components for isolation in local execution environments.

## Troubleshooting

Ensure your Python environment is consistent for all contributors; it prevents conflicts that might lead to unexpected behavior on different system installations or Python interpreter versions. If errors are thrown during application run or task creation or update, it may be indicative of database errors (such as incorrect configurations or connection failure, and verify connectivity, credentials). Ensure the Rabbitmq instance is operational by testing connections and message flow from outside T M E environment using a different messaging library like python client or any available CLI tool from a messaging queue toolset. Incorrect Rabbit configurations are common issues in distributed deployments and proper settings should guarantee that your engine functions without connection interruptions to queue messaging systems and ensure messages and jobs get correctly transmitted to queue workers to execute and provide timely and efficient performance results.. Finally review the application error logs that are saved at /var/log for debugging errors to diagnose application issues or unexpected runtime behavior which may lead troubleshooting towards identifying code flaws during production deployment and operations to ensure smooth system behavior for all end- user.

## Performance and Optimization

The framework leverages Cel ery workers. Increase this setting by increasing  ` WORKER_COUNT ` setting to enable parallelism of jobs running across various servers. Caching commonly used results or query sets can reduce latency and database load when the worker performs the job and can be further increased. Use appropriate message serialization and compression protocols, reducing the data sizes to accelerate messaging traffic, and reduce memory load when exchanging data over networks and queue connections.. Monitor resource usages across all servers. Use tools which provide insights for optimization. Profile code to understand which routines use the maximum computational resource consumption during task processes and refactor for more efficient algorithms if applicable. Consider horizontal and vertical scaling, adding hardware for higher task volumes or optimizing existing computational power. Finally, optimize query queries in the Django Models and apply index where relevant in DB schemas when needed..

## Security Considerations

Protecting secrets such as API keys, passwords and credentials is vital; always manage this via encrypted secrets. Use strong authentication for access control and ensure permission models align correctly based roles to limit exposure when managing operations on resources to authorized roles in organization. Validate inputs for all tasks submitted for preventing SQL injection. Regularly monitor systems to prevent potential security breaches, by analyzing access activity for abnormal patterns to catch potential exploits quickly.. Patch the application and workers immediately as updates released for vulnerabilities to prevent attacks, keeping code always current in security best practice.. Employ firewalls to filter access to network, controlling connections between external networks to TME servers and internal communication channels which prevents intrusion of external entities accessing your critical application and services . 

## Roadmap

-  Enhance UI: Add more sophisticated visualizations for task progress in a user-friendly and informative manner
- Integration of external systems. Expand the existing list with integration for common data stores for increased operational versatility. 
- Implement role-based Access control to provide a granular level management control across users. Fine granularity control can enable security compliance needs to manage access levels in different environments .
- Implement improved task dependencies with error recovery for complex workflow scenarios with better exception recovery for increased resiliency of complex chains . 
- Introduce real- time streaming and notifications of events, to provide an eventing based infrastructure support .
- Build improved dashboards. Create an analytics view, allowing tracking trends for performance.  
## FAQ (Frequently Asked Questions)

*  Why can't the worker connect to the rabbitMQ ? -  Please make sure, the host in default yaml, Rabbit mq running. Check firewall settings to confirm connections on ports allowed between servers.. 
* I have dependency issue while installation ? Ensure Python versions correct with compatible version with project dependency and upgrade pip to resolve dependency version issues with packages and dependencies in environment during deployment process for reliable installations of all modules needed for running application . 
* Why task never executes? Please, inspect queue size to see that there aren not a long delays due message queue and ensure that tasks configured properly and scheduler works correctly .   

## Citation

If this work inspires or informs academic papers and publications we request you include a reference for TME:

` @misc{TaskManagementEngine,
 author = {Organization name},
 title = {Task Management Engine},
 year = {2024},
 url = {https://github.com/organization/task-management}
 }` 

Proper citation helps promote project recognition in scholarly environments.

## Contact

You can contact the project's development team through our project mailing list: support@taskmanagment.org or via community support Slack at link slack-group

Report security-related issue, report it via support channels. Provide a concise patch when possible to improve our overall code.