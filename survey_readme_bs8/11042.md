# Automated Log Processor (ALPEX - ALP v2)

This is ALP v2, also referred to as Automated Log Processor, built primarily with the aim of streamlining system and application logs. It automatically parses, filters, and aggregates log data, allowing engineers to rapidly troubleshoot production and development issues. This second version offers a significantly expanded feature set from the first.

Our focus is on reducing MTTR (Mean Resolution Time), especially in distributed applications. ALPEX achieves this by offering real- time insights through customizable rule sets and integrations with various notification providers. It supports multiple file types and provides a web UI for visual exploration and filtering.

The application leverages a message queue (RabbitMQ) to ensure high availability and resilience across distributed deployments. Its architecture is designed to handle large volume log data while offering low latencies.  Furthermore it is easily integrated with cloud logging services like AWS CloudWatch and Azure Event Hubs

ALPEX is designed to be deployed using Docker and Kubernetes. It is written in Python for its rapid development and broad third-party library ecosystems. The architecture prioritizes separation of concerns, allowing for modularity and future extensibility through a plugin based mechanism.

We aim for a project that is not only powerful but also easy to use and maintain.  This README is designed to guide developers and operations personnel through setup, usage, customization, and contributions to the project's evolution.

## Installation Instructions

Prerequisites include the following software installed on your development machine before proceeding with further steps. You should ensure your system is up to standard for this project.

You need a compatible system with Python 3.8 or later. It is highly recommended to use a Python virtual environment. Install pip if you donâ€™t have it by using apt-get on Debian or equivalent for other operating systems.

Make sure Docker and Docker Composer are installed and functioning. Download the latest version for your host OS via Docker official website.  Verify the installation by executing a test `docker --version` and ` docker-compose --version` in your console.

You will need RabbitMQ installed and running. This can be installed using `apt-get` on Linux or by downloading a Docker image. Make sure the user has the necessary access rights.

Clone the ALPEX repository locally. Run `git clone https://github.com/your-org/alpex`. Change directory into the newly cloned `alpex` directory to proceed.

Install the project's dependencies using pip. Navigate to the project's root directory and invoke the following command: ` pip install -r requirements.txt`. This step is essential for installing required libraries.

Configure RabbitMQ with the appropriate username, password and connection. These details will be needed in later configurations as they provide the credentials to communicate.

For Windows users, ensure that Docker Desktop is running and integrated with your preferred terminal. WSL2 (Windows Subsystem for Linux 2) is highly recommended for a smoother development experience.

On Linux machines, you might need to adjust firewall settings to allow communication on RabbitMQ's port (usually 5672). Use the command `sudo ufw allow 5672` or relevant tools for your environment.

For macOS users, ensure that Docker Desktop is running, and you have sufficient resources allocated to the virtual machine. Consider using a tool like Brew to manage dependencies.

Finally, run the database migration commands to initialize the database. Execute ` python manage.py migrate` after setting up the database configuration.

## Usage Instructions

To start ALPEX, ensure that both RabbitMQ and the database server are running.  Start the worker processes with `python manage.py run_worker`. The main application can be run with `python manage.py run_app`.

You can feed log files to the ALPEX system by placing them in the designated `/logs` directory, or by configuring it to monitor specific folders.  The system will automatically process new log files as they appear.

To test the system, create a sample log file with the following contents. `2023-10-27 10:00:00 ERROR: Something went wrong`. Verify that the logs are successfully ingested and displayed in the web interface.

Access the web interface by navigating to `http://localhost:8000`. You will see a dashboard with aggregated log statistics and filtering capabilities.

To configure rules, navigate to the "Rules" section in the web interface. Create a new rule that filters for "ERROR" logs and sends notifications to your configured providers like Slack or Email.

You can utilize the CLI for tasks such as adding/removing rule and managing user access, the available parameters can be referred via command. Example of running: `python manage.py add-rule "critical error" --level error --notification slack`.

The API allows developers integrate into the pipeline for automated actions based on parsed and aggregated logs, this allows a seamless interaction with other services in complex architecture.  Example request with python, use HTTP and pass in your desired parameters and payloads.

Advanced users can define custom parsing formats. The default configuration supports JSON, text, and logstash logs and many formats can be integrated through the custom plugin.
For performance optimizations, ensure your rabbitemq is set appropriately as it handles a lot of traffic internally.

## Configuration

ALPEX is highly configurable via environment variables and a `config.yaml` file located in the project base. Ensure both configurations align properly before starting.

The `config.yaml` file defines application-level settings, such as logging level, API keys for external services and the base path for log input directory. You may edit them accordingly to adapt this to production settings.

RabbitMQ connection details like username, password, and host can be set through environment variables `RABBITMQ_USERNAME`, `RABBITMQ_PASSWORD` and `RABBITMQ_HOST`. Incorrectly setting any will affect communication and performance of logs aggregation.

For security reasons, sensitive credentials like API keys for notification providers are ideally passed through environment variables rather than directly embedded in `config.yaml`.

To adjust thread pools of workers for optimal processing speed set parameters `NUM_WORKER_THREADS` accordingly with CPU and processing demand, it will allow efficient parallel execution for parsing log.

You can override these settings with environment variables, environment variables always takes precedent. For instance `DEBUG=True` sets the environment `DEBUG`. The configuration should allow customization and be modular for different deployments.

You may use a docker-compose for easier deployment of both components together allowing easy management with single script. Create and update a new docker config with required services.

Logging verbosity can be adjusted based on the value passed into configuration file, allowing better tracing of activities on a running server instance, important when troubleshooting problems.

Consider implementing secret management solutions like HashiCorp Vault for securely storing sensitive data within environment variables during deployment phases, ensuring enhanced application stability.

You might need to define additional rules to adjust the filtering of incoming messages according to specific needs and requirements from each business context that can vary a lot from case to case.

The default notification channels are Slack and email which allows the engineers a broad and efficient way to get alerted and to act promptly based on critical errors that may be observed through incoming traffic

## Project Structure

The repository's top-level contains core scripts that handle overall control flows such as reading files, processing logs with the defined parser configurations, pushing events through messaging services, webserver for visualization

`src/` directory hosts all the Python application logic. `core/` includes main business functions, like processing of raw logs; `parsing/` focuses on log formats, including text files or complex structures in json/XML; and `api/` exposes endpoints using REST architecture to enable interactions between different systems in an enterprise setting.

`tests/` houses various test components for all the code. This contains `unit_tests/` testing core components such as functions in isolation;  `integration_tests/` tests different integrations, especially with other third party providers like messaging services such as AWS SQS, GCloud, MSFT queueing service,

`configs/` contains YAML-based configurations to handle external providers settings; it holds RabbitMQ parameters or cloud services configurations and API keys required during operational stages, to ensure seamless communication between the internal application flow

`docker/` provides configurations required for docker deployments. The `Dockerfile` specifies instructions that builds the application in container. Compose files manages container relationships.

`logs/` acts as designated storage space, receiving logs as files or messages, to be later ingested within ALP and monitored in terms of performance and availability; the folder is usually empty upon project initializations but grows as messages start flowing inside

The documentation and examples for configuration parameters, installation guidelines as well the overall code style standards exist within `/documentation`. They help new users get started faster on using it with a better user understanding as the product matures

Finally `requirements.txt` holds dependency packages that must be downloaded when installing ALP locally using package managers such as Pip to run the application properly.



## Contributing

We welcome contributions to ALPEX!  To ensure a smooth and productive development experience, please adhere to our contribution guidelines.

Report any bugs or feature requests as issues on the GitHub repository. Be detailed and specific.

When submitting a pull request, ensure your code adheres to PEP 8 style guidelines. You can install `flake8` to perform checks during development to catch common stylistic deviations from standard practices.

Write thorough unit tests for all new code or modifications. Tests will improve robustness in the product

Please use a meaningful branch name related to feature/fixing. Describe clearly in a PR why your pull request changes, with all context necessary from development. 

If submitting changes that significantly modify behavior, please include comprehensive documentation changes along with. It will help new team mates onboard the feature.

Please use standard versioning semantics for PR's. It will facilitate proper releases in later cycles as it allows proper version bumps as per the nature of feature or change involved.



## License

ALPEX is licensed under the **MIT License**. By using, contributing to, or distributing this software, you agree to be bound by the terms of this license. This ensures freedom and usage.

The MIT License grants you broad usage permissions - use this project, change, distribute, sell it or create derived projects for personal commercial or academic uses; this freedom ensures broad community support, with a permissive and straightforward legal approach that minimizes legal complexities.

This also states your responsibility to preserve original licensing notices if used in derivative code - which protects original creator of the codebase to receive credits for their contributions while maintaining a flexible usage and sharing approach for derived works in open-source projects



## Acknowledgments

We extend our gratitude to the entire open-source community for providing essential tools and inspiration. Specifically, we thank the RabbitMQ team for their highly performant messaging platform and to docker, and their associated containerization infrastructure.

Python and Django are also critical for providing us flexibility. We thank those contributors. Furthermore we would thank all individuals from this team for providing support to make the open project happen in timely fashion, it is the combined effect and support which drives a positive momentum of growth



## System Architecture

ALPEX has a modular design with distinct components communicating via a message queue, allowing asynchronous operation and scalability for high data rates as the number of messages coming in from multiple locations. It consists primarily of a worker node, parser components and an admin console for users and management tasks.

Incoming logs stream from the configured locations (files or network inputs). The Ingestion Agent captures this information then pushes to a dedicated Queue managed through the broker: `rabbitmq`. It handles log data, and ensure proper format and delivery

RabbitMQ then acts as an inter mediator which receives and queues these messages for subsequent parsing and enrichment by workers and processes. It also serves a key purpose - ensuring reliability through persistent and reordering-guaranteed queue management capabilities for message flows, regardless system availability, to guarantee all message delivered correctly in the sequence as ingested initially.

Workers subscribe and consume these logs via a queue, applying parsing based configured patterns for extraction from each source type such as `syslog` , or structured text format and then enriching contexts for better visualization, for reporting and further processing

Lastly admin interface exposes UI components allowing users configure rule sets and policies which determine routing actions for processed information as well provide them dashboard with real-time views for performance and status insights on system components across deployments in complex systems environments .

The architecture supports both centralized and distributed setups and uses the plugin architecture approach so new log parsing methods/integration/providers/notifications can easily integrate to increase flexibility

Data storage & caching layers provide persistent state and reduce response times, with database used as storage layer to provide historical logs. Caching helps speed-up performance. It provides an additional level to prevent unnecessary processing overheads from incoming logs.



## API Reference

The ALPEX API provides an REST interface that enables programatic log filtering, alerting configuration, users/group administration as part a fully programmable workflow that enables a lot flexibility and automations on a broader basis .
It includes endpoint like, `GET /logs` retrieve recent processed and indexed events;

 `POST /rules`:  creates an instance of rule configuration and enables the filter criteria for log parsing,

`POST/notification` creates notification instance, and provides options that allow configuring different delivery channel for notifications (ex, emails, SLACK ) . API supports json input payloads with authorization using API tokens

Error handling includes status numbers for successful or failure response and json responses to display messages to the users

Example endpoint request to add new logging criteria is POST to /api/logs/create?filters = error, severity > critical, message like warning . Response: Status `201`. `json response {"message: Logging configured"}`



## Testing

Testing plays an important role in quality ensuring for our production deployments of log data pipeline for ensuring accuracy & stability across different environments with a modular structure of code through comprehensive testing strategy and processes to minimize risk of defects & improve user satisfactions overall through consistent results in production and non-production settings .

The ALPEX uses a test driven-develop (TDD) model. New code changes requires associated test suites that must have sufficient and appropriate test-code cases for ensuring high-fidelity code coverage.

Unit Tests test functionality with mock environments; `Integration-Test tests the integration and interoperability among modules; E2E Tests verify complete user scenarios in close environment mirroring live production deployments to capture realistic interaction and system response characteristics.` 

Run Unit/Integrate using Python with pytest `pytest --collect-only tests`, for docker environment the testing framework is integrated inside docker container alongside main code base and requires running the following shell commands:   './docker/test run '.   

The project contains comprehensive coverage and continuous intergraded pipeline for automated quality assurance processes during development phases .

## Troubleshooting

Common problem include connectivity failure, misconfigured configurations with rabbit or the databases that are not correctly running.

Ensure the RabbitMQ instance running properly; check its console output or access management interfaces via the web for status details, or verify it using command like :'netstat' on port `5672 `. Check that the firewall doesn`t drop incoming and outgoing communications with proper configurations .

Incorrect permissions to access data directories or file configurations are a usual cause of operational failure, ensure your current execution is with a role having necessary read- write permissions and that user-groups has correct privileges and permissions in production. 

If errors related parsing formats happen during log ingestion. Examine the configurations to confirm that parsing methods are aligned to the actual source file formats and that proper patterns have be configured correctly -

The system will log extensive logs on console which are essential during problem identification and diagnostics . Check those and analyze error codes or stack tracing to pinpoint causes of error, for advanced cases review configuration parameters and check the version of the python dependencies and ensure compatibility

## Performance and Optimization

Several techniques help ALPEX optimize throughput while reducing latency when parsing huge amount log messages with varying degrees from source and formats, achieving higher-scale and more efficient log aggregation. Message Queue Tuning and Scaling is very useful in high volumes . Increasing the number queues workers and consumers helps handle incoming logs faster and more robust way .  Optimize parser algorithms for speedier and effective information and metadata. Employ techniques, and avoid unnescesary regex operations or data structures in the parsing routines. Additionally using cache layer can help minimize data processing.
Caching often parsed data to provide quick response and reduces overall processing burden for common queries

Caching frequently accesses patterns and parsed data, reduces overhead for common tasks like filtering, searching, reducing load, enhancing the throughput in high-volume systems

For very intensive use scenarios it could require scaling horizontally with multi machines to accommodate load increases .  Consider distributing workload between several APP servers for enhanced scaling capacity as needed and further distributing the data across distributed data nodes, to handle increased data and improve fault robustness while keeping system responsiveness high

Benchmark using ` Locust`,  provides detailed metrics of the API responses, throughput under various workloads and resource usages. Use tools that will assist the selection appropriate configuration to achieve maximum operational and system stability with minimized latency



## Security Considerations

Data confidentiality and privacy are of the essence when dealing log processing in ALP; securing the systems and configurations and following appropriate data encryption best-practices helps safeguard systems data against potential unauthorized accesses/attacks that may occur when handling production environments or cloud deployments.. Implement strict access management using API Tokens and Roles for access to the admin interface with the specific roles based access for limiting users privileges based job functions

Protect secrets through using secure management like hashicorp or Azure keys vault . Avoid exposing sensitive secrets inside environment variables. Use proper encryption for data at-rest with encryption algorithms like AES for protecting against potential attacks . Regularly check security advisories related third parties to ensure all patches are properly addressed.



## Roadmap

Current priorities for ALPEX includes seamless and automatic scaling and enhanced integrations and improved data enrichment to increase insights on production and improve performance across various data sets

New feature planned, enhanced dashboard visualization that enables better insights and data representation; support new log input/output source integrations.

Support plugins to provide flexible integrations.  Incorporation machine learning based log pattern-detects is under evaluation . 

Versioning follows `Major.Minor.Patch ` approach to facilitate upgrades and compatibility with other tools; Release schedule are bi-weekly, and we aim for quarterly features release based development progress with a high focus to user requests captured.

## FAQ (Frequently Asked Questions)

The project not ingesting all files even they were uploaded into logs. The folder may requires correct write- permissions, or ensure there not enough storage for file to write on destination. It may involve increasing capacity of underlying data volumes.

How can it integrated on different clouds (GCP and AWS?). The cloud configurations should exist inside of `/configuration directory, where the configuration parameters may need adjusting and adapting to match with the provider- specific requirements of your account

Can this system parse compressed formats like gz. The code can handle gzipped archives as well if you specify the format to parse as compressed and add required library

If my rabbit MQ stops, do incoming logs are skip/loosed ? It's an important characteristic as we aim data consistency, so it does retry after RabbitMq restarts.  The configuration allows retry and fail/over options

Where logs persist ? Database provides primary persistent data with a retention strategy in-placed to maintain efficient disk usage with only critical/high impact records



## Citation

Proper citation when utilizing our ALP for publication helps give back and ensure it's credited properly . We acknowledge this work as it supports and helps advance log parsing. 

Here's BibTeX reference for your academic citations to reference it properly within research or scientific related works and maintain consistent standards for references: 
```bibtex
@misc{ALPEX,
  author = {YourOrganization},
  title = {Automated Log Processor (ALPEX)},
  year = {2024},
  howpublished = {\url{https://github.com/your-org/alpex}},
  note = {Version 2.0}
}
```
Ensure accurate version of ALP version in citations; and always provide links for access and discoveries of the product



## Contact

Contact information and assistance channels will facilitate a productive community support environment where developers users are welcome and encouraged engage and contribute back

Contact the main maintainers by sending the email at email.support@companydomain for general questions, reporting bug issues, feedback or enhancement suggestions to ensure prompt support responses.

For technical help engage within the project GitHub community forums where you find from experience developers to provide insights or assist debugging complex situations that occur in operational deployment

Join open community discussion group on [slack.example.org](https://slack.example.org/) and share knowledge and assist to other community members and to help contribute for a shared growth with collective expertise from multiple participants