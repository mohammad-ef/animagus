# PyDataStream Processor - Real-time Stream Data Analytics

A powerful Python library to stream data, process in real time, and apply analytics. This project provides an infrastructure for efficient data handling, processing and analysis from real- time input sources.

## 1. Description

PyDataStream Processor enables developers to build high performance data pipelines for processing real- time data. It is designed for analyzing streaming data with low latencies, supporting diverse operations like filtering aggregation, transformation, and window functions. PyDS is modular, extensible allowing custom operators and data sources to seamlessly extend. Our aim is to offer a robust platform for real- world analytics and applications.

Data streaming is increasingly crucial for many applications from financial analytics to the IoT. Current methods frequently need specialized tools, which can be intricate.  PyDS bridges this gap.  By leveraging a Python ecosystem, we make real time data processing approachable for a wider audience familiar within the programming language itself.

The architecture is built upon a reactive programming model where data drives operations. Each stream source produces a stream of values, then the processing elements apply the transformations and analytics functions. The results are then published to various destinations, such as databases, cloud storage, dashboards or alerting mechanisms. This modular design facilitates rapid pipeline creation by assembling different processing components.

PyDS supports a wide array of features. Windowed aggregations allow calculating stats within a specific time range, like hourly averages or daily totals.  Customizable filter logic allows the selective routing of data for different operations. Transformations can manipulate the data as required for analysis by applying various Python functions. 

PyDS is extensible using custom operators. This allows users to define specialized data processes to cater for specific use cases, ensuring the platform's applicability across diverse fields. We also intend to add more integrations with various external data providers and destination systems for a more comprehensive ecosystem to handle a greater number of use cases and workflows.

## 2. Installs

To start using PyDataStream Processor, you need:
*   Python 3.7+ is required for execution compatibility reasons
*   Pip package manager
*   A basic command-line/ terminal environment setup

You should first ensure pip has been updated to its latest version using command:

```bash
    pip install --upgrade pip
```

Then the installation can be done easily from your terminal using:

  ```python
    python -m pip install PyDataStream
  ```

On Windows, ensure you are running the terminal with appropriate execution permissions, as installing Python packages sometimes requires elevation depending on the directory where it' s located. 

If you need to install PyDataStream Processor in a particular virtual environments (recommended), you must create a new virtualenv and then activate it prior to executing pip commands within it. The command sequence to do so is:

```python
   python -m venv myenv # Creates the environment
   # On Linux: source my env/ bin/activate
   #. On Windows: .\ myenv \ Scripts  activate
```

If you face errors during installation due to incompatible system libraries, you should try updating your system's packages first to resolve possible dependencies issues that the package might require. You also might consider to install the packages in a clean environment. 

If the installation continues fails, it is likely you may need to install the underlying library dependencies separately, or consider consulting our official documentation. You will also need to update pip regularly for security. 

To install a specific PyDataStream version you can specify it when running installation:

``` python 
    # Install version 1 .2 .3 using the -r  option
   python -m pip in PyDataStream ==1 .2 .3 
```

This helps maintain consistency across different development and production environments. It is always suggested to pin specific package versions to ensure consistent results, and reduce instability from unexpected updates.

Finally, after the installation, it is a great practice to ensure all dependencies are properly loaded within your Python environment:

```python
    import PyDataStream

    # The absence of an ImportError means the package installed successfully. 
```

## 3. Usage Instruction

PyDataStream Processor is designed for ease of configuration and execution of streaming processes. To illustrate, consider this common pattern:

  ```python
import PyDataStream

# Create a source for your data, like from a Kafka queue or a simple file
my_source = Py DataStream.Source(type="file", file_path="data.txt")

# Create a processor for filtering data
my_filter = PyDataStream .Filter("value > 10")# filter all values above 10

# Create a processor for averaging
my aggregator = PyDataStream.Average()# average the values from each batch

# Create a destination to output to the processed data like a file or database
my_sink destination =Py DataStream.Destination(type="output", file_path ="output.txt")

# Assemble a processing pipeline
processor =    PyDataStream.Processor ([my_source,  my_filter, my_aggregator, my_sinkdestination])

# Run the pipeline to process and analyze the data stream in real- time
processor.run()
  ```

Consider this basic data ingestion pipeline where data is continuously fetched from a file and filtered and subsequently averaged to create an aggregation pipeline which outputs the data: The pipeline is then initiated to start processing and outputting data. 

For more advanced scenarios consider defining custom operators using the PyDS class. The custom class can inherit properties, and override specific behaviors to fit your application requirements.

The PyDataStream Processor can accept different types of sources: Kafka messages from a streaming server or a simple local file. You will need to define appropriate source parameters depending on the type selected, e .g providing Kafka broker addresses and topic names.

When filtering is needed to select relevant data, the filters can accept simple expressions using Python syntax. More complicated filter logic requires defining custom operators and providing additional configurations for these specialized scenarios which will be explained in a separate tutorial. 

To monitor your real- time analytics pipeline in terms of throughput, processing delay, and potential bottlenecks, you have a dashboard interface within PyDataStream which can be initiated. This allows to view the pipeline state as it is being run, and provides real- time diagnostics.  

You can configure the destination to store or output the processed analytics to various storage locations, including cloud services or local databases. For instance you could send it to an Apache Kafka cluster or store them in S 3. It is recommended to choose your destination type carefully considering performance and cost implications. 

## 4. Configure 

Configuring PyDataStream Processor is done primarily through the YAML file, typically named 'config.yaml'. 

   ```yaml
 # Source
 source:
   type: Kafka
   broker: mybroker .kafka

 #Filter
 filter:
   expression : "value > 10"


 # Aggregation configuration
 aggregation:
   type: Average

 # Destination configuration
 destination :
   destination_type : File
  file_destination : output.txt
 ```

Environment variables can supplement the configuration in `config.yaml`. For example, you could use environment variables to define sensitive information like API keys or passwords to ensure data security when using the platform across different setups and configurations. 

Custom processors are configured through their class constructors which accept arguments. The argument names will vary for each operator. It is always recommended to look at the operator's documentation for the specific configuration requirements.

You can also set global variables for Py DataStream Processor using the 'global_ config.yaml' configuration. These variables can be leveraged for cross- module parameters, such as the logging levels or thread count used for parallel processing.

The PyDataStream Processor offers different modes: Development, Debug, and Production. Each mode impacts the logging configuration to facilitate development and debugging while ensuring minimal overhead in production deployments. 

If you need to modify the behavior dynamically during a pipeline execution consider using runtime configuration parameters for certain components, like altering filter conditions or window parameters to adapt the data analysis to incoming conditions. This enables a flexible real- time environment. 

You can define default values for your parameters and ensure the program can function when certain configuration options are missing. Providing defaults reduces configuration complexity and helps maintain a robust pipeline execution.

## 5. Project Structure

*   **src/:**  Contains the core codebase for PyDataStream Processor, including classes, functions, and modules.
   *   **core/:**  Core functionalities, like data stream management, processing and configuration.
   *   **operators/:**  Custom data processing logic ( filters, aggregators) for specific analytics. Includes a basic set of predefined processors. 
   *    **sources/:**  Data source adapters like Kafka, files, API integrations and custom adapters.
   *   `**destinations/:"` - S 3.csv

*  **tests/: ** Contains unit and integration tests to check the code.
*   **configs :** Contains the default and sample configuration files used during deployment. This folder is used for the YAML configuration files.
*   **examples/:**  Demonstrates practical usages and scenarios with PyDataStream Processor. It contains examples with sample datasets.
*   **README.md:**  Project documentation, including installation and usage instructions. 
*   `PYSETUP.PY`:  The main setup for installation

## 6. Contributing

We invite you to contribute towards Py DataStream Processor. First, review the open Issues to identify areas for enhancements, new features, or bug fixes. 

To submit fixes, enhancements, or other improvements, create a new branch in our Git repository:

```bash  
git checkout -b <your branch name>
```

Make the necessary changes and commit them following the project's coding guidelines. We follow PEP8 for code style. All commits must be descriptive.

Submit code via pull requests. The pull requests should adhere to our code style and include tests for the changes. Code must pass all tests. 

We will review the pull request and provide feedback. After addressing any concerns, you might be asked to rebase the changes, update any dependencies etc.

## 7. License

Py DataStream Processor released under MIT License, offering flexibility for usage in commercial applications. You are free to copy, adapt the library for any purpose, as the license does not restrict the usage for commercial purposes with the requirement of including copyright notice and license terms in redistributed copies. Redistribution is permitted, you can also modify this software.

However, any modifications and redistribution of the software, including derivative works, must retain the copyright notices and this license statement. If you make changes to the code or create a derivative, please give attribution and indicate that you have modified the PyDataStream Processor' s code.

## 8. Acknowledgments

We would like to acknowledge:

 *   Apache Kafka for providing a powerful real- time streaming platform.
 *   The entire Python community for their invaluable support and open source libraries.
 *   Individual contributors who have shared their insights and helped refine the codebase with their suggestions on features and functionalities.
   *   We are thankful for open- source testing frameworks like `pytest`, enabling robust verification of the code logic and features.
  *   The cloud providers ( AWS,Azure,GCP).

## 9. System Architecture

PyDS employs a modular, pipeline-based architecture to process real- time data streams. The core comprises a stream processing engine, source adapters, processing operators, and destination sinks that operate together.

Data enters the pipeline via `sources`, which extract data from input sources like Kafka, files and databases. This raw data then passes into the stream engine for transformations, filtering and aggregation. Processing `operators`, like filters and aggregators manipulate data, enabling specific analytics logic.

The engine utilizes a reactive model where operators are triggered as data flows, facilitating real- time processing. Data transformations can be implemented as Python functions and seamlessly integrated into the stream pipeline, making the process very extensible. 

`Destinations` handle the outputs. It writes transformed analytics result in storage, databases, or alerts for various consumption points, offering diverse output capabilities to match the analytical pipeline needs. This layered structure simplifies modular maintenance, enabling new integrations without disturbing system functionality,

The engine' s internal threading architecture manages the concurrency required by real- time operations to process the large throughput and ensure the system meets stringent latency needs of analytics applications. PyDataStream's design is focused to offer scalable processing for both simple data transformations to very demanding analytical requirements. 

## 10. API Reference

The primary `PyDataStream` class is where data processing flows originate. 

  *  `PyDataStream.Source()` - Configures the stream's origin
   *   `PyDataStream.Operator(operation)` Creates a operator. operation = < filter_type>, < aggregator >. 
   *   `PyDataStream.Processor()` Defines stream pipelines by joining various processing blocks: ` Source`, operators and `Destinations`.
  * ` Processor.run()` Initiates streaming process from first step until outputting data into the defined destination 

Example Usage of `filter()`:

  ```python    
  from PyDataStream import  PyDataStream  

   PyStream = Py DataStream()
   myFilter =   my Stream. filter ( condition =" value>10) 
   ...  
 ```
## 11. Testing

You can initiate testing by simply executing commands within your local environment:

  ```bash
  cd tests
  pytest # For all the test
  python -m pytest <filename>.py
 ```
 The `PyDataStream` comes with several tests for the main components of each of operators: `Filter`,`Average`, as well. The project includes unit and integration testing using Python and libraries from its environment: The main goal from these integration steps will ensure all functionalities from all operators work seamlessly in conjunction with the other elements from your data processing stream 
 We expect all external libraries or interoperable components should provide test cases for the project

## 12. Troubleshooting

A frequent installation failure relates to the Python dependencies or environment issues, often resulting from version conflict with the packages and environment versions used on deployment systems. If installation problems emerge: update packages. Try reinstall with virtual environment: 

Common error "ModuleNotFound Error". It can emerge with missing packages. Ensure dependencies from PydataStream package. Use command to check and verify that your dependencies were fully loaded : import Pydatastream
If an exception happens with processing data, review configurations of `source`,`operator and destination.
Verify all data source connection credentials as well
Ensure that the operators filter conditions have the valid parameters. 

## 13. Performance Optimization

For improved processing efficiency implement appropriate caching and buffer size. Optimize operations to prevent unnecessary computations and I / O overhead by limiting data access from outside storage when available

Utilize multi- threaded/concurrent architecture where appropriate when applicable by leveraging parallel processors.  Optimize filter operations: Use indexes where applicable for quick access when searching for values

Implement monitoring systems and tools like profilers in order to detect potential system bottlenecks: Analyze system resources, optimize data structures as applicable

## 14. Security Considerations

Ensure you store API Keys and other private data safely with encryption techniques, never in public versioned repository and utilize a key management solution for access controls

 Validate any user input, data sources or any data streams, before it gets processed by applying input sanitisation or encoding techniques to mitigate security concerns related to potential injections, XSS etc

 Apply least permission to data storage locations and other services. 

 Keep libraries/framework versions to mitigate against vulnerabilities and implement regular scans for any reported vulnerability and patch them with appropriate versioning and upkeep practices to reduce any risk factors from malicious threats and intrusion
 
## 15. Roadmap

Future versions plan the integration of a machine learning integration to allow more dynamic real- time predictions

Add better debugging tools, allowing to pinpoint issues and monitor pipeline performance 

We also will expand data integrations with a more robust selection of data streams

Improve support from additional databases such as No SQL options to support greater analytical workflows. The enhancement for hardware accelerations such as the GPU.

## 16. FAQ

How is Pydatastream Processor best integrated in cloud deployments. Cloud integrations for services from AWS or Google are available to enhance cloud deployments with easily integrated configurations
Is support possible on multiple different Python environment setups, for multiple OS types and architectures like windows MacOS Linux

 Yes. The code runs fine as Pydatastream supports many python environment and it's been successfully verified with Linux.Windows as MacOS 
  How is Pydatastream configured for different real time streaming platforms?

 We will soon support more than 2 types for real time stream sources and provide detailed instructions and configuration options. The integration documentation should also provide the configurations required by a user. 

## 17. Citation

If used for scientific/research publications:

   ```bibtex
 @misc {pydatastream,
    author = {Authors},
    title = {PyDataStream Processor: A Library for Real-time Data Analytics},
    year = {2024},
    note = {https://github.com/example/pydatastream}
 }
 ```

 Proper citation allows us give recognition and credit when Py DataStream processor assists research

## 18. Contact

You can find us here to ask question, raise issue: GitHub, mailing groups. You should feel to contribute by suggesting a pull request for bug fix improvements

Contact:

`example email`  will be answered by team of engineers

Please submit your bugs here GitHub Issues tracker, to ensure quick support

Find out community channel at discord for quick support and discussions