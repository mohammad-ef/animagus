# PyDataStream Processor - Real-time Stream Data Analytics

A powerful Python library to facilitate stream data processing, aggregation, and anomaly detection with configurable pipelines.  It enables users to easily build and deploy streaming applications. The project is aimed at simplifying complex data stream management. It provides flexible data handling, customizable pipelines. The library is suitable across various fields requiring fast-paced data analytics. The architecture is designed for high efficiency and scalability.  We use an actor pattern.


## Overview & Function Description
This package, PyDataStream, is an engine to efficiently handle real time data from a number of various inputs. This is handled by a pipeline which consists of various actors. An ' actor' is a component which performs a function on the data stream. For example, a filter might select only events that fit a defined criteria, while an aggregate component calculates statistics for a period of time. The system also allows you to easily inject an AI/ML agent.

Currently the pipeline is configured and run from configuration file. This file specifies the order of components. Each component in the configuration is configured via a number of key/value parameters passed into each actor' class constructor.

It' provides an easy to use API.  Users can easily extend the actor system by providing new custom components.  All the core components are tested.

The project is intended to be used by both data scientists and engineers.



## Installation Instructions

The following sections describe how setup your environment. Make ensure all of the prerequisities described exist on your local environment before moving on.

**Pr Prerequisites**
Before beginning, you must have installed Python (3.8+) alongside pip (Python's dependency install). It will require the numpy, scipy, pandas and other data analysis packages. These are all included.  Make certain a compatible IDE has been installed.

**Create an environment**
The recommended installation method is to create one isolated env with a `venv`.
    
First, create a directory structure to house the project:
  ```
mkdir PyDataStream/ && cd /PyDataStream
```    
**Install a new python env**
   ``` 
venv = py -3 -m venv venv
```     
   Then activate the env:

   On macOS/Linux:
     
   ```
   source venv /bin/bash 
   ```

   On Windows PowerShell:

   ```
     .\venv\Scripts\Activate
   ```

**Installing Packages**    
   After activation of the venv, proceed by installing all of the required dependencies.

  ```
   python -m pip install -U pip
   ```
 **Using the requirement file**

   We use ` requirements . txt`
   ```
     pip -r requirements.txt install
   ```

**Platform Specific Notes**
On Windows, some dependencies may necessitate using an appropriate compiler or package installation tools. Ensure these are set up beforehand.  For MacOS, brew install may assist the dependencies installation, while Linux users should check the distro specific tools such as `apt-get `or equivalent package manager. 



## Usage Instructions

The basic process is to define a pipeline configuration then load the configuration and run. The first step involves importing the PyDataStream library.

```python
  from pystream.pipeline import Pipeline
```

**Loading and Initializing Pipeline**

To start up a pipeline you will need a YAML pipeline configuration file.

```python   
 pipeline_config = ' config / stream_process . yaml '
  
  my_stream   =   Pipeline (pipeline_config )
```

**Running the stream**
Once instantiated it can be run.

```python
  my_ stream . run (data_producer ) # data_producer must be an iterable
```

 **Command Line Example**
   The system has a CLI to run and configure pipelines from the command line. This feature can be invoked by running
  ```
  ./stream_launcher --config stream_config. yaml --data_stream file . txt --output  outfile.txt
   ```
  It can be configured via a CLI.

**API Integration Example**
  It provides a Python API for integrating it within a data ingestion application.  It allows data streams to be fed to the processor via a standard python iterable.   
## Configuration

Py DataStream heavily relies on YAML files to define the pipeline. Each file contains a description of the pipeline, specifying each actor involved.

The root key `pipeline` contains a ` stages ` key. Within ` stages`, you'll define the order and configuration of actors. Each stage will have a ` module `, which will indicate the python file containing the code of the particular stage as an import statement. Each stage needs a ` parameters ` key to be configured. This can be any key / value combination that can be fed to the class contructor of the particular class specified by `module `. The module is required. The name of a module is used.

Environment variables override configuration file values during the pipeline run time. For instance, the number of worker actors in a parallel processing stage could be set through an environment variable.

**Example of a simple YAML config file**

```yaml
#stream processing
pipeline:
  stages:
    - module: src.pystream.actors.filter # path to file in src directory 
      id: first_filter # name given internally for pipeline reference. 
      parameters: #parameters to pass to class init, all are optional. This example requires ` threshold` and passes an optional value ` threshold_override `. The second is optional. It has no default value so an error will result if it does not get a parameter. This allows to override it at runtime 1

    #another simple stage
     -  module:  src.pystream.actors .  aggregate   # path to file 
       id: my_aggregate
        # all the parameters can be defined here
       
```

## Configuration File Locations
The config file can be provided via a command line flag. Otherwise the default config will load.

## Project Structure

Here's a breakdown of the repository's structure:
    
- `src/`:  Contains all source Python code. This will contain all the actor components. `pystream/` is the root of the modules.
- ` tests/`:  Unit and integration tests for all components.
- ` configs/`: Sample configurations and default settings.
-   `docs/`: Documentation files, API documentation is stored here.

**Tree Structure**    

```txt    
PyDataStream/
|-- README.md
|-- LICENSE
|--  configs/
|   | -- default_config.yml
|   | -- other_stream_config.yml
  
|--  pyproject.toml
|   
  
|- src/
| -- pystream/  
|   |-- pipeline . py #pipeline definition
|   |-- __init__.py   #make pystream package
|   | -- models . py # Data models
|   | -- actors/   #actor implementation
|   |   | --  __init__ .  py #make the actors a sub- package
|   | --  filter .  py
 |   |  -- aggregate .py     #other actor files
   
   |-- cli .     

|- tests
|-- test_example . py
 |- test_actor .   py
 |- test_pipeline . py
```

 ## Contributing

We encourage contributions to PyDataStream.  Feel free and welcome to fork the project.

**Reporting Issues**
If you encounter a bug or have feedback, create a new issue on the GitHub repository. Include a clear description of the issue as well as the steps to reproduce it. Include version and system details.

**Pull Requests**
Submit pull- requests with your changes. Before submitting make sure the code is tested.

** Coding Standards**
We use PEP8 coding standards to maintain code style. We use type annotations to maintain static type information.

** Testing Requirements**
All new components should have accompanying unit tests. Ensure these tests pass on a clean environment. We use pytest for testing.

## License

This project is licensed under the MIT License - see the LICENSE file in the repository for details.

You are free to use, modify, and distribute the software, but it is imperative to retain copyright notice and license disclaimers.

## Acknowledg ments

This work would not have be possible without contributions from: the pandas community, the numpy contributors, scipy and other packages involved with data analysis.

## System Architecture

PyDataStream's architecture is composed of three core parts: the `Pipeline Engine`, the `Actor System`, and the `Configuration Loader`. Data flows sequentially through the pipeline based on the config file. The engine manages the lifecycle of the entire processing operation including configuration loading, actor instantiation, and data stream handling. Actors are independent units of functionality, designed to transform, filter, or aggregate data within the pipeline.

The configuration loader parses the YAML configuration file, mapping stage definitions to specific actor classes and their respective parameters. A simple actor model is used.

Each actor class must implement at least an `init` (constructor) and a method `process`, to process each data point.  It provides modularity, extensibility, and allows to easily inject custom components. This architecture promotes loose coupling between components making debugging easier and facilitating independent testing of individual actors. Data flow through the pipeline is controlled by the engine with each actor transforming the stream sequentially.

## API Reference

The API provides a high-level interface to define and execute data processing pipelines. Key components include: the `Pipeline` class and actor classes.   
The `Pipeline` class allows to configure and run a stream processor.   
The `Pipeline` class provides the following method:
    
- `run(data_stream, output_destination)`: Runs the pipeline with the given input and writes output to the given destination.
    
**Actor Classes**  
    
Each actor class must implement at least the following:

- ` __init__(self, parameters)` Constructor method.
-   ` process( self data )`: Takes a single data item as input.

** Example**
```python
   from pystream.models     import DataPoint
   from pystream.actors.aggregate     import Aggregate   
   my_  aggregate    =    Aggregate (threshold = 5)
      my_aggregate . process ( DataPoint (value=10)) #returns 10
```

## Testing

We utilize pytest for our testing framework.

 **To Run Tests**
   ```   
   pip install pytest
   pytest
   ```
The tests are organized by modules. Each actor should have its own test file, which tests edge cases. The tests can be executed using the provided command.

  **Setup Testing Environment**
    
   Ensure pytest is installed.
   We use mocking to simulate external components.

 ## Troubleshooting

- * ImportError: "No module named 'pystream'*
Check that the library is installed in the active environment using `pip list`. If not, re-run the installation steps described previously.

- *Configuration Error: Invalid YAML format*
Verify the YAML configuration file. Ensure proper indentation and syntax. Use a YAML validator tool if available.

- *Actor Initialization Error: Missing Parameters*
Check your pipeline configuration. Each actor needs its configuration keys specified in the config.

 ## Performance and Optimization

For optimal performance, PyDataStream can be optimized via various techniques. The number of workers should be set appropriately. Using efficient data types can improve speed of the data processing.  Caching intermediate results can reduce computational overhead and enhance throughput, particularly for frequently accessed data.

**Example Benchmark Results**
(Placeholder, insert example performance data and benchmark setup here).

## Security Considerations

Secure your PyDataStream deployment by following best practices for secret management - avoid hardcoding API keys or passwords directly in the code. Store them in secure environment variables. Sanitize all inputs to the stream to prevent injection attacks. Use proper authentication and authorization mechanisms when dealing with sensitive data.

## Roadmap

 - [x] Support for different data source types (Kafka, RabbitMQ).
 - [ ] Advanced anomaly detection algorithms based on machine learning models.
 - [ ] Real-time data visualization dashboard integration.
 -   [ ] Enhanced fault tolerance mechanisms for high-availability operation.
 -   [ ] Support for distributed processing using technologies like Apache Spark.

## FAQ (Frequently Asked Questions)

- *What data sources are currently supported?*  The system is designed to use python iterables as source.
- *How can I extend PyDataStream with my custom actor?* Inherit from a base actor class and define a `process` method.
- *What are the minimum Python requirements?* PyDataStream requires Python 3.8 or higher.
-   *Is PyDataStream suitable for very high-volume data streams?*  While it offers efficiency, very high-volume data streams may benefit from a distributed framework like Spark.

## Citation

If you use PyDataStream in an academic or research project, please cite it as follows:

```bibtex
@misc{pydatastream,
  author = {Your Name},
  title = {PyDataStream: Real Time Streaming Processor},
  year = {2024},
  note = {https://github.com/your-username/ PyDataStream}
}

```

## Contact

For any issues, feature requests, or general feedback, please reach out to:

Email: support@pydatastream.com

GitHub Issues: [https://github.com/your-username/PyDataStream/issues](https://github.com/your-username/ PyDataStream/issues)

We encourage the community to participate and contribute to the project.
  
  Join our discussion forum at: [example.com/ forum/pystream](http:// www.exmaple.com/ forum)